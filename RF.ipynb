{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T05:33:54.333980Z",
     "start_time": "2020-07-26T05:33:54.327911Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import gc\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_columns', 600)\n",
    "pd.set_option('display.max_rows', 600)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T05:35:54.260096Z",
     "start_time": "2020-07-26T05:35:53.351120Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = ''\n",
    "data_train = pd.read_csv(data_path+'sensor_train.csv')\n",
    "data_test = pd.read_csv(data_path+'sensor_test.csv')\n",
    "data_test['fragment_id'] += 10000\n",
    "label = 'behavior_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T05:36:25.974661Z",
     "start_time": "2020-07-26T05:36:01.493505Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:22<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "['acc_x_min', 'acc_x_max', 'acc_x_mean', 'acc_x_median', 'acc_x_std', 'acc_x_skew', 'acc_y_min', 'acc_y_max', 'acc_y_mean', 'acc_y_median', 'acc_y_std', 'acc_y_skew', 'acc_z_min', 'acc_z_max', 'acc_z_mean', 'acc_z_median', 'acc_z_std', 'acc_z_skew', 'acc_xg_min', 'acc_xg_max', 'acc_xg_mean', 'acc_xg_median', 'acc_xg_std', 'acc_xg_skew', 'acc_yg_min', 'acc_yg_max', 'acc_yg_mean', 'acc_yg_median', 'acc_yg_std', 'acc_yg_skew', 'acc_zg_min', 'acc_zg_max', 'acc_zg_mean', 'acc_zg_median', 'acc_zg_std', 'acc_zg_skew', 'acc_min', 'acc_max', 'acc_mean', 'acc_median', 'acc_std', 'acc_skew', 'accg_min', 'accg_max', 'accg_mean', 'accg_median', 'accg_std', 'accg_skew']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3395: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.loc._setitem_with_indexer((slice(None), indexer), value)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3367: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_array(key, value)\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3395: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.loc._setitem_with_indexer((slice(None), indexer), value)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3367: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_array(key, value)\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([data_train, data_test], sort=False)\n",
    "df = data.drop_duplicates(subset=['fragment_id']).reset_index(drop=True)[['fragment_id', 'behavior_id']]\n",
    "\n",
    "data['acc'] = (data['acc_x'] ** 2 + data['acc_y'] ** 2 + data['acc_z'] ** 2) ** 0.5\n",
    "data['accg'] = (data['acc_xg'] ** 2 + data['acc_yg'] ** 2 + data['acc_zg'] ** 2) ** 0.5\n",
    "data['xy'] = (data['acc_x'] ** 2 + data['acc_y'] ** 2 ) ** 0.5\n",
    "data['xy_g'] = (data['acc_xg'] ** 2 + data['acc_yg'] ** 2) ** 0.5\n",
    "\n",
    "for f in tqdm([f for f in data.columns if 'acc' in f]):\n",
    "    for stat in ['min', 'max', 'mean', 'median', 'std', 'skew']:\n",
    "        df[f+'_'+stat] = data.groupby('fragment_id')[f].agg(stat).values\n",
    "        \n",
    "train_df = df[df[label].isna()==False].reset_index(drop=True)\n",
    "test_df = df[df[label].isna()==True].reset_index(drop=True)\n",
    "\n",
    "drop_feat = []\n",
    "used_feat = [f for f in train_df.columns if f not in (['fragment_id', label] + drop_feat)]\n",
    "print(len(used_feat))\n",
    "print(used_feat)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_x = train_df[used_feat]\n",
    "train_y = train_df[label]\n",
    "test_x = test_df[used_feat]\n",
    "\n",
    "train_x[used_feat] = scaler.fit_transform(train_x)\n",
    "test_x[used_feat] =scaler.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T03:02:02.737367Z",
     "start_time": "2020-07-26T03:02:02.734303Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(oob_score=True, random_state=10,n_estimators=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T03:15:18.527298Z",
     "start_time": "2020-07-26T03:02:05.734581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=5000, oob_score=True, random_state=10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "383"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=5000, oob_score=True, random_state=10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=5000, oob_score=True, random_state=10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=5000, oob_score=True, random_state=10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=5000, oob_score=True, random_state=10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_train = np.zeros((len(train_x), 19))\n",
    "preds = np.zeros((len(test_x), 19))\n",
    "folds = 5\n",
    "seeds = [44]#, 2020, 527, 1527]\n",
    "for seed in seeds:\n",
    "    kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        x_trn, y_trn, x_val, y_val = train_x.iloc[trn_idx], train_y.iloc[trn_idx], train_x.iloc[val_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        rf.fit(x_trn,y_trn)\n",
    "        oof_train[val_idx] += rf.predict_proba(x_val) / len(seeds)\n",
    "        preds += rf.predict_proba(test_x) / folds / len(seeds)\n",
    "        #scores.append(model.best_score['valid_1']['multi_error'])\n",
    "        #imp['gain' + str(fold + 1)] = model.feature_importance(importance_type='gain')\n",
    "        #imp['split' + str(fold + 1)] = model.feature_importance(importance_type='split')\n",
    "        del x_trn, y_trn, x_val, y_val\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T03:00:20.351208Z",
     "start_time": "2020-07-26T03:00:20.345135Z"
    }
   },
   "outputs": [],
   "source": [
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "        16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T03:18:49.320782Z",
     "start_time": "2020-07-26T03:18:49.315754Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = np.argmax(preds, axis=1)\n",
    "oof_y = np.argmax(oof_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T03:18:50.942408Z",
     "start_time": "2020-07-26T03:18:50.916396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72134\n",
      "0.76185\n"
     ]
    }
   ],
   "source": [
    "print(round(accuracy_score(train_y, oof_y), 5))\n",
    "score = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(train_y, oof_y)) / oof_y.shape[0]\n",
    "print(round(score, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T03:21:59.733543Z",
     "start_time": "2020-07-26T03:21:59.727563Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7292, 19)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T03:25:11.615778Z",
     "start_time": "2020-07-26T03:25:11.579900Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df[['fragment_id']].to_csv('oof_rf_prob.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T03:25:34.544122Z",
     "start_time": "2020-07-26T03:25:34.522109Z"
    }
   },
   "outputs": [],
   "source": [
    "oof_rf_prob = pd.read_csv('oof_rf_prob.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T03:35:03.678820Z",
     "start_time": "2020-07-26T03:35:03.674546Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = [str(i)+'_proba' for i in range(19)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T03:37:02.273566Z",
     "start_time": "2020-07-26T03:37:02.260608Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in columns:\n",
    "    oof_rf_prob[col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T03:37:44.956159Z",
     "start_time": "2020-07-26T03:37:44.899431Z"
    }
   },
   "outputs": [],
   "source": [
    "oof_rf_prob[columns] = oof_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T03:38:53.612929Z",
     "start_time": "2020-07-26T03:38:53.450474Z"
    }
   },
   "outputs": [],
   "source": [
    "oof_rf_prob.to_csv('oof_rf_prob.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T03:39:34.435828Z",
     "start_time": "2020-07-26T03:39:34.409916Z"
    }
   },
   "outputs": [],
   "source": [
    "oof_rf_prob_test = pd.read_csv('sub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T03:40:09.112546Z",
     "start_time": "2020-07-26T03:40:09.105496Z"
    }
   },
   "outputs": [],
   "source": [
    "oof_rf_prob_test.drop(['behavior_id'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T03:40:31.501176Z",
     "start_time": "2020-07-26T03:40:31.486229Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in columns:\n",
    "    oof_rf_prob_test[col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T03:41:57.641232Z",
     "start_time": "2020-07-26T03:41:57.435931Z"
    }
   },
   "outputs": [],
   "source": [
    "oof_rf_prob_test.to_csv('oof_rf_prob_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T05:51:26.864255Z",
     "start_time": "2020-07-26T05:48:51.106138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[20]\ttraining's multi_error: 0.338934\tvalid_1's multi_error: 0.438657\n",
      "[40]\ttraining's multi_error: 0.250986\tvalid_1's multi_error: 0.385881\n",
      "[60]\ttraining's multi_error: 0.211726\tvalid_1's multi_error: 0.356408\n",
      "[80]\ttraining's multi_error: 0.181382\tvalid_1's multi_error: 0.349554\n",
      "[100]\ttraining's multi_error: 0.162181\tvalid_1's multi_error: 0.337903\n",
      "[120]\ttraining's multi_error: 0.145037\tvalid_1's multi_error: 0.32488\n",
      "[140]\ttraining's multi_error: 0.125493\tvalid_1's multi_error: 0.320082\n",
      "[160]\ttraining's multi_error: 0.108863\tvalid_1's multi_error: 0.318026\n",
      "[180]\ttraining's multi_error: 0.0925767\tvalid_1's multi_error: 0.311857\n",
      "[200]\ttraining's multi_error: 0.0792045\tvalid_1's multi_error: 0.307745\n",
      "[220]\ttraining's multi_error: 0.0666895\tvalid_1's multi_error: 0.305689\n",
      "[240]\ttraining's multi_error: 0.0564032\tvalid_1's multi_error: 0.305003\n",
      "[260]\ttraining's multi_error: 0.0469741\tvalid_1's multi_error: 0.302262\n",
      "[280]\ttraining's multi_error: 0.0382308\tvalid_1's multi_error: 0.300891\n",
      "[300]\ttraining's multi_error: 0.0324019\tvalid_1's multi_error: 0.297464\n",
      "[320]\ttraining's multi_error: 0.0267444\tvalid_1's multi_error: 0.294722\n",
      "[340]\ttraining's multi_error: 0.0202297\tvalid_1's multi_error: 0.292666\n",
      "[360]\ttraining's multi_error: 0.0154295\tvalid_1's multi_error: 0.291295\n",
      "[380]\ttraining's multi_error: 0.0113149\tvalid_1's multi_error: 0.29061\n",
      "[400]\ttraining's multi_error: 0.00840048\tvalid_1's multi_error: 0.285812\n",
      "[420]\ttraining's multi_error: 0.00651466\tvalid_1's multi_error: 0.285812\n",
      "[440]\ttraining's multi_error: 0.00462884\tvalid_1's multi_error: 0.284441\n",
      "[460]\ttraining's multi_error: 0.00360021\tvalid_1's multi_error: 0.282385\n",
      "[480]\ttraining's multi_error: 0.00325733\tvalid_1's multi_error: 0.283071\n",
      "[500]\ttraining's multi_error: 0.00291445\tvalid_1's multi_error: 0.281014\n",
      "[520]\ttraining's multi_error: 0.0022287\tvalid_1's multi_error: 0.278958\n",
      "[540]\ttraining's multi_error: 0.00171438\tvalid_1's multi_error: 0.280329\n",
      "[560]\ttraining's multi_error: 0.00120007\tvalid_1's multi_error: 0.276902\n",
      "[580]\ttraining's multi_error: 0.000514315\tvalid_1's multi_error: 0.27279\n",
      "[600]\ttraining's multi_error: 0.000514315\tvalid_1's multi_error: 0.272104\n",
      "[620]\ttraining's multi_error: 0.000342877\tvalid_1's multi_error: 0.272104\n",
      "[640]\ttraining's multi_error: 0.000342877\tvalid_1's multi_error: 0.271419\n",
      "Early stopping, best iteration is:\n",
      "[607]\ttraining's multi_error: 0.000514315\tvalid_1's multi_error: 0.271419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[20]\ttraining's multi_error: 0.327276\tvalid_1's multi_error: 0.438657\n",
      "[40]\ttraining's multi_error: 0.2455\tvalid_1's multi_error: 0.387252\n",
      "[60]\ttraining's multi_error: 0.209669\tvalid_1's multi_error: 0.379027\n",
      "[80]\ttraining's multi_error: 0.184982\tvalid_1's multi_error: 0.367375\n",
      "[100]\ttraining's multi_error: 0.161324\tvalid_1's multi_error: 0.358465\n",
      "[120]\ttraining's multi_error: 0.141951\tvalid_1's multi_error: 0.352296\n",
      "[140]\ttraining's multi_error: 0.127893\tvalid_1's multi_error: 0.344071\n",
      "[160]\ttraining's multi_error: 0.111092\tvalid_1's multi_error: 0.334476\n",
      "[180]\ttraining's multi_error: 0.0949769\tvalid_1's multi_error: 0.320768\n",
      "[200]\ttraining's multi_error: 0.0812618\tvalid_1's multi_error: 0.317341\n",
      "[220]\ttraining's multi_error: 0.0699469\tvalid_1's multi_error: 0.311172\n",
      "[240]\ttraining's multi_error: 0.0576033\tvalid_1's multi_error: 0.311857\n",
      "[260]\ttraining's multi_error: 0.0483456\tvalid_1's multi_error: 0.306374\n",
      "[280]\ttraining's multi_error: 0.0406309\tvalid_1's multi_error: 0.306374\n",
      "[300]\ttraining's multi_error: 0.0318875\tvalid_1's multi_error: 0.305003\n",
      "[320]\ttraining's multi_error: 0.0262301\tvalid_1's multi_error: 0.304318\n",
      "[340]\ttraining's multi_error: 0.0205726\tvalid_1's multi_error: 0.305003\n",
      "[360]\ttraining's multi_error: 0.0164581\tvalid_1's multi_error: 0.302262\n",
      "[380]\ttraining's multi_error: 0.0133722\tvalid_1's multi_error: 0.300891\n",
      "[400]\ttraining's multi_error: 0.00925767\tvalid_1's multi_error: 0.296093\n",
      "[420]\ttraining's multi_error: 0.00754329\tvalid_1's multi_error: 0.295408\n",
      "[440]\ttraining's multi_error: 0.00617178\tvalid_1's multi_error: 0.296093\n",
      "Early stopping, best iteration is:\n",
      "[408]\ttraining's multi_error: 0.00822904\tvalid_1's multi_error: 0.294722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[20]\ttraining's multi_error: 0.331505\tvalid_1's multi_error: 0.435528\n",
      "[40]\ttraining's multi_error: 0.2494\tvalid_1's multi_error: 0.378601\n",
      "[60]\ttraining's multi_error: 0.214775\tvalid_1's multi_error: 0.358025\n",
      "[80]\ttraining's multi_error: 0.189921\tvalid_1's multi_error: 0.33882\n",
      "[100]\ttraining's multi_error: 0.168152\tvalid_1's multi_error: 0.337449\n",
      "[120]\ttraining's multi_error: 0.148783\tvalid_1's multi_error: 0.328532\n",
      "[140]\ttraining's multi_error: 0.127185\tvalid_1's multi_error: 0.317558\n",
      "[160]\ttraining's multi_error: 0.114501\tvalid_1's multi_error: 0.311385\n",
      "[180]\ttraining's multi_error: 0.0980459\tvalid_1's multi_error: 0.305898\n",
      "[200]\ttraining's multi_error: 0.0839904\tvalid_1's multi_error: 0.296982\n",
      "[220]\ttraining's multi_error: 0.0714775\tvalid_1's multi_error: 0.293553\n",
      "[240]\ttraining's multi_error: 0.0610216\tvalid_1's multi_error: 0.291495\n",
      "[260]\ttraining's multi_error: 0.0529654\tvalid_1's multi_error: 0.290123\n",
      "[280]\ttraining's multi_error: 0.0430237\tvalid_1's multi_error: 0.28738\n",
      "[300]\ttraining's multi_error: 0.0335962\tvalid_1's multi_error: 0.286008\n",
      "[320]\ttraining's multi_error: 0.0262256\tvalid_1's multi_error: 0.284636\n",
      "[340]\ttraining's multi_error: 0.0224546\tvalid_1's multi_error: 0.284636\n",
      "[360]\ttraining's multi_error: 0.0181694\tvalid_1's multi_error: 0.285322\n",
      "[380]\ttraining's multi_error: 0.0149126\tvalid_1's multi_error: 0.280521\n",
      "[400]\ttraining's multi_error: 0.01217\tvalid_1's multi_error: 0.27572\n",
      "[420]\ttraining's multi_error: 0.00925609\tvalid_1's multi_error: 0.27572\n",
      "[440]\ttraining's multi_error: 0.00685636\tvalid_1's multi_error: 0.273663\n",
      "[460]\ttraining's multi_error: 0.00531368\tvalid_1's multi_error: 0.271605\n",
      "[480]\ttraining's multi_error: 0.00428522\tvalid_1's multi_error: 0.265432\n",
      "[500]\ttraining's multi_error: 0.00308536\tvalid_1's multi_error: 0.268176\n",
      "[520]\ttraining's multi_error: 0.00274254\tvalid_1's multi_error: 0.26749\n",
      "Early stopping, best iteration is:\n",
      "[480]\ttraining's multi_error: 0.00428522\tvalid_1's multi_error: 0.265432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[20]\ttraining's multi_error: 0.322077\tvalid_1's multi_error: 0.419753\n",
      "[40]\ttraining's multi_error: 0.244086\tvalid_1's multi_error: 0.364198\n",
      "[60]\ttraining's multi_error: 0.21169\tvalid_1's multi_error: 0.349794\n",
      "[80]\ttraining's multi_error: 0.18735\tvalid_1's multi_error: 0.340192\n",
      "[100]\ttraining's multi_error: 0.164553\tvalid_1's multi_error: 0.32716\n",
      "[120]\ttraining's multi_error: 0.143469\tvalid_1's multi_error: 0.320988\n",
      "[140]\ttraining's multi_error: 0.124614\tvalid_1's multi_error: 0.305898\n",
      "[160]\ttraining's multi_error: 0.106616\tvalid_1's multi_error: 0.301783\n",
      "[180]\ttraining's multi_error: 0.0934179\tvalid_1's multi_error: 0.29561\n",
      "[200]\ttraining's multi_error: 0.0807336\tvalid_1's multi_error: 0.290809\n",
      "[220]\ttraining's multi_error: 0.0689064\tvalid_1's multi_error: 0.288752\n",
      "[240]\ttraining's multi_error: 0.0550223\tvalid_1's multi_error: 0.285322\n",
      "[260]\ttraining's multi_error: 0.0438807\tvalid_1's multi_error: 0.283951\n",
      "[280]\ttraining's multi_error: 0.0363387\tvalid_1's multi_error: 0.277778\n",
      "[300]\ttraining's multi_error: 0.0287967\tvalid_1's multi_error: 0.277092\n",
      "[320]\ttraining's multi_error: 0.0246829\tvalid_1's multi_error: 0.277778\n",
      "[340]\ttraining's multi_error: 0.0198834\tvalid_1's multi_error: 0.276406\n",
      "[360]\ttraining's multi_error: 0.0155982\tvalid_1's multi_error: 0.274348\n",
      "[380]\ttraining's multi_error: 0.0123414\tvalid_1's multi_error: 0.272977\n",
      "[400]\ttraining's multi_error: 0.00994172\tvalid_1's multi_error: 0.270233\n",
      "[420]\ttraining's multi_error: 0.00719918\tvalid_1's multi_error: 0.270919\n",
      "Early stopping, best iteration is:\n",
      "[386]\ttraining's multi_error: 0.0116558\tvalid_1's multi_error: 0.269547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "684"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[20]\ttraining's multi_error: 0.330305\tvalid_1's multi_error: 0.438957\n",
      "[40]\ttraining's multi_error: 0.250086\tvalid_1's multi_error: 0.391632\n",
      "[60]\ttraining's multi_error: 0.211862\tvalid_1's multi_error: 0.375171\n",
      "[80]\ttraining's multi_error: 0.189407\tvalid_1's multi_error: 0.358025\n",
      "[100]\ttraining's multi_error: 0.164038\tvalid_1's multi_error: 0.347051\n",
      "[120]\ttraining's multi_error: 0.144155\tvalid_1's multi_error: 0.335391\n",
      "[140]\ttraining's multi_error: 0.1289\tvalid_1's multi_error: 0.335391\n",
      "[160]\ttraining's multi_error: 0.111587\tvalid_1's multi_error: 0.323731\n",
      "[180]\ttraining's multi_error: 0.098903\tvalid_1's multi_error: 0.318244\n",
      "[200]\ttraining's multi_error: 0.0850189\tvalid_1's multi_error: 0.312071\n",
      "[220]\ttraining's multi_error: 0.072506\tvalid_1's multi_error: 0.309328\n",
      "[240]\ttraining's multi_error: 0.0613644\tvalid_1's multi_error: 0.305213\n",
      "[260]\ttraining's multi_error: 0.0515941\tvalid_1's multi_error: 0.303155\n",
      "[280]\ttraining's multi_error: 0.0411382\tvalid_1's multi_error: 0.296982\n",
      "[300]\ttraining's multi_error: 0.0335962\tvalid_1's multi_error: 0.292181\n",
      "[320]\ttraining's multi_error: 0.0277683\tvalid_1's multi_error: 0.294925\n",
      "[340]\ttraining's multi_error: 0.022626\tvalid_1's multi_error: 0.290809\n",
      "Early stopping, best iteration is:\n",
      "[303]\ttraining's multi_error: 0.0325677\tvalid_1's multi_error: 0.290123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "imp = pd.DataFrame()\n",
    "imp['feat'] = used_feat\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.02,\n",
    "    'metric': 'multi_error',\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 19,\n",
    "    'feature_fraction': 0.80,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'bagging_freq': 2,\n",
    "    'n_jobs': 4,\n",
    "    'seed': 2020,\n",
    "    'max_depth': 7,\n",
    "    'num_leaves': 31,\n",
    "    'lambda_l1': 0.7,\n",
    "    'lambda_l2': 0.7,\n",
    "}\n",
    "\n",
    "oof_train = np.zeros((len(train_x), 19))\n",
    "preds = np.zeros((len(test_x), 19))\n",
    "folds = 5\n",
    "seeds = [44]#, 2020, 527, 1527]\n",
    "for seed in seeds:\n",
    "    kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        x_trn, y_trn, x_val, y_val = train_x.iloc[trn_idx], train_y.iloc[trn_idx], train_x.iloc[val_idx], train_y.iloc[val_idx]\n",
    "        train_set = lgb.Dataset(x_trn, y_trn)\n",
    "        val_set = lgb.Dataset(x_val, y_val)\n",
    "\n",
    "        model = lgb.train(params, train_set, num_boost_round=500000,\n",
    "                          valid_sets=(train_set, val_set), early_stopping_rounds=50,\n",
    "                          verbose_eval=20)\n",
    "        oof_train[val_idx] += model.predict(x_val) / len(seeds)\n",
    "        preds += model.predict(test_x) / folds / len(seeds)\n",
    "        scores.append(model.best_score['valid_1']['multi_error'])\n",
    "        imp['gain' + str(fold + 1)] = model.feature_importance(importance_type='gain')\n",
    "        imp['split' + str(fold + 1)] = model.feature_importance(importance_type='split')\n",
    "        del x_trn, y_trn, x_val, y_val, model, train_set, val_set\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T06:15:57.512877Z",
     "start_time": "2020-07-26T06:15:57.489954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72175\n",
      "0.76273\n"
     ]
    }
   ],
   "source": [
    "labels = np.argmax(preds, axis=1)\n",
    "oof_y = np.argmax(oof_train, axis=1)\n",
    "\n",
    "print(round(accuracy_score(train_y, oof_y), 5))\n",
    "score = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(train_y, oof_y)) / oof_y.shape[0]\n",
    "print(round(score, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T04:38:36.647166Z",
     "start_time": "2020-07-26T04:38:36.025139Z"
    }
   },
   "outputs": [],
   "source": [
    "oof_lgb_prob = pd.read_csv('oof_rf_prob.csv')\n",
    "oof_lgb_prob_test = pd.read_csv('oof_rf_prob_test.csv')\n",
    "oof_rf_prob[columns] = oof_train\n",
    "oof_lgb_prob_test[columns] = preds\n",
    "oof_lgb_prob.to_csv('oof_lgb_prob.csv',index=False)\n",
    "oof_lgb_prob_test.to_csv('oof_lgb_prob_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T05:45:36.113628Z",
     "start_time": "2020-07-26T05:45:11.445292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "oof_train = np.zeros((len(train_x), 19))\n",
    "preds = np.zeros((len(test_x), 19))\n",
    "folds = 5\n",
    "seeds = [44]#, 2020, 527, 1527]\n",
    "for seed in seeds:\n",
    "    kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        x_trn, y_trn, x_val, y_val = train_x.iloc[trn_idx], train_y.iloc[trn_idx], train_x.iloc[val_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        knn.fit(x_trn,y_trn)\n",
    "        oof_train[val_idx] += knn.predict_proba(x_val) / len(seeds)\n",
    "        preds += knn.predict_proba(test_x) / folds / len(seeds)\n",
    "        #scores.append(model.best_score['valid_1']['multi_error'])\n",
    "        #imp['gain' + str(fold + 1)] = model.feature_importance(importance_type='gain')\n",
    "        #imp['split' + str(fold + 1)] = model.feature_importance(importance_type='split')\n",
    "        del x_trn, y_trn, x_val, y_val\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T05:48:15.481256Z",
     "start_time": "2020-07-26T05:48:15.457332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62562\n",
      "0.67798\n"
     ]
    }
   ],
   "source": [
    "labels = np.argmax(preds, axis=1)\n",
    "oof_y = np.argmax(oof_train, axis=1)\n",
    "\n",
    "print(round(accuracy_score(train_y, oof_y), 5))\n",
    "score = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(train_y, oof_y)) / oof_y.shape[0]\n",
    "print(round(score, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T05:48:28.865609Z",
     "start_time": "2020-07-26T05:48:28.414989Z"
    }
   },
   "outputs": [],
   "source": [
    "oof_knn_prob = pd.read_csv('oof_rf_prob.csv')\n",
    "oof_knn_prob_test = pd.read_csv('oof_rf_prob_test.csv')\n",
    "oof_knn_prob[columns] = oof_train\n",
    "oof_knn_prob_test[columns] = preds\n",
    "oof_knn_prob.to_csv('oof_knn_prob.csv',index=False)\n",
    "oof_knn_prob_test.to_csv('oof_knn_prob_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T07:33:21.492431Z",
     "start_time": "2020-07-26T07:32:12.031430Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, gamma=10, kernel='linear', probability=True)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=100, gamma=10, kernel='linear', probability=True)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=100, gamma=10, kernel='linear', probability=True)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=100, gamma=10, kernel='linear', probability=True)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=100, gamma=10, kernel='linear', probability=True)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(kernel='linear',probability=True,C=100,gamma=10)\n",
    "oof_train = np.zeros((len(train_x), 19))\n",
    "preds = np.zeros((len(test_x), 19))\n",
    "folds = 5\n",
    "seeds = [44]#, 2020, 527, 1527]\n",
    "for seed in seeds:\n",
    "    kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):\n",
    "        x_trn, y_trn, x_val, y_val = train_x.iloc[trn_idx], train_y.iloc[trn_idx], train_x.iloc[val_idx], train_y.iloc[val_idx]\n",
    "\n",
    "        svc.fit(x_trn,y_trn)\n",
    "        oof_train[val_idx] += svc.predict_proba(x_val) / len(seeds)\n",
    "        preds += svc.predict_proba(test_x) / folds / len(seeds)\n",
    "        #scores.append(model.best_score['valid_1']['multi_error'])\n",
    "        #imp['gain' + str(fold + 1)] = model.feature_importance(importance_type='gain')\n",
    "        #imp['split' + str(fold + 1)] = model.feature_importance(importance_type='split')\n",
    "        del x_trn, y_trn, x_val, y_val\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T07:38:55.046173Z",
     "start_time": "2020-07-26T07:38:55.020237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50192\n",
      "0.57809\n"
     ]
    }
   ],
   "source": [
    "labels = np.argmax(preds, axis=1)\n",
    "oof_y = np.argmax(oof_train, axis=1)\n",
    "\n",
    "print(round(accuracy_score(train_y, oof_y), 5))\n",
    "score = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(train_y, oof_y)) / oof_y.shape[0]\n",
    "print(round(score, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T07:22:43.403782Z",
     "start_time": "2020-07-26T07:22:40.984518Z"
    }
   },
   "outputs": [],
   "source": [
    "oof_svc_prob = pd.read_csv('oof_rf_prob.csv')\n",
    "oof_svc_prob_test = pd.read_csv('oof_rf_prob_test.csv')\n",
    "oof_svc_prob[columns] = oof_train\n",
    "oof_svc_prob_test[columns] = preds\n",
    "oof_svc_prob.to_csv('oof_svc_prob.csv',index=False)\n",
    "oof_svc_prob_test.to_csv('oof_svc_prob_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T07:51:53.746728Z",
     "start_time": "2020-07-26T07:51:53.723814Z"
    }
   },
   "outputs": [],
   "source": [
    "data1 = pd.merge(oof_rf_prob,oof_lgb_prob,on=['fragment_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T07:53:26.489985Z",
     "start_time": "2020-07-26T07:53:26.454103Z"
    }
   },
   "outputs": [],
   "source": [
    "data1 =pd.merge(data1,oof_knn_prob,on=['fragment_id'])\n",
    "data1 =pd.merge(data1,oof_svc_prob,on=['fragment_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T07:57:56.959053Z",
     "start_time": "2020-07-26T07:57:56.880404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fragment_id</th>\n",
       "      <th>0_proba_x</th>\n",
       "      <th>1_proba_x</th>\n",
       "      <th>2_proba_x</th>\n",
       "      <th>3_proba_x</th>\n",
       "      <th>4_proba_x</th>\n",
       "      <th>5_proba_x</th>\n",
       "      <th>6_proba_x</th>\n",
       "      <th>7_proba_x</th>\n",
       "      <th>8_proba_x</th>\n",
       "      <th>9_proba_x</th>\n",
       "      <th>10_proba_x</th>\n",
       "      <th>11_proba_x</th>\n",
       "      <th>12_proba_x</th>\n",
       "      <th>13_proba_x</th>\n",
       "      <th>14_proba_x</th>\n",
       "      <th>15_proba_x</th>\n",
       "      <th>16_proba_x</th>\n",
       "      <th>17_proba_x</th>\n",
       "      <th>18_proba_x</th>\n",
       "      <th>0_proba_y</th>\n",
       "      <th>1_proba_y</th>\n",
       "      <th>2_proba_y</th>\n",
       "      <th>3_proba_y</th>\n",
       "      <th>4_proba_y</th>\n",
       "      <th>5_proba_y</th>\n",
       "      <th>6_proba_y</th>\n",
       "      <th>7_proba_y</th>\n",
       "      <th>8_proba_y</th>\n",
       "      <th>9_proba_y</th>\n",
       "      <th>10_proba_y</th>\n",
       "      <th>11_proba_y</th>\n",
       "      <th>12_proba_y</th>\n",
       "      <th>13_proba_y</th>\n",
       "      <th>14_proba_y</th>\n",
       "      <th>15_proba_y</th>\n",
       "      <th>16_proba_y</th>\n",
       "      <th>17_proba_y</th>\n",
       "      <th>18_proba_y</th>\n",
       "      <th>0_proba_x</th>\n",
       "      <th>1_proba_x</th>\n",
       "      <th>2_proba_x</th>\n",
       "      <th>3_proba_x</th>\n",
       "      <th>4_proba_x</th>\n",
       "      <th>5_proba_x</th>\n",
       "      <th>6_proba_x</th>\n",
       "      <th>7_proba_x</th>\n",
       "      <th>8_proba_x</th>\n",
       "      <th>9_proba_x</th>\n",
       "      <th>10_proba_x</th>\n",
       "      <th>11_proba_x</th>\n",
       "      <th>12_proba_x</th>\n",
       "      <th>13_proba_x</th>\n",
       "      <th>14_proba_x</th>\n",
       "      <th>15_proba_x</th>\n",
       "      <th>16_proba_x</th>\n",
       "      <th>17_proba_x</th>\n",
       "      <th>18_proba_x</th>\n",
       "      <th>0_proba_y</th>\n",
       "      <th>1_proba_y</th>\n",
       "      <th>2_proba_y</th>\n",
       "      <th>3_proba_y</th>\n",
       "      <th>4_proba_y</th>\n",
       "      <th>5_proba_y</th>\n",
       "      <th>6_proba_y</th>\n",
       "      <th>7_proba_y</th>\n",
       "      <th>8_proba_y</th>\n",
       "      <th>9_proba_y</th>\n",
       "      <th>10_proba_y</th>\n",
       "      <th>11_proba_y</th>\n",
       "      <th>12_proba_y</th>\n",
       "      <th>13_proba_y</th>\n",
       "      <th>14_proba_y</th>\n",
       "      <th>15_proba_y</th>\n",
       "      <th>16_proba_y</th>\n",
       "      <th>17_proba_y</th>\n",
       "      <th>18_proba_y</th>\n",
       "      <th>0_proba_x</th>\n",
       "      <th>1_proba_x</th>\n",
       "      <th>2_proba_x</th>\n",
       "      <th>3_proba_x</th>\n",
       "      <th>4_proba_x</th>\n",
       "      <th>5_proba_x</th>\n",
       "      <th>6_proba_x</th>\n",
       "      <th>7_proba_x</th>\n",
       "      <th>8_proba_x</th>\n",
       "      <th>9_proba_x</th>\n",
       "      <th>10_proba_x</th>\n",
       "      <th>11_proba_x</th>\n",
       "      <th>12_proba_x</th>\n",
       "      <th>13_proba_x</th>\n",
       "      <th>14_proba_x</th>\n",
       "      <th>15_proba_x</th>\n",
       "      <th>16_proba_x</th>\n",
       "      <th>17_proba_x</th>\n",
       "      <th>18_proba_x</th>\n",
       "      <th>0_proba_y</th>\n",
       "      <th>1_proba_y</th>\n",
       "      <th>2_proba_y</th>\n",
       "      <th>3_proba_y</th>\n",
       "      <th>4_proba_y</th>\n",
       "      <th>5_proba_y</th>\n",
       "      <th>6_proba_y</th>\n",
       "      <th>7_proba_y</th>\n",
       "      <th>8_proba_y</th>\n",
       "      <th>9_proba_y</th>\n",
       "      <th>10_proba_y</th>\n",
       "      <th>11_proba_y</th>\n",
       "      <th>12_proba_y</th>\n",
       "      <th>13_proba_y</th>\n",
       "      <th>14_proba_y</th>\n",
       "      <th>15_proba_y</th>\n",
       "      <th>16_proba_y</th>\n",
       "      <th>17_proba_y</th>\n",
       "      <th>18_proba_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.031669</td>\n",
       "      <td>0.441127</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.175464</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.037579</td>\n",
       "      <td>0.063476</td>\n",
       "      <td>0.005356</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.023875</td>\n",
       "      <td>0.175142</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.003027</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.015173</td>\n",
       "      <td>0.001460</td>\n",
       "      <td>0.002880</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.3552</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0992</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0512</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020270</td>\n",
       "      <td>0.591118</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.076698</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.013208</td>\n",
       "      <td>0.015505</td>\n",
       "      <td>0.022670</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.015394</td>\n",
       "      <td>0.148776</td>\n",
       "      <td>0.012079</td>\n",
       "      <td>0.002453</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.067546</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.481034</td>\n",
       "      <td>0.374553</td>\n",
       "      <td>6.250838e-06</td>\n",
       "      <td>8.488125e-02</td>\n",
       "      <td>5.771121e-06</td>\n",
       "      <td>3.099303e-04</td>\n",
       "      <td>1.461453e-04</td>\n",
       "      <td>3.566003e-07</td>\n",
       "      <td>1.441350e-08</td>\n",
       "      <td>1.626174e-03</td>\n",
       "      <td>0.049993</td>\n",
       "      <td>6.524021e-03</td>\n",
       "      <td>1.115529e-06</td>\n",
       "      <td>1.644272e-05</td>\n",
       "      <td>9.510262e-08</td>\n",
       "      <td>8.849970e-04</td>\n",
       "      <td>1.559794e-05</td>\n",
       "      <td>1.518542e-06</td>\n",
       "      <td>1.312220e-06</td>\n",
       "      <td>0.276344</td>\n",
       "      <td>4.974163e-03</td>\n",
       "      <td>3.196806e-07</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>3.635841e-06</td>\n",
       "      <td>4.885335e-03</td>\n",
       "      <td>4.949490e-08</td>\n",
       "      <td>1.744127e-07</td>\n",
       "      <td>9.497017e-12</td>\n",
       "      <td>5.787967e-05</td>\n",
       "      <td>0.713638</td>\n",
       "      <td>4.377463e-06</td>\n",
       "      <td>1.541949e-07</td>\n",
       "      <td>3.370034e-05</td>\n",
       "      <td>2.815372e-07</td>\n",
       "      <td>2.940170e-05</td>\n",
       "      <td>4.646852e-07</td>\n",
       "      <td>5.648534e-10</td>\n",
       "      <td>4.114973e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.008463</td>\n",
       "      <td>0.881487</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.017043</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.021602</td>\n",
       "      <td>0.032368</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.016896</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.004859</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.8154</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0498</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>0.845728</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.038582</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.014556</td>\n",
       "      <td>0.027319</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.032406</td>\n",
       "      <td>0.004537</td>\n",
       "      <td>0.025768</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.573070</td>\n",
       "      <td>0.079127</td>\n",
       "      <td>2.785844e-07</td>\n",
       "      <td>2.294374e-05</td>\n",
       "      <td>1.792608e-04</td>\n",
       "      <td>7.888665e-06</td>\n",
       "      <td>9.447433e-06</td>\n",
       "      <td>4.797030e-06</td>\n",
       "      <td>6.715825e-10</td>\n",
       "      <td>1.557029e-01</td>\n",
       "      <td>0.191702</td>\n",
       "      <td>7.186552e-05</td>\n",
       "      <td>9.514194e-10</td>\n",
       "      <td>2.383866e-07</td>\n",
       "      <td>5.640025e-07</td>\n",
       "      <td>1.009518e-04</td>\n",
       "      <td>6.820856e-08</td>\n",
       "      <td>5.948169e-07</td>\n",
       "      <td>1.545804e-08</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>8.662991e-01</td>\n",
       "      <td>1.819706e-09</td>\n",
       "      <td>0.007660</td>\n",
       "      <td>1.195446e-07</td>\n",
       "      <td>4.581188e-06</td>\n",
       "      <td>4.465006e-05</td>\n",
       "      <td>3.523876e-06</td>\n",
       "      <td>7.000848e-10</td>\n",
       "      <td>1.082985e-01</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>1.673466e-02</td>\n",
       "      <td>1.676316e-08</td>\n",
       "      <td>1.725869e-06</td>\n",
       "      <td>1.140225e-05</td>\n",
       "      <td>7.543473e-05</td>\n",
       "      <td>3.884795e-07</td>\n",
       "      <td>6.601894e-06</td>\n",
       "      <td>2.564135e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.093019</td>\n",
       "      <td>0.292943</td>\n",
       "      <td>0.002693</td>\n",
       "      <td>0.094578</td>\n",
       "      <td>0.003387</td>\n",
       "      <td>0.018731</td>\n",
       "      <td>0.081917</td>\n",
       "      <td>0.005751</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>0.055956</td>\n",
       "      <td>0.206233</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.004864</td>\n",
       "      <td>0.003710</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.090085</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.1746</td>\n",
       "      <td>0.1836</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.1022</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0746</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062659</td>\n",
       "      <td>0.253161</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>0.216549</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>0.026043</td>\n",
       "      <td>0.023008</td>\n",
       "      <td>0.014735</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>0.054802</td>\n",
       "      <td>0.149235</td>\n",
       "      <td>0.032038</td>\n",
       "      <td>0.006927</td>\n",
       "      <td>0.004962</td>\n",
       "      <td>0.003788</td>\n",
       "      <td>0.057288</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>0.075951</td>\n",
       "      <td>0.003743</td>\n",
       "      <td>0.806572</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>2.580042e-06</td>\n",
       "      <td>6.057014e-02</td>\n",
       "      <td>1.033879e-04</td>\n",
       "      <td>2.398190e-04</td>\n",
       "      <td>2.423308e-07</td>\n",
       "      <td>3.745946e-06</td>\n",
       "      <td>6.820599e-07</td>\n",
       "      <td>4.296690e-03</td>\n",
       "      <td>0.085365</td>\n",
       "      <td>2.586398e-02</td>\n",
       "      <td>1.232927e-07</td>\n",
       "      <td>4.215697e-04</td>\n",
       "      <td>1.705838e-07</td>\n",
       "      <td>1.158001e-02</td>\n",
       "      <td>3.563197e-03</td>\n",
       "      <td>2.959465e-04</td>\n",
       "      <td>4.850493e-04</td>\n",
       "      <td>0.491533</td>\n",
       "      <td>2.232068e-03</td>\n",
       "      <td>1.448590e-05</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>2.383762e-03</td>\n",
       "      <td>1.103980e-04</td>\n",
       "      <td>1.710348e-06</td>\n",
       "      <td>2.358376e-05</td>\n",
       "      <td>1.211643e-07</td>\n",
       "      <td>1.120005e-03</td>\n",
       "      <td>0.425814</td>\n",
       "      <td>4.230822e-04</td>\n",
       "      <td>3.531143e-05</td>\n",
       "      <td>2.149621e-04</td>\n",
       "      <td>2.756539e-03</td>\n",
       "      <td>7.284040e-02</td>\n",
       "      <td>7.481589e-06</td>\n",
       "      <td>1.729156e-04</td>\n",
       "      <td>1.339113e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.533759</td>\n",
       "      <td>0.186844</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>0.045731</td>\n",
       "      <td>0.003993</td>\n",
       "      <td>0.045217</td>\n",
       "      <td>0.053578</td>\n",
       "      <td>0.007821</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.020327</td>\n",
       "      <td>0.027389</td>\n",
       "      <td>0.009348</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.003455</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.037426</td>\n",
       "      <td>0.003093</td>\n",
       "      <td>0.004004</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.4816</td>\n",
       "      <td>0.0860</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0470</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0752</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.0978</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447850</td>\n",
       "      <td>0.161704</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.075070</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.056924</td>\n",
       "      <td>0.026587</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>0.008121</td>\n",
       "      <td>0.109260</td>\n",
       "      <td>0.009079</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.076622</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.985897</td>\n",
       "      <td>0.013341</td>\n",
       "      <td>5.429727e-12</td>\n",
       "      <td>1.496944e-07</td>\n",
       "      <td>4.233210e-07</td>\n",
       "      <td>1.535554e-10</td>\n",
       "      <td>1.174871e-10</td>\n",
       "      <td>1.274067e-12</td>\n",
       "      <td>6.783629e-13</td>\n",
       "      <td>7.623699e-08</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>3.216478e-09</td>\n",
       "      <td>5.155637e-13</td>\n",
       "      <td>7.990734e-10</td>\n",
       "      <td>1.821890e-08</td>\n",
       "      <td>8.297696e-09</td>\n",
       "      <td>9.532391e-09</td>\n",
       "      <td>6.062763e-13</td>\n",
       "      <td>4.525606e-10</td>\n",
       "      <td>0.187337</td>\n",
       "      <td>7.678191e-01</td>\n",
       "      <td>3.207933e-09</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.147073e-05</td>\n",
       "      <td>1.475435e-07</td>\n",
       "      <td>1.232974e-09</td>\n",
       "      <td>2.567746e-06</td>\n",
       "      <td>3.892232e-12</td>\n",
       "      <td>1.021793e-04</td>\n",
       "      <td>0.044670</td>\n",
       "      <td>8.229596e-07</td>\n",
       "      <td>4.422402e-13</td>\n",
       "      <td>1.417321e-09</td>\n",
       "      <td>1.225622e-05</td>\n",
       "      <td>1.733765e-05</td>\n",
       "      <td>7.913169e-11</td>\n",
       "      <td>8.105658e-08</td>\n",
       "      <td>1.764113e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.019979</td>\n",
       "      <td>0.010034</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.004741</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>0.016694</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.298494</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.624385</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.1492</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.3028</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.3492</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.077124</td>\n",
       "      <td>0.014179</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.017430</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.051605</td>\n",
       "      <td>0.003278</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.005994</td>\n",
       "      <td>0.462109</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.002051</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.344156</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.119137</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>3.136365e-08</td>\n",
       "      <td>7.199874e-05</td>\n",
       "      <td>6.545521e-05</td>\n",
       "      <td>7.743306e-09</td>\n",
       "      <td>1.623164e-11</td>\n",
       "      <td>5.751525e-10</td>\n",
       "      <td>1.068964e-13</td>\n",
       "      <td>2.886634e-08</td>\n",
       "      <td>0.875192</td>\n",
       "      <td>1.724513e-08</td>\n",
       "      <td>1.845764e-13</td>\n",
       "      <td>5.002106e-09</td>\n",
       "      <td>4.061707e-09</td>\n",
       "      <td>2.432308e-03</td>\n",
       "      <td>7.688006e-08</td>\n",
       "      <td>8.530826e-12</td>\n",
       "      <td>5.536405e-07</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>6.101326e-08</td>\n",
       "      <td>2.910954e-09</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>3.484668e-07</td>\n",
       "      <td>2.579309e-08</td>\n",
       "      <td>1.646639e-11</td>\n",
       "      <td>6.765297e-12</td>\n",
       "      <td>4.121616e-14</td>\n",
       "      <td>2.615919e-10</td>\n",
       "      <td>0.999772</td>\n",
       "      <td>4.373644e-10</td>\n",
       "      <td>4.195139e-14</td>\n",
       "      <td>1.118071e-11</td>\n",
       "      <td>1.429955e-08</td>\n",
       "      <td>3.131265e-07</td>\n",
       "      <td>1.082345e-13</td>\n",
       "      <td>1.314704e-11</td>\n",
       "      <td>8.264869e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fragment_id  0_proba_x  1_proba_x  2_proba_x  3_proba_x  4_proba_x  \\\n",
       "0            0   0.031669   0.441127   0.001782   0.175464   0.002389   \n",
       "1            1   0.008463   0.881487   0.000831   0.017043   0.001019   \n",
       "2            2   0.093019   0.292943   0.002693   0.094578   0.003387   \n",
       "3            3   0.533759   0.186844   0.003628   0.045731   0.003993   \n",
       "4            4   0.019979   0.010034   0.000516   0.004741   0.001214   \n",
       "\n",
       "   5_proba_x  6_proba_x  7_proba_x  8_proba_x  9_proba_x  10_proba_x  \\\n",
       "0   0.037579   0.063476   0.005356   0.002257   0.023875    0.175142   \n",
       "1   0.021602   0.032368   0.001473   0.000847   0.016896    0.002923   \n",
       "2   0.018731   0.081917   0.005751   0.002520   0.055956    0.206233   \n",
       "3   0.045217   0.053578   0.007821   0.002985   0.020327    0.027389   \n",
       "4   0.004336   0.016694   0.004563   0.000455   0.004997    0.298494   \n",
       "\n",
       "   11_proba_x  12_proba_x  13_proba_x  14_proba_x  15_proba_x  16_proba_x  \\\n",
       "0    0.012161    0.003027    0.001950    0.000838    0.015173    0.001460   \n",
       "1    0.004792    0.001404    0.000730    0.000418    0.004859    0.000707   \n",
       "2    0.030800    0.004864    0.003710    0.001461    0.090085    0.002187   \n",
       "3    0.009348    0.006114    0.003455    0.001628    0.037426    0.003093   \n",
       "4    0.001269    0.002567    0.001071    0.000752    0.624385    0.000455   \n",
       "\n",
       "   17_proba_x  18_proba_x  0_proba_y  1_proba_y  2_proba_y  3_proba_y  \\\n",
       "0    0.002880    0.002395     0.1024     0.3552     0.0002     0.0992   \n",
       "1    0.001239    0.000899     0.0168     0.8154     0.0002     0.0350   \n",
       "2    0.004168    0.004997     0.1746     0.1836     0.0006     0.1022   \n",
       "3    0.004004    0.003661     0.4816     0.0860     0.0006     0.0762   \n",
       "4    0.001615    0.001862     0.1492     0.0210     0.0008     0.0178   \n",
       "\n",
       "   4_proba_y  5_proba_y  6_proba_y  7_proba_y  8_proba_y  9_proba_y  \\\n",
       "0     0.0002     0.0316     0.1304     0.0116     0.0034     0.0412   \n",
       "1     0.0000     0.0498     0.0418     0.0016     0.0006     0.0166   \n",
       "2     0.0010     0.0210     0.0746     0.0042     0.0024     0.0394   \n",
       "3     0.0018     0.0200     0.0470     0.0044     0.0004     0.0122   \n",
       "4     0.0134     0.0096     0.0350     0.0062     0.0004     0.0136   \n",
       "\n",
       "   10_proba_y  11_proba_y  12_proba_y  13_proba_y  14_proba_y  15_proba_y  \\\n",
       "0      0.1276      0.0190      0.0092      0.0002      0.0000      0.0512   \n",
       "1      0.0054      0.0076      0.0000      0.0006      0.0000      0.0074   \n",
       "2      0.1270      0.0432      0.0176      0.0018      0.0042      0.1600   \n",
       "3      0.0752      0.0190      0.0214      0.0014      0.0240      0.0978   \n",
       "4      0.3028      0.0082      0.0202      0.0052      0.0188      0.3492   \n",
       "\n",
       "   16_proba_y  17_proba_y  18_proba_y  0_proba_x  1_proba_x  2_proba_x  \\\n",
       "0       0.000      0.0162      0.0012        0.0        0.6        0.0   \n",
       "1       0.000      0.0012      0.0000        0.0        0.8        0.0   \n",
       "2       0.000      0.0328      0.0098        0.0        0.4        0.0   \n",
       "3       0.000      0.0294      0.0016        0.2        0.8        0.0   \n",
       "4       0.001      0.0186      0.0090        0.0        0.0        0.0   \n",
       "\n",
       "   3_proba_x  4_proba_x  5_proba_x  6_proba_x  7_proba_x  8_proba_x  \\\n",
       "0        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "1        0.2        0.0        0.0        0.0        0.0        0.0   \n",
       "2        0.0        0.0        0.0        0.2        0.0        0.0   \n",
       "3        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "4        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "   9_proba_x  10_proba_x  11_proba_x  12_proba_x  13_proba_x  14_proba_x  \\\n",
       "0        0.0         0.4         0.0         0.0         0.0         0.0   \n",
       "1        0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2        0.2         0.0         0.0         0.0         0.0         0.0   \n",
       "3        0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "4        0.0         0.4         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   15_proba_x  16_proba_x  17_proba_x  18_proba_x  0_proba_y  1_proba_y  \\\n",
       "0         0.0         0.0         0.0         0.0   0.020270   0.591118   \n",
       "1         0.0         0.0         0.0         0.0   0.005188   0.845728   \n",
       "2         0.0         0.0         0.2         0.0   0.062659   0.253161   \n",
       "3         0.0         0.0         0.0         0.0   0.447850   0.161704   \n",
       "4         0.6         0.0         0.0         0.0   0.077124   0.014179   \n",
       "\n",
       "   2_proba_y  3_proba_y  4_proba_y  5_proba_y  6_proba_y  7_proba_y  \\\n",
       "0   0.001390   0.076698   0.001630   0.013208   0.015505   0.022670   \n",
       "1   0.000357   0.038582   0.000395   0.014556   0.027319   0.000945   \n",
       "2   0.003676   0.216549   0.004394   0.026043   0.023008   0.014735   \n",
       "3   0.001688   0.075070   0.002133   0.056924   0.026587   0.004066   \n",
       "4   0.000845   0.017430   0.003509   0.005217   0.051605   0.003278   \n",
       "\n",
       "   8_proba_y  9_proba_y  10_proba_y  11_proba_y  12_proba_y  13_proba_y  \\\n",
       "0   0.001507   0.015394    0.148776    0.012079    0.002453    0.001991   \n",
       "1   0.000409   0.032406    0.004537    0.025768    0.000484    0.000544   \n",
       "2   0.004070   0.054802    0.149235    0.032038    0.006927    0.004962   \n",
       "3   0.001655   0.008121    0.109260    0.009079    0.006684    0.003642   \n",
       "4   0.000820   0.005994    0.462109    0.001720    0.002051    0.002199   \n",
       "\n",
       "   14_proba_y  15_proba_y  16_proba_y  17_proba_y  18_proba_y  0_proba_x  \\\n",
       "0    0.001300    0.067546    0.001159    0.004001    0.001304   0.481034   \n",
       "1    0.000331    0.000981    0.000295    0.000872    0.000305   0.573070   \n",
       "2    0.003788    0.057288    0.002972    0.075951    0.003743   0.806572   \n",
       "3    0.002312    0.076622    0.001384    0.003340    0.001880   0.985897   \n",
       "4    0.003200    0.344156    0.000824    0.002465    0.001275   0.119137   \n",
       "\n",
       "   1_proba_x     2_proba_x     3_proba_x     4_proba_x     5_proba_x  \\\n",
       "0   0.374553  6.250838e-06  8.488125e-02  5.771121e-06  3.099303e-04   \n",
       "1   0.079127  2.785844e-07  2.294374e-05  1.792608e-04  7.888665e-06   \n",
       "2   0.000636  2.580042e-06  6.057014e-02  1.033879e-04  2.398190e-04   \n",
       "3   0.013341  5.429727e-12  1.496944e-07  4.233210e-07  1.535554e-10   \n",
       "4   0.003101  3.136365e-08  7.199874e-05  6.545521e-05  7.743306e-09   \n",
       "\n",
       "      6_proba_x     7_proba_x     8_proba_x     9_proba_x  10_proba_x  \\\n",
       "0  1.461453e-04  3.566003e-07  1.441350e-08  1.626174e-03    0.049993   \n",
       "1  9.447433e-06  4.797030e-06  6.715825e-10  1.557029e-01    0.191702   \n",
       "2  2.423308e-07  3.745946e-06  6.820599e-07  4.296690e-03    0.085365   \n",
       "3  1.174871e-10  1.274067e-12  6.783629e-13  7.623699e-08    0.000761   \n",
       "4  1.623164e-11  5.751525e-10  1.068964e-13  2.886634e-08    0.875192   \n",
       "\n",
       "     11_proba_x    12_proba_x    13_proba_x    14_proba_x    15_proba_x  \\\n",
       "0  6.524021e-03  1.115529e-06  1.644272e-05  9.510262e-08  8.849970e-04   \n",
       "1  7.186552e-05  9.514194e-10  2.383866e-07  5.640025e-07  1.009518e-04   \n",
       "2  2.586398e-02  1.232927e-07  4.215697e-04  1.705838e-07  1.158001e-02   \n",
       "3  3.216478e-09  5.155637e-13  7.990734e-10  1.821890e-08  8.297696e-09   \n",
       "4  1.724513e-08  1.845764e-13  5.002106e-09  4.061707e-09  2.432308e-03   \n",
       "\n",
       "     16_proba_x    17_proba_x    18_proba_x  0_proba_y     1_proba_y  \\\n",
       "0  1.559794e-05  1.518542e-06  1.312220e-06   0.276344  4.974163e-03   \n",
       "1  6.820856e-08  5.948169e-07  1.545804e-08   0.000562  8.662991e-01   \n",
       "2  3.563197e-03  2.959465e-04  4.850493e-04   0.491533  2.232068e-03   \n",
       "3  9.532391e-09  6.062763e-13  4.525606e-10   0.187337  7.678191e-01   \n",
       "4  7.688006e-08  8.530826e-12  5.536405e-07   0.000226  6.101326e-08   \n",
       "\n",
       "      2_proba_y  3_proba_y     4_proba_y     5_proba_y     6_proba_y  \\\n",
       "0  3.196806e-07   0.000028  3.635841e-06  4.885335e-03  4.949490e-08   \n",
       "1  1.819706e-09   0.007660  1.195446e-07  4.581188e-06  4.465006e-05   \n",
       "2  1.448590e-05   0.000314  2.383762e-03  1.103980e-04  1.710348e-06   \n",
       "3  3.207933e-09   0.000027  1.147073e-05  1.475435e-07  1.232974e-09   \n",
       "4  2.910954e-09   0.000001  3.484668e-07  2.579309e-08  1.646639e-11   \n",
       "\n",
       "      7_proba_y     8_proba_y     9_proba_y  10_proba_y    11_proba_y  \\\n",
       "0  1.744127e-07  9.497017e-12  5.787967e-05    0.713638  4.377463e-06   \n",
       "1  3.523876e-06  7.000848e-10  1.082985e-01    0.000297  1.673466e-02   \n",
       "2  2.358376e-05  1.211643e-07  1.120005e-03    0.425814  4.230822e-04   \n",
       "3  2.567746e-06  3.892232e-12  1.021793e-04    0.044670  8.229596e-07   \n",
       "4  6.765297e-12  4.121616e-14  2.615919e-10    0.999772  4.373644e-10   \n",
       "\n",
       "     12_proba_y    13_proba_y    14_proba_y    15_proba_y    16_proba_y  \\\n",
       "0  1.541949e-07  3.370034e-05  2.815372e-07  2.940170e-05  4.646852e-07   \n",
       "1  1.676316e-08  1.725869e-06  1.140225e-05  7.543473e-05  3.884795e-07   \n",
       "2  3.531143e-05  2.149621e-04  2.756539e-03  7.284040e-02  7.481589e-06   \n",
       "3  4.422402e-13  1.417321e-09  1.225622e-05  1.733765e-05  7.913169e-11   \n",
       "4  4.195139e-14  1.118071e-11  1.429955e-08  3.131265e-07  1.082345e-13   \n",
       "\n",
       "     17_proba_y    18_proba_y  \n",
       "0  5.648534e-10  4.114973e-08  \n",
       "1  6.601894e-06  2.564135e-07  \n",
       "2  1.729156e-04  1.339113e-06  \n",
       "3  8.105658e-08  1.764113e-09  \n",
       "4  1.314704e-11  8.264869e-10  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T08:40:43.811540Z",
     "start_time": "2020-07-26T08:40:43.348568Z"
    }
   },
   "outputs": [],
   "source": [
    "oof_nn1_prob =pd.read_csv('oof_nn1_prob.csv')\n",
    "oof_nn1_prob_test =pd.read_csv('oof_nn1_prob_test.csv')\n",
    "oof_nn4_prob =pd.read_csv('oof_nn4_prob.csv')\n",
    "oof_nn4_prob_test =pd.read_csv('oof_nn4_prob_test.csv')\n",
    "oof_nn1_prob =pd.read_csv('oof_nn1_prob.csv')\n",
    "oof_nn1_prob_test =pd.read_csv('oof_nn1_prob_test.csv')\n",
    "oof_nn2_prob =pd.read_csv('oof_nn2_prob.csv')\n",
    "oof_nn2_prob_test =pd.read_csv('oof_nn2_prob_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T07:57:44.530592Z",
     "start_time": "2020-07-26T07:57:44.487739Z"
    }
   },
   "outputs": [],
   "source": [
    "data1 =pd.merge(data1,oof_nn1_prob,on=['fragment_id'])\n",
    "data1 =pd.merge(data1,oof_nn4_prob,on=['fragment_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T08:40:53.323359Z",
     "start_time": "2020-07-26T08:40:53.286480Z"
    }
   },
   "outputs": [],
   "source": [
    "data1 =pd.merge(data1,oof_nn2_prob,on=['fragment_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T08:43:56.347104Z",
     "start_time": "2020-07-26T08:43:56.253415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fragment_id</th>\n",
       "      <th>0_proba_x</th>\n",
       "      <th>1_proba_x</th>\n",
       "      <th>2_proba_x</th>\n",
       "      <th>3_proba_x</th>\n",
       "      <th>4_proba_x</th>\n",
       "      <th>5_proba_x</th>\n",
       "      <th>6_proba_x</th>\n",
       "      <th>7_proba_x</th>\n",
       "      <th>8_proba_x</th>\n",
       "      <th>9_proba_x</th>\n",
       "      <th>10_proba_x</th>\n",
       "      <th>11_proba_x</th>\n",
       "      <th>12_proba_x</th>\n",
       "      <th>13_proba_x</th>\n",
       "      <th>14_proba_x</th>\n",
       "      <th>15_proba_x</th>\n",
       "      <th>16_proba_x</th>\n",
       "      <th>17_proba_x</th>\n",
       "      <th>18_proba_x</th>\n",
       "      <th>0_proba_y</th>\n",
       "      <th>1_proba_y</th>\n",
       "      <th>2_proba_y</th>\n",
       "      <th>3_proba_y</th>\n",
       "      <th>4_proba_y</th>\n",
       "      <th>5_proba_y</th>\n",
       "      <th>6_proba_y</th>\n",
       "      <th>7_proba_y</th>\n",
       "      <th>8_proba_y</th>\n",
       "      <th>9_proba_y</th>\n",
       "      <th>10_proba_y</th>\n",
       "      <th>11_proba_y</th>\n",
       "      <th>12_proba_y</th>\n",
       "      <th>13_proba_y</th>\n",
       "      <th>14_proba_y</th>\n",
       "      <th>15_proba_y</th>\n",
       "      <th>16_proba_y</th>\n",
       "      <th>17_proba_y</th>\n",
       "      <th>18_proba_y</th>\n",
       "      <th>0_proba_x</th>\n",
       "      <th>1_proba_x</th>\n",
       "      <th>2_proba_x</th>\n",
       "      <th>3_proba_x</th>\n",
       "      <th>4_proba_x</th>\n",
       "      <th>5_proba_x</th>\n",
       "      <th>6_proba_x</th>\n",
       "      <th>7_proba_x</th>\n",
       "      <th>8_proba_x</th>\n",
       "      <th>9_proba_x</th>\n",
       "      <th>10_proba_x</th>\n",
       "      <th>11_proba_x</th>\n",
       "      <th>12_proba_x</th>\n",
       "      <th>13_proba_x</th>\n",
       "      <th>14_proba_x</th>\n",
       "      <th>15_proba_x</th>\n",
       "      <th>16_proba_x</th>\n",
       "      <th>17_proba_x</th>\n",
       "      <th>18_proba_x</th>\n",
       "      <th>0_proba_y</th>\n",
       "      <th>1_proba_y</th>\n",
       "      <th>2_proba_y</th>\n",
       "      <th>3_proba_y</th>\n",
       "      <th>4_proba_y</th>\n",
       "      <th>5_proba_y</th>\n",
       "      <th>6_proba_y</th>\n",
       "      <th>7_proba_y</th>\n",
       "      <th>8_proba_y</th>\n",
       "      <th>9_proba_y</th>\n",
       "      <th>10_proba_y</th>\n",
       "      <th>11_proba_y</th>\n",
       "      <th>12_proba_y</th>\n",
       "      <th>13_proba_y</th>\n",
       "      <th>14_proba_y</th>\n",
       "      <th>15_proba_y</th>\n",
       "      <th>16_proba_y</th>\n",
       "      <th>17_proba_y</th>\n",
       "      <th>18_proba_y</th>\n",
       "      <th>0_proba_x</th>\n",
       "      <th>1_proba_x</th>\n",
       "      <th>2_proba_x</th>\n",
       "      <th>3_proba_x</th>\n",
       "      <th>4_proba_x</th>\n",
       "      <th>5_proba_x</th>\n",
       "      <th>6_proba_x</th>\n",
       "      <th>7_proba_x</th>\n",
       "      <th>8_proba_x</th>\n",
       "      <th>9_proba_x</th>\n",
       "      <th>10_proba_x</th>\n",
       "      <th>11_proba_x</th>\n",
       "      <th>12_proba_x</th>\n",
       "      <th>13_proba_x</th>\n",
       "      <th>14_proba_x</th>\n",
       "      <th>15_proba_x</th>\n",
       "      <th>16_proba_x</th>\n",
       "      <th>17_proba_x</th>\n",
       "      <th>18_proba_x</th>\n",
       "      <th>0_proba_y</th>\n",
       "      <th>1_proba_y</th>\n",
       "      <th>2_proba_y</th>\n",
       "      <th>3_proba_y</th>\n",
       "      <th>4_proba_y</th>\n",
       "      <th>5_proba_y</th>\n",
       "      <th>6_proba_y</th>\n",
       "      <th>7_proba_y</th>\n",
       "      <th>8_proba_y</th>\n",
       "      <th>9_proba_y</th>\n",
       "      <th>10_proba_y</th>\n",
       "      <th>11_proba_y</th>\n",
       "      <th>12_proba_y</th>\n",
       "      <th>13_proba_y</th>\n",
       "      <th>14_proba_y</th>\n",
       "      <th>15_proba_y</th>\n",
       "      <th>16_proba_y</th>\n",
       "      <th>17_proba_y</th>\n",
       "      <th>18_proba_y</th>\n",
       "      <th>0_proba</th>\n",
       "      <th>1_proba</th>\n",
       "      <th>2_proba</th>\n",
       "      <th>3_proba</th>\n",
       "      <th>4_proba</th>\n",
       "      <th>5_proba</th>\n",
       "      <th>6_proba</th>\n",
       "      <th>7_proba</th>\n",
       "      <th>8_proba</th>\n",
       "      <th>9_proba</th>\n",
       "      <th>10_proba</th>\n",
       "      <th>11_proba</th>\n",
       "      <th>12_proba</th>\n",
       "      <th>13_proba</th>\n",
       "      <th>14_proba</th>\n",
       "      <th>15_proba</th>\n",
       "      <th>16_proba</th>\n",
       "      <th>17_proba</th>\n",
       "      <th>18_proba</th>\n",
       "      <th>behavior_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.031669</td>\n",
       "      <td>0.441127</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.175464</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.037579</td>\n",
       "      <td>0.063476</td>\n",
       "      <td>0.005356</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.023875</td>\n",
       "      <td>0.175142</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.003027</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.015173</td>\n",
       "      <td>0.001460</td>\n",
       "      <td>0.002880</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.3552</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0992</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0512</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020270</td>\n",
       "      <td>0.591118</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.076698</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.013208</td>\n",
       "      <td>0.015505</td>\n",
       "      <td>0.022670</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.015394</td>\n",
       "      <td>0.148776</td>\n",
       "      <td>0.012079</td>\n",
       "      <td>0.002453</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.067546</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.481034</td>\n",
       "      <td>0.374553</td>\n",
       "      <td>6.250838e-06</td>\n",
       "      <td>8.488125e-02</td>\n",
       "      <td>5.771121e-06</td>\n",
       "      <td>3.099303e-04</td>\n",
       "      <td>1.461453e-04</td>\n",
       "      <td>3.566003e-07</td>\n",
       "      <td>1.441350e-08</td>\n",
       "      <td>1.626174e-03</td>\n",
       "      <td>0.049993</td>\n",
       "      <td>6.524021e-03</td>\n",
       "      <td>1.115529e-06</td>\n",
       "      <td>1.644272e-05</td>\n",
       "      <td>9.510262e-08</td>\n",
       "      <td>8.849970e-04</td>\n",
       "      <td>1.559794e-05</td>\n",
       "      <td>1.518542e-06</td>\n",
       "      <td>1.312220e-06</td>\n",
       "      <td>0.276344</td>\n",
       "      <td>4.974163e-03</td>\n",
       "      <td>3.196806e-07</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>3.635841e-06</td>\n",
       "      <td>4.885335e-03</td>\n",
       "      <td>4.949490e-08</td>\n",
       "      <td>1.744127e-07</td>\n",
       "      <td>9.497017e-12</td>\n",
       "      <td>5.787967e-05</td>\n",
       "      <td>0.713638</td>\n",
       "      <td>4.377463e-06</td>\n",
       "      <td>1.541949e-07</td>\n",
       "      <td>3.370034e-05</td>\n",
       "      <td>2.815372e-07</td>\n",
       "      <td>2.940170e-05</td>\n",
       "      <td>4.646852e-07</td>\n",
       "      <td>5.648534e-10</td>\n",
       "      <td>4.114973e-08</td>\n",
       "      <td>0.943114</td>\n",
       "      <td>0.014947</td>\n",
       "      <td>2.832391e-07</td>\n",
       "      <td>3.168322e-02</td>\n",
       "      <td>8.313984e-07</td>\n",
       "      <td>4.897588e-04</td>\n",
       "      <td>2.101600e-08</td>\n",
       "      <td>5.589885e-07</td>\n",
       "      <td>1.111513e-09</td>\n",
       "      <td>5.037563e-03</td>\n",
       "      <td>4.688866e-03</td>\n",
       "      <td>2.013336e-05</td>\n",
       "      <td>2.677102e-10</td>\n",
       "      <td>1.379965e-06</td>\n",
       "      <td>6.209310e-07</td>\n",
       "      <td>1.455863e-05</td>\n",
       "      <td>1.057558e-07</td>\n",
       "      <td>4.065118e-07</td>\n",
       "      <td>5.204594e-07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.008463</td>\n",
       "      <td>0.881487</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.017043</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.021602</td>\n",
       "      <td>0.032368</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.016896</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.004859</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.8154</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0498</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>0.845728</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.038582</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.014556</td>\n",
       "      <td>0.027319</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.032406</td>\n",
       "      <td>0.004537</td>\n",
       "      <td>0.025768</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.573070</td>\n",
       "      <td>0.079127</td>\n",
       "      <td>2.785844e-07</td>\n",
       "      <td>2.294374e-05</td>\n",
       "      <td>1.792608e-04</td>\n",
       "      <td>7.888665e-06</td>\n",
       "      <td>9.447433e-06</td>\n",
       "      <td>4.797030e-06</td>\n",
       "      <td>6.715825e-10</td>\n",
       "      <td>1.557029e-01</td>\n",
       "      <td>0.191702</td>\n",
       "      <td>7.186552e-05</td>\n",
       "      <td>9.514194e-10</td>\n",
       "      <td>2.383866e-07</td>\n",
       "      <td>5.640025e-07</td>\n",
       "      <td>1.009518e-04</td>\n",
       "      <td>6.820856e-08</td>\n",
       "      <td>5.948169e-07</td>\n",
       "      <td>1.545804e-08</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>8.662991e-01</td>\n",
       "      <td>1.819706e-09</td>\n",
       "      <td>0.007660</td>\n",
       "      <td>1.195446e-07</td>\n",
       "      <td>4.581188e-06</td>\n",
       "      <td>4.465006e-05</td>\n",
       "      <td>3.523876e-06</td>\n",
       "      <td>7.000848e-10</td>\n",
       "      <td>1.082985e-01</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>1.673466e-02</td>\n",
       "      <td>1.676316e-08</td>\n",
       "      <td>1.725869e-06</td>\n",
       "      <td>1.140225e-05</td>\n",
       "      <td>7.543473e-05</td>\n",
       "      <td>3.884795e-07</td>\n",
       "      <td>6.601894e-06</td>\n",
       "      <td>2.564135e-07</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>0.968507</td>\n",
       "      <td>5.522135e-08</td>\n",
       "      <td>3.101615e-08</td>\n",
       "      <td>1.772393e-02</td>\n",
       "      <td>3.026936e-08</td>\n",
       "      <td>2.151280e-08</td>\n",
       "      <td>1.092184e-07</td>\n",
       "      <td>9.782048e-09</td>\n",
       "      <td>3.087210e-03</td>\n",
       "      <td>2.351299e-03</td>\n",
       "      <td>4.590864e-06</td>\n",
       "      <td>7.544784e-15</td>\n",
       "      <td>2.370737e-11</td>\n",
       "      <td>6.984537e-11</td>\n",
       "      <td>4.949271e-07</td>\n",
       "      <td>1.316929e-11</td>\n",
       "      <td>1.755843e-09</td>\n",
       "      <td>4.897525e-14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.093019</td>\n",
       "      <td>0.292943</td>\n",
       "      <td>0.002693</td>\n",
       "      <td>0.094578</td>\n",
       "      <td>0.003387</td>\n",
       "      <td>0.018731</td>\n",
       "      <td>0.081917</td>\n",
       "      <td>0.005751</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>0.055956</td>\n",
       "      <td>0.206233</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.004864</td>\n",
       "      <td>0.003710</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.090085</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.1746</td>\n",
       "      <td>0.1836</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.1022</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0746</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062659</td>\n",
       "      <td>0.253161</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>0.216549</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>0.026043</td>\n",
       "      <td>0.023008</td>\n",
       "      <td>0.014735</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>0.054802</td>\n",
       "      <td>0.149235</td>\n",
       "      <td>0.032038</td>\n",
       "      <td>0.006927</td>\n",
       "      <td>0.004962</td>\n",
       "      <td>0.003788</td>\n",
       "      <td>0.057288</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>0.075951</td>\n",
       "      <td>0.003743</td>\n",
       "      <td>0.806572</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>2.580042e-06</td>\n",
       "      <td>6.057014e-02</td>\n",
       "      <td>1.033879e-04</td>\n",
       "      <td>2.398190e-04</td>\n",
       "      <td>2.423308e-07</td>\n",
       "      <td>3.745946e-06</td>\n",
       "      <td>6.820599e-07</td>\n",
       "      <td>4.296690e-03</td>\n",
       "      <td>0.085365</td>\n",
       "      <td>2.586398e-02</td>\n",
       "      <td>1.232927e-07</td>\n",
       "      <td>4.215697e-04</td>\n",
       "      <td>1.705838e-07</td>\n",
       "      <td>1.158001e-02</td>\n",
       "      <td>3.563197e-03</td>\n",
       "      <td>2.959465e-04</td>\n",
       "      <td>4.850493e-04</td>\n",
       "      <td>0.491533</td>\n",
       "      <td>2.232068e-03</td>\n",
       "      <td>1.448590e-05</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>2.383762e-03</td>\n",
       "      <td>1.103980e-04</td>\n",
       "      <td>1.710348e-06</td>\n",
       "      <td>2.358376e-05</td>\n",
       "      <td>1.211643e-07</td>\n",
       "      <td>1.120005e-03</td>\n",
       "      <td>0.425814</td>\n",
       "      <td>4.230822e-04</td>\n",
       "      <td>3.531143e-05</td>\n",
       "      <td>2.149621e-04</td>\n",
       "      <td>2.756539e-03</td>\n",
       "      <td>7.284040e-02</td>\n",
       "      <td>7.481589e-06</td>\n",
       "      <td>1.729156e-04</td>\n",
       "      <td>1.339113e-06</td>\n",
       "      <td>0.911892</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>6.129648e-06</td>\n",
       "      <td>6.423689e-02</td>\n",
       "      <td>5.044804e-06</td>\n",
       "      <td>4.790473e-05</td>\n",
       "      <td>4.772185e-08</td>\n",
       "      <td>9.070901e-08</td>\n",
       "      <td>8.025390e-08</td>\n",
       "      <td>1.027245e-04</td>\n",
       "      <td>3.157057e-04</td>\n",
       "      <td>3.155915e-03</td>\n",
       "      <td>2.675100e-08</td>\n",
       "      <td>3.449972e-08</td>\n",
       "      <td>3.318110e-08</td>\n",
       "      <td>2.009196e-02</td>\n",
       "      <td>7.357116e-07</td>\n",
       "      <td>1.261588e-04</td>\n",
       "      <td>5.562270e-08</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.533759</td>\n",
       "      <td>0.186844</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>0.045731</td>\n",
       "      <td>0.003993</td>\n",
       "      <td>0.045217</td>\n",
       "      <td>0.053578</td>\n",
       "      <td>0.007821</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.020327</td>\n",
       "      <td>0.027389</td>\n",
       "      <td>0.009348</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.003455</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.037426</td>\n",
       "      <td>0.003093</td>\n",
       "      <td>0.004004</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.4816</td>\n",
       "      <td>0.0860</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0470</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0752</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.0978</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447850</td>\n",
       "      <td>0.161704</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.075070</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.056924</td>\n",
       "      <td>0.026587</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>0.008121</td>\n",
       "      <td>0.109260</td>\n",
       "      <td>0.009079</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.076622</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.985897</td>\n",
       "      <td>0.013341</td>\n",
       "      <td>5.429727e-12</td>\n",
       "      <td>1.496944e-07</td>\n",
       "      <td>4.233210e-07</td>\n",
       "      <td>1.535554e-10</td>\n",
       "      <td>1.174871e-10</td>\n",
       "      <td>1.274067e-12</td>\n",
       "      <td>6.783629e-13</td>\n",
       "      <td>7.623699e-08</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>3.216478e-09</td>\n",
       "      <td>5.155637e-13</td>\n",
       "      <td>7.990734e-10</td>\n",
       "      <td>1.821890e-08</td>\n",
       "      <td>8.297696e-09</td>\n",
       "      <td>9.532391e-09</td>\n",
       "      <td>6.062763e-13</td>\n",
       "      <td>4.525606e-10</td>\n",
       "      <td>0.187337</td>\n",
       "      <td>7.678191e-01</td>\n",
       "      <td>3.207933e-09</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.147073e-05</td>\n",
       "      <td>1.475435e-07</td>\n",
       "      <td>1.232974e-09</td>\n",
       "      <td>2.567746e-06</td>\n",
       "      <td>3.892232e-12</td>\n",
       "      <td>1.021793e-04</td>\n",
       "      <td>0.044670</td>\n",
       "      <td>8.229596e-07</td>\n",
       "      <td>4.422402e-13</td>\n",
       "      <td>1.417321e-09</td>\n",
       "      <td>1.225622e-05</td>\n",
       "      <td>1.733765e-05</td>\n",
       "      <td>7.913169e-11</td>\n",
       "      <td>8.105658e-08</td>\n",
       "      <td>1.764113e-09</td>\n",
       "      <td>0.999362</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>1.819358e-14</td>\n",
       "      <td>5.291691e-09</td>\n",
       "      <td>9.966475e-09</td>\n",
       "      <td>6.542968e-14</td>\n",
       "      <td>3.064274e-16</td>\n",
       "      <td>6.200213e-12</td>\n",
       "      <td>2.166220e-13</td>\n",
       "      <td>1.913936e-10</td>\n",
       "      <td>2.869724e-07</td>\n",
       "      <td>8.110437e-12</td>\n",
       "      <td>6.669734e-19</td>\n",
       "      <td>2.637020e-15</td>\n",
       "      <td>5.307910e-11</td>\n",
       "      <td>7.321822e-11</td>\n",
       "      <td>5.662729e-13</td>\n",
       "      <td>1.543808e-13</td>\n",
       "      <td>5.968376e-16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.019979</td>\n",
       "      <td>0.010034</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.004741</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>0.016694</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.298494</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.624385</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.1492</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.3028</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.3492</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.077124</td>\n",
       "      <td>0.014179</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.017430</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.051605</td>\n",
       "      <td>0.003278</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.005994</td>\n",
       "      <td>0.462109</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.002051</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.344156</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.119137</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>3.136365e-08</td>\n",
       "      <td>7.199874e-05</td>\n",
       "      <td>6.545521e-05</td>\n",
       "      <td>7.743306e-09</td>\n",
       "      <td>1.623164e-11</td>\n",
       "      <td>5.751525e-10</td>\n",
       "      <td>1.068964e-13</td>\n",
       "      <td>2.886634e-08</td>\n",
       "      <td>0.875192</td>\n",
       "      <td>1.724513e-08</td>\n",
       "      <td>1.845764e-13</td>\n",
       "      <td>5.002106e-09</td>\n",
       "      <td>4.061707e-09</td>\n",
       "      <td>2.432308e-03</td>\n",
       "      <td>7.688006e-08</td>\n",
       "      <td>8.530826e-12</td>\n",
       "      <td>5.536405e-07</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>6.101326e-08</td>\n",
       "      <td>2.910954e-09</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>3.484668e-07</td>\n",
       "      <td>2.579309e-08</td>\n",
       "      <td>1.646639e-11</td>\n",
       "      <td>6.765297e-12</td>\n",
       "      <td>4.121616e-14</td>\n",
       "      <td>2.615919e-10</td>\n",
       "      <td>0.999772</td>\n",
       "      <td>4.373644e-10</td>\n",
       "      <td>4.195139e-14</td>\n",
       "      <td>1.118071e-11</td>\n",
       "      <td>1.429955e-08</td>\n",
       "      <td>3.131265e-07</td>\n",
       "      <td>1.082345e-13</td>\n",
       "      <td>1.314704e-11</td>\n",
       "      <td>8.264869e-10</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>9.241681e-10</td>\n",
       "      <td>2.532094e-07</td>\n",
       "      <td>1.995125e-06</td>\n",
       "      <td>2.685778e-09</td>\n",
       "      <td>4.976275e-06</td>\n",
       "      <td>6.186190e-08</td>\n",
       "      <td>1.725046e-09</td>\n",
       "      <td>2.811346e-08</td>\n",
       "      <td>9.118110e-01</td>\n",
       "      <td>8.716271e-09</td>\n",
       "      <td>2.307553e-12</td>\n",
       "      <td>6.871327e-08</td>\n",
       "      <td>1.848035e-06</td>\n",
       "      <td>7.907541e-02</td>\n",
       "      <td>1.134578e-07</td>\n",
       "      <td>6.290770e-11</td>\n",
       "      <td>2.722544e-11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fragment_id  0_proba_x  1_proba_x  2_proba_x  3_proba_x  4_proba_x  \\\n",
       "0            0   0.031669   0.441127   0.001782   0.175464   0.002389   \n",
       "1            1   0.008463   0.881487   0.000831   0.017043   0.001019   \n",
       "2            2   0.093019   0.292943   0.002693   0.094578   0.003387   \n",
       "3            3   0.533759   0.186844   0.003628   0.045731   0.003993   \n",
       "4            4   0.019979   0.010034   0.000516   0.004741   0.001214   \n",
       "\n",
       "   5_proba_x  6_proba_x  7_proba_x  8_proba_x  9_proba_x  10_proba_x  \\\n",
       "0   0.037579   0.063476   0.005356   0.002257   0.023875    0.175142   \n",
       "1   0.021602   0.032368   0.001473   0.000847   0.016896    0.002923   \n",
       "2   0.018731   0.081917   0.005751   0.002520   0.055956    0.206233   \n",
       "3   0.045217   0.053578   0.007821   0.002985   0.020327    0.027389   \n",
       "4   0.004336   0.016694   0.004563   0.000455   0.004997    0.298494   \n",
       "\n",
       "   11_proba_x  12_proba_x  13_proba_x  14_proba_x  15_proba_x  16_proba_x  \\\n",
       "0    0.012161    0.003027    0.001950    0.000838    0.015173    0.001460   \n",
       "1    0.004792    0.001404    0.000730    0.000418    0.004859    0.000707   \n",
       "2    0.030800    0.004864    0.003710    0.001461    0.090085    0.002187   \n",
       "3    0.009348    0.006114    0.003455    0.001628    0.037426    0.003093   \n",
       "4    0.001269    0.002567    0.001071    0.000752    0.624385    0.000455   \n",
       "\n",
       "   17_proba_x  18_proba_x  0_proba_y  1_proba_y  2_proba_y  3_proba_y  \\\n",
       "0    0.002880    0.002395     0.1024     0.3552     0.0002     0.0992   \n",
       "1    0.001239    0.000899     0.0168     0.8154     0.0002     0.0350   \n",
       "2    0.004168    0.004997     0.1746     0.1836     0.0006     0.1022   \n",
       "3    0.004004    0.003661     0.4816     0.0860     0.0006     0.0762   \n",
       "4    0.001615    0.001862     0.1492     0.0210     0.0008     0.0178   \n",
       "\n",
       "   4_proba_y  5_proba_y  6_proba_y  7_proba_y  8_proba_y  9_proba_y  \\\n",
       "0     0.0002     0.0316     0.1304     0.0116     0.0034     0.0412   \n",
       "1     0.0000     0.0498     0.0418     0.0016     0.0006     0.0166   \n",
       "2     0.0010     0.0210     0.0746     0.0042     0.0024     0.0394   \n",
       "3     0.0018     0.0200     0.0470     0.0044     0.0004     0.0122   \n",
       "4     0.0134     0.0096     0.0350     0.0062     0.0004     0.0136   \n",
       "\n",
       "   10_proba_y  11_proba_y  12_proba_y  13_proba_y  14_proba_y  15_proba_y  \\\n",
       "0      0.1276      0.0190      0.0092      0.0002      0.0000      0.0512   \n",
       "1      0.0054      0.0076      0.0000      0.0006      0.0000      0.0074   \n",
       "2      0.1270      0.0432      0.0176      0.0018      0.0042      0.1600   \n",
       "3      0.0752      0.0190      0.0214      0.0014      0.0240      0.0978   \n",
       "4      0.3028      0.0082      0.0202      0.0052      0.0188      0.3492   \n",
       "\n",
       "   16_proba_y  17_proba_y  18_proba_y  0_proba_x  1_proba_x  2_proba_x  \\\n",
       "0       0.000      0.0162      0.0012        0.0        0.6        0.0   \n",
       "1       0.000      0.0012      0.0000        0.0        0.8        0.0   \n",
       "2       0.000      0.0328      0.0098        0.0        0.4        0.0   \n",
       "3       0.000      0.0294      0.0016        0.2        0.8        0.0   \n",
       "4       0.001      0.0186      0.0090        0.0        0.0        0.0   \n",
       "\n",
       "   3_proba_x  4_proba_x  5_proba_x  6_proba_x  7_proba_x  8_proba_x  \\\n",
       "0        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "1        0.2        0.0        0.0        0.0        0.0        0.0   \n",
       "2        0.0        0.0        0.0        0.2        0.0        0.0   \n",
       "3        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "4        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "   9_proba_x  10_proba_x  11_proba_x  12_proba_x  13_proba_x  14_proba_x  \\\n",
       "0        0.0         0.4         0.0         0.0         0.0         0.0   \n",
       "1        0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2        0.2         0.0         0.0         0.0         0.0         0.0   \n",
       "3        0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "4        0.0         0.4         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   15_proba_x  16_proba_x  17_proba_x  18_proba_x  0_proba_y  1_proba_y  \\\n",
       "0         0.0         0.0         0.0         0.0   0.020270   0.591118   \n",
       "1         0.0         0.0         0.0         0.0   0.005188   0.845728   \n",
       "2         0.0         0.0         0.2         0.0   0.062659   0.253161   \n",
       "3         0.0         0.0         0.0         0.0   0.447850   0.161704   \n",
       "4         0.6         0.0         0.0         0.0   0.077124   0.014179   \n",
       "\n",
       "   2_proba_y  3_proba_y  4_proba_y  5_proba_y  6_proba_y  7_proba_y  \\\n",
       "0   0.001390   0.076698   0.001630   0.013208   0.015505   0.022670   \n",
       "1   0.000357   0.038582   0.000395   0.014556   0.027319   0.000945   \n",
       "2   0.003676   0.216549   0.004394   0.026043   0.023008   0.014735   \n",
       "3   0.001688   0.075070   0.002133   0.056924   0.026587   0.004066   \n",
       "4   0.000845   0.017430   0.003509   0.005217   0.051605   0.003278   \n",
       "\n",
       "   8_proba_y  9_proba_y  10_proba_y  11_proba_y  12_proba_y  13_proba_y  \\\n",
       "0   0.001507   0.015394    0.148776    0.012079    0.002453    0.001991   \n",
       "1   0.000409   0.032406    0.004537    0.025768    0.000484    0.000544   \n",
       "2   0.004070   0.054802    0.149235    0.032038    0.006927    0.004962   \n",
       "3   0.001655   0.008121    0.109260    0.009079    0.006684    0.003642   \n",
       "4   0.000820   0.005994    0.462109    0.001720    0.002051    0.002199   \n",
       "\n",
       "   14_proba_y  15_proba_y  16_proba_y  17_proba_y  18_proba_y  0_proba_x  \\\n",
       "0    0.001300    0.067546    0.001159    0.004001    0.001304   0.481034   \n",
       "1    0.000331    0.000981    0.000295    0.000872    0.000305   0.573070   \n",
       "2    0.003788    0.057288    0.002972    0.075951    0.003743   0.806572   \n",
       "3    0.002312    0.076622    0.001384    0.003340    0.001880   0.985897   \n",
       "4    0.003200    0.344156    0.000824    0.002465    0.001275   0.119137   \n",
       "\n",
       "   1_proba_x     2_proba_x     3_proba_x     4_proba_x     5_proba_x  \\\n",
       "0   0.374553  6.250838e-06  8.488125e-02  5.771121e-06  3.099303e-04   \n",
       "1   0.079127  2.785844e-07  2.294374e-05  1.792608e-04  7.888665e-06   \n",
       "2   0.000636  2.580042e-06  6.057014e-02  1.033879e-04  2.398190e-04   \n",
       "3   0.013341  5.429727e-12  1.496944e-07  4.233210e-07  1.535554e-10   \n",
       "4   0.003101  3.136365e-08  7.199874e-05  6.545521e-05  7.743306e-09   \n",
       "\n",
       "      6_proba_x     7_proba_x     8_proba_x     9_proba_x  10_proba_x  \\\n",
       "0  1.461453e-04  3.566003e-07  1.441350e-08  1.626174e-03    0.049993   \n",
       "1  9.447433e-06  4.797030e-06  6.715825e-10  1.557029e-01    0.191702   \n",
       "2  2.423308e-07  3.745946e-06  6.820599e-07  4.296690e-03    0.085365   \n",
       "3  1.174871e-10  1.274067e-12  6.783629e-13  7.623699e-08    0.000761   \n",
       "4  1.623164e-11  5.751525e-10  1.068964e-13  2.886634e-08    0.875192   \n",
       "\n",
       "     11_proba_x    12_proba_x    13_proba_x    14_proba_x    15_proba_x  \\\n",
       "0  6.524021e-03  1.115529e-06  1.644272e-05  9.510262e-08  8.849970e-04   \n",
       "1  7.186552e-05  9.514194e-10  2.383866e-07  5.640025e-07  1.009518e-04   \n",
       "2  2.586398e-02  1.232927e-07  4.215697e-04  1.705838e-07  1.158001e-02   \n",
       "3  3.216478e-09  5.155637e-13  7.990734e-10  1.821890e-08  8.297696e-09   \n",
       "4  1.724513e-08  1.845764e-13  5.002106e-09  4.061707e-09  2.432308e-03   \n",
       "\n",
       "     16_proba_x    17_proba_x    18_proba_x  0_proba_y     1_proba_y  \\\n",
       "0  1.559794e-05  1.518542e-06  1.312220e-06   0.276344  4.974163e-03   \n",
       "1  6.820856e-08  5.948169e-07  1.545804e-08   0.000562  8.662991e-01   \n",
       "2  3.563197e-03  2.959465e-04  4.850493e-04   0.491533  2.232068e-03   \n",
       "3  9.532391e-09  6.062763e-13  4.525606e-10   0.187337  7.678191e-01   \n",
       "4  7.688006e-08  8.530826e-12  5.536405e-07   0.000226  6.101326e-08   \n",
       "\n",
       "      2_proba_y  3_proba_y     4_proba_y     5_proba_y     6_proba_y  \\\n",
       "0  3.196806e-07   0.000028  3.635841e-06  4.885335e-03  4.949490e-08   \n",
       "1  1.819706e-09   0.007660  1.195446e-07  4.581188e-06  4.465006e-05   \n",
       "2  1.448590e-05   0.000314  2.383762e-03  1.103980e-04  1.710348e-06   \n",
       "3  3.207933e-09   0.000027  1.147073e-05  1.475435e-07  1.232974e-09   \n",
       "4  2.910954e-09   0.000001  3.484668e-07  2.579309e-08  1.646639e-11   \n",
       "\n",
       "      7_proba_y     8_proba_y     9_proba_y  10_proba_y    11_proba_y  \\\n",
       "0  1.744127e-07  9.497017e-12  5.787967e-05    0.713638  4.377463e-06   \n",
       "1  3.523876e-06  7.000848e-10  1.082985e-01    0.000297  1.673466e-02   \n",
       "2  2.358376e-05  1.211643e-07  1.120005e-03    0.425814  4.230822e-04   \n",
       "3  2.567746e-06  3.892232e-12  1.021793e-04    0.044670  8.229596e-07   \n",
       "4  6.765297e-12  4.121616e-14  2.615919e-10    0.999772  4.373644e-10   \n",
       "\n",
       "     12_proba_y    13_proba_y    14_proba_y    15_proba_y    16_proba_y  \\\n",
       "0  1.541949e-07  3.370034e-05  2.815372e-07  2.940170e-05  4.646852e-07   \n",
       "1  1.676316e-08  1.725869e-06  1.140225e-05  7.543473e-05  3.884795e-07   \n",
       "2  3.531143e-05  2.149621e-04  2.756539e-03  7.284040e-02  7.481589e-06   \n",
       "3  4.422402e-13  1.417321e-09  1.225622e-05  1.733765e-05  7.913169e-11   \n",
       "4  4.195139e-14  1.118071e-11  1.429955e-08  3.131265e-07  1.082345e-13   \n",
       "\n",
       "     17_proba_y    18_proba_y   0_proba   1_proba       2_proba       3_proba  \\\n",
       "0  5.648534e-10  4.114973e-08  0.943114  0.014947  2.832391e-07  3.168322e-02   \n",
       "1  6.601894e-06  2.564135e-07  0.008326  0.968507  5.522135e-08  3.101615e-08   \n",
       "2  1.729156e-04  1.339113e-06  0.911892  0.000019  6.129648e-06  6.423689e-02   \n",
       "3  8.105658e-08  1.764113e-09  0.999362  0.000638  1.819358e-14  5.291691e-09   \n",
       "4  1.314704e-11  8.264869e-10  0.009052  0.000052  9.241681e-10  2.532094e-07   \n",
       "\n",
       "        4_proba       5_proba       6_proba       7_proba       8_proba  \\\n",
       "0  8.313984e-07  4.897588e-04  2.101600e-08  5.589885e-07  1.111513e-09   \n",
       "1  1.772393e-02  3.026936e-08  2.151280e-08  1.092184e-07  9.782048e-09   \n",
       "2  5.044804e-06  4.790473e-05  4.772185e-08  9.070901e-08  8.025390e-08   \n",
       "3  9.966475e-09  6.542968e-14  3.064274e-16  6.200213e-12  2.166220e-13   \n",
       "4  1.995125e-06  2.685778e-09  4.976275e-06  6.186190e-08  1.725046e-09   \n",
       "\n",
       "        9_proba      10_proba      11_proba      12_proba      13_proba  \\\n",
       "0  5.037563e-03  4.688866e-03  2.013336e-05  2.677102e-10  1.379965e-06   \n",
       "1  3.087210e-03  2.351299e-03  4.590864e-06  7.544784e-15  2.370737e-11   \n",
       "2  1.027245e-04  3.157057e-04  3.155915e-03  2.675100e-08  3.449972e-08   \n",
       "3  1.913936e-10  2.869724e-07  8.110437e-12  6.669734e-19  2.637020e-15   \n",
       "4  2.811346e-08  9.118110e-01  8.716271e-09  2.307553e-12  6.871327e-08   \n",
       "\n",
       "       14_proba      15_proba      16_proba      17_proba      18_proba  \\\n",
       "0  6.209310e-07  1.455863e-05  1.057558e-07  4.065118e-07  5.204594e-07   \n",
       "1  6.984537e-11  4.949271e-07  1.316929e-11  1.755843e-09  4.897525e-14   \n",
       "2  3.318110e-08  2.009196e-02  7.357116e-07  1.261588e-04  5.562270e-08   \n",
       "3  5.307910e-11  7.321822e-11  5.662729e-13  1.543808e-13  5.968376e-16   \n",
       "4  1.848035e-06  7.907541e-02  1.134578e-07  6.290770e-11  2.722544e-11   \n",
       "\n",
       "   behavior_id  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          0.0  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T08:42:04.675639Z",
     "start_time": "2020-07-26T08:42:04.632944Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_x_min</th>\n",
       "      <th>acc_x_max</th>\n",
       "      <th>acc_x_mean</th>\n",
       "      <th>acc_x_median</th>\n",
       "      <th>acc_x_std</th>\n",
       "      <th>acc_x_skew</th>\n",
       "      <th>acc_y_min</th>\n",
       "      <th>acc_y_max</th>\n",
       "      <th>acc_y_mean</th>\n",
       "      <th>acc_y_median</th>\n",
       "      <th>acc_y_std</th>\n",
       "      <th>acc_y_skew</th>\n",
       "      <th>acc_z_min</th>\n",
       "      <th>acc_z_max</th>\n",
       "      <th>acc_z_mean</th>\n",
       "      <th>acc_z_median</th>\n",
       "      <th>acc_z_std</th>\n",
       "      <th>acc_z_skew</th>\n",
       "      <th>acc_xg_min</th>\n",
       "      <th>acc_xg_max</th>\n",
       "      <th>acc_xg_mean</th>\n",
       "      <th>acc_xg_median</th>\n",
       "      <th>acc_xg_std</th>\n",
       "      <th>acc_xg_skew</th>\n",
       "      <th>acc_yg_min</th>\n",
       "      <th>acc_yg_max</th>\n",
       "      <th>acc_yg_mean</th>\n",
       "      <th>acc_yg_median</th>\n",
       "      <th>acc_yg_std</th>\n",
       "      <th>acc_yg_skew</th>\n",
       "      <th>acc_zg_min</th>\n",
       "      <th>acc_zg_max</th>\n",
       "      <th>acc_zg_mean</th>\n",
       "      <th>acc_zg_median</th>\n",
       "      <th>acc_zg_std</th>\n",
       "      <th>acc_zg_skew</th>\n",
       "      <th>acc_min</th>\n",
       "      <th>acc_max</th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>acc_median</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>acc_skew</th>\n",
       "      <th>accg_min</th>\n",
       "      <th>accg_max</th>\n",
       "      <th>accg_mean</th>\n",
       "      <th>accg_median</th>\n",
       "      <th>accg_std</th>\n",
       "      <th>accg_skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.011594</td>\n",
       "      <td>0.413926</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>0.039158</td>\n",
       "      <td>0.401678</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.390096</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.023771</td>\n",
       "      <td>0.467862</td>\n",
       "      <td>0.929134</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>0.592247</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.069515</td>\n",
       "      <td>0.510347</td>\n",
       "      <td>0.659933</td>\n",
       "      <td>0.255155</td>\n",
       "      <td>0.515841</td>\n",
       "      <td>0.518135</td>\n",
       "      <td>0.029712</td>\n",
       "      <td>0.438977</td>\n",
       "      <td>0.800752</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.722787</td>\n",
       "      <td>0.719780</td>\n",
       "      <td>0.025638</td>\n",
       "      <td>0.518347</td>\n",
       "      <td>0.860963</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.862863</td>\n",
       "      <td>0.898955</td>\n",
       "      <td>0.059745</td>\n",
       "      <td>0.495878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027435</td>\n",
       "      <td>0.052375</td>\n",
       "      <td>0.043145</td>\n",
       "      <td>0.047137</td>\n",
       "      <td>0.248385</td>\n",
       "      <td>0.843662</td>\n",
       "      <td>0.028872</td>\n",
       "      <td>0.190793</td>\n",
       "      <td>0.340451</td>\n",
       "      <td>0.056427</td>\n",
       "      <td>0.496838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.005797</td>\n",
       "      <td>0.405896</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>0.022183</td>\n",
       "      <td>0.412803</td>\n",
       "      <td>0.993197</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.396795</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.016324</td>\n",
       "      <td>0.528035</td>\n",
       "      <td>0.952756</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.621546</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.043564</td>\n",
       "      <td>0.539036</td>\n",
       "      <td>0.680135</td>\n",
       "      <td>0.247423</td>\n",
       "      <td>0.515912</td>\n",
       "      <td>0.518135</td>\n",
       "      <td>0.018729</td>\n",
       "      <td>0.484943</td>\n",
       "      <td>0.808271</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.725878</td>\n",
       "      <td>0.719780</td>\n",
       "      <td>0.019945</td>\n",
       "      <td>0.526566</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.310096</td>\n",
       "      <td>0.863612</td>\n",
       "      <td>0.898955</td>\n",
       "      <td>0.042599</td>\n",
       "      <td>0.617168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020538</td>\n",
       "      <td>0.034727</td>\n",
       "      <td>0.027287</td>\n",
       "      <td>0.030781</td>\n",
       "      <td>0.311711</td>\n",
       "      <td>0.931695</td>\n",
       "      <td>0.027672</td>\n",
       "      <td>0.211447</td>\n",
       "      <td>0.342910</td>\n",
       "      <td>0.038872</td>\n",
       "      <td>0.622006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.020290</td>\n",
       "      <td>0.404836</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>0.048338</td>\n",
       "      <td>0.531610</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.389272</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.038021</td>\n",
       "      <td>0.394404</td>\n",
       "      <td>0.889764</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.600464</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.093684</td>\n",
       "      <td>0.476425</td>\n",
       "      <td>0.673401</td>\n",
       "      <td>0.268041</td>\n",
       "      <td>0.521830</td>\n",
       "      <td>0.518135</td>\n",
       "      <td>0.050443</td>\n",
       "      <td>0.553230</td>\n",
       "      <td>0.800752</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.732549</td>\n",
       "      <td>0.725275</td>\n",
       "      <td>0.037898</td>\n",
       "      <td>0.494730</td>\n",
       "      <td>0.860963</td>\n",
       "      <td>0.319712</td>\n",
       "      <td>0.855009</td>\n",
       "      <td>0.891986</td>\n",
       "      <td>0.076404</td>\n",
       "      <td>0.542749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043521</td>\n",
       "      <td>0.067668</td>\n",
       "      <td>0.047263</td>\n",
       "      <td>0.066111</td>\n",
       "      <td>0.346175</td>\n",
       "      <td>0.864244</td>\n",
       "      <td>0.036568</td>\n",
       "      <td>0.194364</td>\n",
       "      <td>0.363272</td>\n",
       "      <td>0.068343</td>\n",
       "      <td>0.530991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.415161</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>0.036107</td>\n",
       "      <td>0.504666</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.396056</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.033806</td>\n",
       "      <td>0.572743</td>\n",
       "      <td>0.929134</td>\n",
       "      <td>0.060284</td>\n",
       "      <td>0.627781</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.117153</td>\n",
       "      <td>0.628434</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>0.252577</td>\n",
       "      <td>0.502066</td>\n",
       "      <td>0.502591</td>\n",
       "      <td>0.047466</td>\n",
       "      <td>0.461912</td>\n",
       "      <td>0.804511</td>\n",
       "      <td>0.570048</td>\n",
       "      <td>0.737811</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.031685</td>\n",
       "      <td>0.496472</td>\n",
       "      <td>0.877005</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.851240</td>\n",
       "      <td>0.885017</td>\n",
       "      <td>0.073086</td>\n",
       "      <td>0.584750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051527</td>\n",
       "      <td>0.074492</td>\n",
       "      <td>0.054574</td>\n",
       "      <td>0.077935</td>\n",
       "      <td>0.356956</td>\n",
       "      <td>0.864739</td>\n",
       "      <td>0.029621</td>\n",
       "      <td>0.180501</td>\n",
       "      <td>0.336686</td>\n",
       "      <td>0.069349</td>\n",
       "      <td>0.559849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.026087</td>\n",
       "      <td>0.415161</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>0.068191</td>\n",
       "      <td>0.450772</td>\n",
       "      <td>0.965986</td>\n",
       "      <td>0.051852</td>\n",
       "      <td>0.391361</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.053467</td>\n",
       "      <td>0.529983</td>\n",
       "      <td>0.858268</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.600103</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.166556</td>\n",
       "      <td>0.529148</td>\n",
       "      <td>0.649832</td>\n",
       "      <td>0.260309</td>\n",
       "      <td>0.512318</td>\n",
       "      <td>0.518135</td>\n",
       "      <td>0.062259</td>\n",
       "      <td>0.489646</td>\n",
       "      <td>0.774436</td>\n",
       "      <td>0.560386</td>\n",
       "      <td>0.715791</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.047943</td>\n",
       "      <td>0.475288</td>\n",
       "      <td>0.786096</td>\n",
       "      <td>0.334135</td>\n",
       "      <td>0.866266</td>\n",
       "      <td>0.905923</td>\n",
       "      <td>0.132888</td>\n",
       "      <td>0.505414</td>\n",
       "      <td>0.084365</td>\n",
       "      <td>0.054263</td>\n",
       "      <td>0.124296</td>\n",
       "      <td>0.105683</td>\n",
       "      <td>0.092122</td>\n",
       "      <td>0.247648</td>\n",
       "      <td>0.758629</td>\n",
       "      <td>0.057496</td>\n",
       "      <td>0.187294</td>\n",
       "      <td>0.359235</td>\n",
       "      <td>0.120903</td>\n",
       "      <td>0.542701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acc_x_min  acc_x_max  acc_x_mean  acc_x_median  acc_x_std  acc_x_skew  \\\n",
       "0   0.920000   0.011594    0.413926      0.542373   0.039158    0.401678   \n",
       "1   0.946667   0.005797    0.405896      0.542373   0.022183    0.412803   \n",
       "2   0.946667   0.020290    0.404836      0.542373   0.048338    0.531610   \n",
       "3   0.953333   0.014493    0.415161      0.542373   0.036107    0.504666   \n",
       "4   0.893333   0.026087    0.415161      0.542373   0.068191    0.450772   \n",
       "\n",
       "   acc_y_min  acc_y_max  acc_y_mean  acc_y_median  acc_y_std  acc_y_skew  \\\n",
       "0   0.979592   0.022222    0.390096      0.294118   0.023771    0.467862   \n",
       "1   0.993197   0.022222    0.396795      0.294118   0.016324    0.528035   \n",
       "2   0.959184   0.022222    0.389272      0.294118   0.038021    0.394404   \n",
       "3   0.979592   0.037037    0.396056      0.294118   0.033806    0.572743   \n",
       "4   0.965986   0.051852    0.391361      0.294118   0.053467    0.529983   \n",
       "\n",
       "   acc_z_min  acc_z_max  acc_z_mean  acc_z_median  acc_z_std  acc_z_skew  \\\n",
       "0   0.929134   0.024823    0.592247          0.60   0.069515    0.510347   \n",
       "1   0.952756   0.021277    0.621546          0.65   0.043564    0.539036   \n",
       "2   0.889764   0.042553    0.600464          0.60   0.093684    0.476425   \n",
       "3   0.929134   0.060284    0.627781          0.60   0.117153    0.628434   \n",
       "4   0.858268   0.063830    0.600103          0.65   0.166556    0.529148   \n",
       "\n",
       "   acc_xg_min  acc_xg_max  acc_xg_mean  acc_xg_median  acc_xg_std  \\\n",
       "0    0.659933    0.255155     0.515841       0.518135    0.029712   \n",
       "1    0.680135    0.247423     0.515912       0.518135    0.018729   \n",
       "2    0.673401    0.268041     0.521830       0.518135    0.050443   \n",
       "3    0.646465    0.252577     0.502066       0.502591    0.047466   \n",
       "4    0.649832    0.260309     0.512318       0.518135    0.062259   \n",
       "\n",
       "   acc_xg_skew  acc_yg_min  acc_yg_max  acc_yg_mean  acc_yg_median  \\\n",
       "0     0.438977    0.800752    0.555556     0.722787       0.719780   \n",
       "1     0.484943    0.808271    0.550725     0.725878       0.719780   \n",
       "2     0.553230    0.800752    0.565217     0.732549       0.725275   \n",
       "3     0.461912    0.804511    0.570048     0.737811       0.730769   \n",
       "4     0.489646    0.774436    0.560386     0.715791       0.714286   \n",
       "\n",
       "   acc_yg_std  acc_yg_skew  acc_zg_min  acc_zg_max  acc_zg_mean  \\\n",
       "0    0.025638     0.518347    0.860963    0.307692     0.862863   \n",
       "1    0.019945     0.526566    0.909091    0.310096     0.863612   \n",
       "2    0.037898     0.494730    0.860963    0.319712     0.855009   \n",
       "3    0.031685     0.496472    0.877005    0.312500     0.851240   \n",
       "4    0.047943     0.475288    0.786096    0.334135     0.866266   \n",
       "\n",
       "   acc_zg_median  acc_zg_std  acc_zg_skew   acc_min   acc_max  acc_mean  \\\n",
       "0       0.898955    0.059745     0.495878  0.000000  0.027435  0.052375   \n",
       "1       0.898955    0.042599     0.617168  0.000000  0.020538  0.034727   \n",
       "2       0.891986    0.076404     0.542749  0.000000  0.043521  0.067668   \n",
       "3       0.885017    0.073086     0.584750  0.000000  0.051527  0.074492   \n",
       "4       0.905923    0.132888     0.505414  0.084365  0.054263  0.124296   \n",
       "\n",
       "   acc_median   acc_std  acc_skew  accg_min  accg_max  accg_mean  accg_median  \\\n",
       "0    0.043145  0.047137  0.248385  0.843662  0.028872   0.190793     0.340451   \n",
       "1    0.027287  0.030781  0.311711  0.931695  0.027672   0.211447     0.342910   \n",
       "2    0.047263  0.066111  0.346175  0.864244  0.036568   0.194364     0.363272   \n",
       "3    0.054574  0.077935  0.356956  0.864739  0.029621   0.180501     0.336686   \n",
       "4    0.105683  0.092122  0.247648  0.758629  0.057496   0.187294     0.359235   \n",
       "\n",
       "   accg_std  accg_skew  \n",
       "0  0.056427   0.496838  \n",
       "1  0.038872   0.622006  \n",
       "2  0.068343   0.530991  \n",
       "3  0.069349   0.559849  \n",
       "4  0.120903   0.542701  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T08:42:58.404437Z",
     "start_time": "2020-07-26T08:42:58.399433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7292,)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T08:43:17.700562Z",
     "start_time": "2020-07-26T08:43:17.663685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fragment_id</th>\n",
       "      <th>behavior_id</th>\n",
       "      <th>acc_x_min</th>\n",
       "      <th>acc_x_max</th>\n",
       "      <th>acc_x_mean</th>\n",
       "      <th>acc_x_median</th>\n",
       "      <th>acc_x_std</th>\n",
       "      <th>acc_x_skew</th>\n",
       "      <th>acc_y_min</th>\n",
       "      <th>acc_y_max</th>\n",
       "      <th>acc_y_mean</th>\n",
       "      <th>acc_y_median</th>\n",
       "      <th>acc_y_std</th>\n",
       "      <th>acc_y_skew</th>\n",
       "      <th>acc_z_min</th>\n",
       "      <th>acc_z_max</th>\n",
       "      <th>acc_z_mean</th>\n",
       "      <th>acc_z_median</th>\n",
       "      <th>acc_z_std</th>\n",
       "      <th>acc_z_skew</th>\n",
       "      <th>acc_xg_min</th>\n",
       "      <th>acc_xg_max</th>\n",
       "      <th>acc_xg_mean</th>\n",
       "      <th>acc_xg_median</th>\n",
       "      <th>acc_xg_std</th>\n",
       "      <th>acc_xg_skew</th>\n",
       "      <th>acc_yg_min</th>\n",
       "      <th>acc_yg_max</th>\n",
       "      <th>acc_yg_mean</th>\n",
       "      <th>acc_yg_median</th>\n",
       "      <th>acc_yg_std</th>\n",
       "      <th>acc_yg_skew</th>\n",
       "      <th>acc_zg_min</th>\n",
       "      <th>acc_zg_max</th>\n",
       "      <th>acc_zg_mean</th>\n",
       "      <th>acc_zg_median</th>\n",
       "      <th>acc_zg_std</th>\n",
       "      <th>acc_zg_skew</th>\n",
       "      <th>acc_min</th>\n",
       "      <th>acc_max</th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>acc_median</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>acc_skew</th>\n",
       "      <th>accg_min</th>\n",
       "      <th>accg_max</th>\n",
       "      <th>accg_mean</th>\n",
       "      <th>accg_median</th>\n",
       "      <th>accg_std</th>\n",
       "      <th>accg_skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.001754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.194102</td>\n",
       "      <td>-1.523203</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.001754</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.085547</td>\n",
       "      <td>-0.497876</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.294573</td>\n",
       "      <td>-0.283199</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.677193</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.200016</td>\n",
       "      <td>-0.909003</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.773684</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.158707</td>\n",
       "      <td>0.284234</td>\n",
       "      <td>7.2</td>\n",
       "      <td>9.3</td>\n",
       "      <td>8.508772</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.338738</td>\n",
       "      <td>-0.746650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.953939</td>\n",
       "      <td>0.281656</td>\n",
       "      <td>0.223607</td>\n",
       "      <td>0.225918</td>\n",
       "      <td>1.141795</td>\n",
       "      <td>8.635392</td>\n",
       "      <td>10.560776</td>\n",
       "      <td>9.784581</td>\n",
       "      <td>9.786726</td>\n",
       "      <td>0.293469</td>\n",
       "      <td>-0.677138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.109959</td>\n",
       "      <td>-1.350853</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058748</td>\n",
       "      <td>0.434311</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.078571</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.184602</td>\n",
       "      <td>0.129004</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.126080</td>\n",
       "      <td>-0.199871</td>\n",
       "      <td>4.6</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4.830357</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.123465</td>\n",
       "      <td>0.411559</td>\n",
       "      <td>8.1</td>\n",
       "      <td>9.4</td>\n",
       "      <td>8.519643</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.241525</td>\n",
       "      <td>0.917398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714143</td>\n",
       "      <td>0.186749</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>0.147525</td>\n",
       "      <td>1.626938</td>\n",
       "      <td>9.441398</td>\n",
       "      <td>10.521407</td>\n",
       "      <td>9.819616</td>\n",
       "      <td>9.791067</td>\n",
       "      <td>0.202188</td>\n",
       "      <td>1.053017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.028070</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.239609</td>\n",
       "      <td>0.489693</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.005263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136828</td>\n",
       "      <td>-1.635879</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.024561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.396989</td>\n",
       "      <td>-0.770588</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.792982</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.339569</td>\n",
       "      <td>0.853589</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>4.952632</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.234601</td>\n",
       "      <td>-0.081638</td>\n",
       "      <td>7.2</td>\n",
       "      <td>9.8</td>\n",
       "      <td>8.394737</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.433186</td>\n",
       "      <td>-0.103604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.513275</td>\n",
       "      <td>0.363894</td>\n",
       "      <td>0.244949</td>\n",
       "      <td>0.316852</td>\n",
       "      <td>1.890966</td>\n",
       "      <td>8.823831</td>\n",
       "      <td>10.813418</td>\n",
       "      <td>9.790638</td>\n",
       "      <td>9.827004</td>\n",
       "      <td>0.355424</td>\n",
       "      <td>-0.205059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.178980</td>\n",
       "      <td>0.072280</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.023636</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.121661</td>\n",
       "      <td>1.126930</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.094545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.496438</td>\n",
       "      <td>1.413450</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.410909</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.319532</td>\n",
       "      <td>-0.555181</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.049091</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.196141</td>\n",
       "      <td>-0.054651</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>8.340000</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.414371</td>\n",
       "      <td>0.472631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791647</td>\n",
       "      <td>0.400589</td>\n",
       "      <td>0.282843</td>\n",
       "      <td>0.373524</td>\n",
       "      <td>1.973559</td>\n",
       "      <td>8.828363</td>\n",
       "      <td>10.585367</td>\n",
       "      <td>9.767121</td>\n",
       "      <td>9.780082</td>\n",
       "      <td>0.360655</td>\n",
       "      <td>0.193839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.338018</td>\n",
       "      <td>-0.762639</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.192415</td>\n",
       "      <td>0.464490</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.023636</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.705787</td>\n",
       "      <td>-0.013071</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.609091</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.419114</td>\n",
       "      <td>-0.127315</td>\n",
       "      <td>3.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>4.645455</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.296784</td>\n",
       "      <td>-0.382833</td>\n",
       "      <td>5.8</td>\n",
       "      <td>10.4</td>\n",
       "      <td>8.558182</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.753434</td>\n",
       "      <td>-0.615809</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>1.886796</td>\n",
       "      <td>0.668420</td>\n",
       "      <td>0.547723</td>\n",
       "      <td>0.441521</td>\n",
       "      <td>1.136149</td>\n",
       "      <td>7.856844</td>\n",
       "      <td>11.500435</td>\n",
       "      <td>9.778645</td>\n",
       "      <td>9.819878</td>\n",
       "      <td>0.628718</td>\n",
       "      <td>-0.043196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fragment_id  behavior_id  acc_x_min  acc_x_max  acc_x_mean  acc_x_median  \\\n",
       "0            0          0.0       -0.9        0.4   -0.001754           0.0   \n",
       "1            1          0.0       -0.5        0.2   -0.025000          -0.0   \n",
       "2            2          0.0       -0.5        0.7   -0.028070          -0.0   \n",
       "3            3          0.0       -0.4        0.5    0.001818           0.0   \n",
       "4            4          0.0       -1.3        0.9    0.001818           0.0   \n",
       "\n",
       "   acc_x_std  acc_x_skew  acc_y_min  acc_y_max  acc_y_mean  acc_y_median  \\\n",
       "0   0.194102   -1.523203       -0.3        0.2   -0.001754          -0.0   \n",
       "1   0.109959   -1.350853       -0.1        0.2    0.026786           0.0   \n",
       "2   0.239609    0.489693       -0.6        0.2   -0.005263           0.0   \n",
       "3   0.178980    0.072280       -0.3        0.4    0.023636          -0.0   \n",
       "4   0.338018   -0.762639       -0.5        0.6    0.003636           0.0   \n",
       "\n",
       "   acc_y_std  acc_y_skew  acc_z_min  acc_z_max  acc_z_mean  acc_z_median  \\\n",
       "0   0.085547   -0.497876       -0.8        0.7    0.003509          -0.0   \n",
       "1   0.058748    0.434311       -0.5        0.6    0.078571           0.1   \n",
       "2   0.136828   -1.635879       -1.3        1.2    0.024561           0.0   \n",
       "3   0.121661    1.126930       -0.8        1.7    0.094545           0.0   \n",
       "4   0.192415    0.464490       -1.7        1.8    0.023636           0.1   \n",
       "\n",
       "   acc_z_std  acc_z_skew  acc_xg_min  acc_xg_max  acc_xg_mean  acc_xg_median  \\\n",
       "0   0.294573   -0.283199        -0.2         1.3     0.677193            0.7   \n",
       "1   0.184602    0.129004         0.4         1.0     0.678571            0.7   \n",
       "2   0.396989   -0.770588         0.2         1.8     0.792982            0.7   \n",
       "3   0.496438    1.413450        -0.6         1.2     0.410909            0.4   \n",
       "4   0.705787   -0.013071        -0.5         1.5     0.609091            0.7   \n",
       "\n",
       "   acc_xg_std  acc_xg_skew  acc_yg_min  acc_yg_max  acc_yg_mean  \\\n",
       "0    0.200016    -0.909003         4.4         5.2     4.773684   \n",
       "1    0.126080    -0.199871         4.6         5.1     4.830357   \n",
       "2    0.339569     0.853589         4.4         5.4     4.952632   \n",
       "3    0.319532    -0.555181         4.5         5.5     5.049091   \n",
       "4    0.419114    -0.127315         3.7         5.3     4.645455   \n",
       "\n",
       "   acc_yg_median  acc_yg_std  acc_yg_skew  acc_zg_min  acc_zg_max  \\\n",
       "0            4.8    0.158707     0.284234         7.2         9.3   \n",
       "1            4.8    0.123465     0.411559         8.1         9.4   \n",
       "2            4.9    0.234601    -0.081638         7.2         9.8   \n",
       "3            5.0    0.196141    -0.054651         7.5         9.5   \n",
       "4            4.7    0.296784    -0.382833         5.8        10.4   \n",
       "\n",
       "   acc_zg_mean  acc_zg_median  acc_zg_std  acc_zg_skew   acc_min   acc_max  \\\n",
       "0     8.508772            8.5    0.338738    -0.746650  0.000000  0.953939   \n",
       "1     8.519643            8.5    0.241525     0.917398  0.000000  0.714143   \n",
       "2     8.394737            8.4    0.433186    -0.103604  0.000000  1.513275   \n",
       "3     8.340000            8.3    0.414371     0.472631  0.000000  1.791647   \n",
       "4     8.558182            8.6    0.753434    -0.615809  0.141421  1.886796   \n",
       "\n",
       "   acc_mean  acc_median   acc_std  acc_skew  accg_min   accg_max  accg_mean  \\\n",
       "0  0.281656    0.223607  0.225918  1.141795  8.635392  10.560776   9.784581   \n",
       "1  0.186749    0.141421  0.147525  1.626938  9.441398  10.521407   9.819616   \n",
       "2  0.363894    0.244949  0.316852  1.890966  8.823831  10.813418   9.790638   \n",
       "3  0.400589    0.282843  0.373524  1.973559  8.828363  10.585367   9.767121   \n",
       "4  0.668420    0.547723  0.441521  1.136149  7.856844  11.500435   9.778645   \n",
       "\n",
       "   accg_median  accg_std  accg_skew  \n",
       "0     9.786726  0.293469  -0.677138  \n",
       "1     9.791067  0.202188   1.053017  \n",
       "2     9.827004  0.355424  -0.205059  \n",
       "3     9.780082  0.360655   0.193839  \n",
       "4     9.819878  0.628718  -0.043196  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T08:43:50.216135Z",
     "start_time": "2020-07-26T08:43:50.209158Z"
    }
   },
   "outputs": [],
   "source": [
    "data1['behavior_id'] = train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T09:14:13.904125Z",
     "start_time": "2020-07-26T09:14:13.900283Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.backend import expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T09:18:06.934497Z",
     "start_time": "2020-07-26T09:18:06.930352Z"
    }
   },
   "outputs": [],
   "source": [
    "def Net():\n",
    "    input_data = Input(shape=(133,)) \n",
    "    X = Dense(128,activation='relu')(input_data)\n",
    "    X = Dense(19,activation='softmax')(X)\n",
    "    return Model([input_data], X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T09:07:43.409132Z",
     "start_time": "2020-07-26T09:07:43.275577Z"
    }
   },
   "outputs": [],
   "source": [
    "stk_test = pd.merge(oof_knn_prob_test,oof_lgb_prob_test,on=['fragment_id'])\n",
    "stk_test = pd.merge(stk_test,oof_nn1_prob_test,on=['fragment_id'])\n",
    "stk_test = pd.merge(stk_test,oof_nn2_prob_test,on=['fragment_id'])\n",
    "stk_test = pd.merge(stk_test,oof_nn4_prob_test,on=['fragment_id'])\n",
    "stk_test = pd.merge(stk_test,oof_rf_prob_test,on=['fragment_id'])\n",
    "stk_test = pd.merge(stk_test,oof_svc_prob_test,on=['fragment_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T09:07:49.060307Z",
     "start_time": "2020-07-26T09:07:49.055322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 134)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stk_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T09:36:03.052528Z",
     "start_time": "2020-07-26T09:36:03.037577Z"
    }
   },
   "outputs": [],
   "source": [
    "x = data1.drop(['fragment_id','behavior_id'],axis=1)\n",
    "y = data1['behavior_id']\n",
    "t = stk_test.drop(['fragment_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T09:37:26.493678Z",
     "start_time": "2020-07-26T09:36:31.635453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.9303 - acc: 0.025 - ETA: 0s - loss: 2.7641 - acc: 0.363 - 3s 217ms/step - loss: 2.7388 - acc: 0.3986 - val_loss: 2.4999 - val_acc: 0.6813 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.5172 - acc: 0.656 - ETA: 0s - loss: 2.3674 - acc: 0.740 - 1s 75ms/step - loss: 2.3123 - acc: 0.7495 - val_loss: 2.0657 - val_acc: 0.8122 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0950 - acc: 0.794 - ETA: 0s - loss: 1.9306 - acc: 0.813 - 1s 75ms/step - loss: 1.8599 - acc: 0.8155 - val_loss: 1.6076 - val_acc: 0.8232 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.5667 - acc: 0.832 - ETA: 0s - loss: 1.4457 - acc: 0.827 - 1s 73ms/step - loss: 1.4092 - acc: 0.8280 - val_loss: 1.1968 - val_acc: 0.8286 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1697 - acc: 0.835 - ETA: 0s - loss: 1.0662 - acc: 0.833 - 1s 81ms/step - loss: 1.0424 - acc: 0.8335 - val_loss: 0.9089 - val_acc: 0.8293 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8603 - acc: 0.841 - ETA: 0s - loss: 0.8112 - acc: 0.840 - 1s 76ms/step - loss: 0.8057 - acc: 0.8390 - val_loss: 0.7482 - val_acc: 0.8300 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7817 - acc: 0.824 - ETA: 0s - loss: 0.6929 - acc: 0.845 - 1s 87ms/step - loss: 0.6779 - acc: 0.8423 - val_loss: 0.6712 - val_acc: 0.8286 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6820 - acc: 0.820 - ETA: 0s - loss: 0.6166 - acc: 0.845 - 1s 73ms/step - loss: 0.6158 - acc: 0.8454 - val_loss: 0.6361 - val_acc: 0.8300 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6107 - acc: 0.847 - ETA: 0s - loss: 0.5962 - acc: 0.844 - 1s 75ms/step - loss: 0.5841 - acc: 0.8467 - val_loss: 0.6182 - val_acc: 0.8300 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5698 - acc: 0.843 - ETA: 0s - loss: 0.5542 - acc: 0.853 - 1s 71ms/step - loss: 0.5662 - acc: 0.8472 - val_loss: 0.6094 - val_acc: 0.8314 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5323 - acc: 0.869 - ETA: 0s - loss: 0.5416 - acc: 0.854 - 1s 79ms/step - loss: 0.5544 - acc: 0.8484 - val_loss: 0.6046 - val_acc: 0.8307 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5504 - acc: 0.853 - ETA: 0s - loss: 0.5500 - acc: 0.848 - 1s 76ms/step - loss: 0.5455 - acc: 0.8493 - val_loss: 0.6000 - val_acc: 0.8300 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6117 - acc: 0.826 - ETA: 0s - loss: 0.5323 - acc: 0.850 - 1s 73ms/step - loss: 0.5383 - acc: 0.8484 - val_loss: 0.5979 - val_acc: 0.8307 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4972 - acc: 0.869 - ETA: 0s - loss: 0.5375 - acc: 0.848 - 1s 74ms/step - loss: 0.5326 - acc: 0.8495 - val_loss: 0.5959 - val_acc: 0.8293 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5682 - acc: 0.835 - ETA: 0s - loss: 0.5332 - acc: 0.847 - 1s 82ms/step - loss: 0.5273 - acc: 0.8505 - val_loss: 0.5934 - val_acc: 0.8314 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5145 - acc: 0.861 - ETA: 0s - loss: 0.5378 - acc: 0.845 - 1s 72ms/step - loss: 0.5227 - acc: 0.8510 - val_loss: 0.5929 - val_acc: 0.8314 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5010 - acc: 0.855 - ETA: 0s - loss: 0.5271 - acc: 0.847 - 1s 74ms/step - loss: 0.5187 - acc: 0.8505 - val_loss: 0.5920 - val_acc: 0.8314 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5346 - acc: 0.851 - ETA: 0s - loss: 0.5136 - acc: 0.852 - 1s 81ms/step - loss: 0.5147 - acc: 0.8510 - val_loss: 0.5911 - val_acc: 0.8314 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5023 - acc: 0.863 - ETA: 0s - loss: 0.5191 - acc: 0.849 - 1s 75ms/step - loss: 0.5113 - acc: 0.8515 - val_loss: 0.5914 - val_acc: 0.8280 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5158 - acc: 0.859 - ETA: 0s - loss: 0.5154 - acc: 0.848 - 1s 75ms/step - loss: 0.5085 - acc: 0.8514 - val_loss: 0.5900 - val_acc: 0.8300 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4841 - acc: 0.869 - ETA: 0s - loss: 0.5097 - acc: 0.853 - ETA: 0s - loss: 0.5048 - acc: 0.853 - 1s 84ms/step - loss: 0.5048 - acc: 0.8531 - val_loss: 0.5898 - val_acc: 0.8300 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4747 - acc: 0.875 - ETA: 0s - loss: 0.4996 - acc: 0.853 - 1s 90ms/step - loss: 0.5023 - acc: 0.8522 - val_loss: 0.5902 - val_acc: 0.8266 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5082 - acc: 0.853 - ETA: 0s - loss: 0.5006 - acc: 0.850 - 1s 69ms/step - loss: 0.4991 - acc: 0.8526 - val_loss: 0.5887 - val_acc: 0.8286 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3689 - acc: 0.888 - ETA: 0s - loss: 0.4925 - acc: 0.852 - 1s 84ms/step - loss: 0.4966 - acc: 0.8527 - val_loss: 0.5888 - val_acc: 0.8280 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4945 - acc: 0.839 - ETA: 0s - loss: 0.4944 - acc: 0.853 - 1s 85ms/step - loss: 0.4938 - acc: 0.8532 - val_loss: 0.5883 - val_acc: 0.8286 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5463 - acc: 0.839 - ETA: 0s - loss: 0.4934 - acc: 0.852 - 1s 77ms/step - loss: 0.4916 - acc: 0.8531 - val_loss: 0.5878 - val_acc: 0.8273 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4510 - acc: 0.867 - ETA: 0s - loss: 0.4829 - acc: 0.855 - 1s 88ms/step - loss: 0.4891 - acc: 0.8538 - val_loss: 0.5893 - val_acc: 0.8273 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5574 - acc: 0.845 - ETA: 0s - loss: 0.5060 - acc: 0.849 - 1s 114ms/step - loss: 0.4869 - acc: 0.8536 - val_loss: 0.5897 - val_acc: 0.8286 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4329 - acc: 0.880 - ETA: 0s - loss: 0.4865 - acc: 0.856 - ETA: 0s - loss: 0.4859 - acc: 0.854 - 1s 110ms/step - loss: 0.4853 - acc: 0.8543 - val_loss: 0.5891 - val_acc: 0.8252 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4114 - acc: 0.875 - ETA: 0s - loss: 0.4730 - acc: 0.854 - 1s 100ms/step - loss: 0.4825 - acc: 0.8536 - val_loss: 0.5888 - val_acc: 0.8273 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5373 - acc: 0.824 - ETA: 0s - loss: 0.4810 - acc: 0.852 - 1s 91ms/step - loss: 0.4806 - acc: 0.8550 - val_loss: 0.5888 - val_acc: 0.8286 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5245 - acc: 0.841 - ETA: 0s - loss: 0.4865 - acc: 0.853 - ETA: 0s - loss: 0.4786 - acc: 0.855 - 1s 93ms/step - loss: 0.4786 - acc: 0.8553 - val_loss: 0.5901 - val_acc: 0.8273 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4191 - acc: 0.877 - ETA: 0s - loss: 0.4729 - acc: 0.854 - 1s 82ms/step - loss: 0.4765 - acc: 0.8544 - val_loss: 0.5903 - val_acc: 0.8266 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4995 - acc: 0.837 - ETA: 0s - loss: 0.4627 - acc: 0.858 - 1s 73ms/step - loss: 0.4742 - acc: 0.8548 - val_loss: 0.5906 - val_acc: 0.8273 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4753 - acc: 0.865 - ETA: 0s - loss: 0.4692 - acc: 0.856 - 1s 80ms/step - loss: 0.4727 - acc: 0.8556 - val_loss: 0.5923 - val_acc: 0.8266 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4781 - acc: 0.845 - ETA: 0s - loss: 0.4571 - acc: 0.859 - 1s 73ms/step - loss: 0.4703 - acc: 0.8563 - val_loss: 0.5901 - val_acc: 0.8239 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4317 - acc: 0.873 - ETA: 0s - loss: 0.4713 - acc: 0.857 - 1s 92ms/step - loss: 0.4691 - acc: 0.8558 - val_loss: 0.5901 - val_acc: 0.8259 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4677 - acc: 0.857 - ETA: 0s - loss: 0.4533 - acc: 0.861 - 1s 75ms/step - loss: 0.4669 - acc: 0.8565 - val_loss: 0.5913 - val_acc: 0.8252 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4223 - acc: 0.867 - ETA: 0s - loss: 0.4589 - acc: 0.855 - 1s 85ms/step - loss: 0.4652 - acc: 0.8563 - val_loss: 0.5925 - val_acc: 0.8280 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4881 - acc: 0.837 - ETA: 0s - loss: 0.4724 - acc: 0.852 - 1s 82ms/step - loss: 0.4631 - acc: 0.8574 - val_loss: 0.5920 - val_acc: 0.8273 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4519 - acc: 0.865 - ETA: 0s - loss: 0.4560 - acc: 0.860 - 1s 74ms/step - loss: 0.4621 - acc: 0.8580 - val_loss: 0.5915 - val_acc: 0.8245 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4462 - acc: 0.857 - ETA: 0s - loss: 0.4650 - acc: 0.854 - 1s 77ms/step - loss: 0.4607 - acc: 0.8567 - val_loss: 0.5949 - val_acc: 0.8245 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4848 - acc: 0.853 - ETA: 0s - loss: 0.4567 - acc: 0.860 - 1s 102ms/step - loss: 0.4594 - acc: 0.8580 - val_loss: 0.5940 - val_acc: 0.8266 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4638 - acc: 0.853 - ETA: 0s - loss: 0.4521 - acc: 0.861 - 1s 84ms/step - loss: 0.4570 - acc: 0.8589 - val_loss: 0.5943 - val_acc: 0.8259 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4207 - acc: 0.873 - ETA: 0s - loss: 0.4604 - acc: 0.856 - 1s 91ms/step - loss: 0.4552 - acc: 0.8586 - val_loss: 0.5938 - val_acc: 0.8273 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4858 - acc: 0.847 - ETA: 0s - loss: 0.4508 - acc: 0.861 - 1s 78ms/step - loss: 0.4538 - acc: 0.8591 - val_loss: 0.5947 - val_acc: 0.8280 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4029 - acc: 0.880 - ETA: 0s - loss: 0.4517 - acc: 0.857 - 1s 84ms/step - loss: 0.4523 - acc: 0.8582 - val_loss: 0.5966 - val_acc: 0.8252 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4707 - acc: 0.847 - ETA: 0s - loss: 0.4423 - acc: 0.860 - 1s 75ms/step - loss: 0.4505 - acc: 0.8601 - val_loss: 0.5949 - val_acc: 0.8252 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5254 - acc: 0.837 - ETA: 0s - loss: 0.4531 - acc: 0.859 - 1s 76ms/step - loss: 0.4490 - acc: 0.8610 - val_loss: 0.5960 - val_acc: 0.8252 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4461 - acc: 0.863 - ETA: 0s - loss: 0.4511 - acc: 0.860 - 1s 70ms/step - loss: 0.4473 - acc: 0.8606 - val_loss: 0.5966 - val_acc: 0.8273 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4500 - acc: 0.849 - ETA: 0s - loss: 0.4450 - acc: 0.859 - 1s 77ms/step - loss: 0.4456 - acc: 0.8603 - val_loss: 0.5973 - val_acc: 0.8245 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4222 - acc: 0.878 - ETA: 0s - loss: 0.4495 - acc: 0.860 - 1s 77ms/step - loss: 0.4442 - acc: 0.8613 - val_loss: 0.5980 - val_acc: 0.8252 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      " 9/12 [=====================>........] - ETA: 0s - loss: 0.4222 - acc: 0.873 - ETA: 0s - loss: 0.4579 - acc: 0.8570"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-223-f49c01317d2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m               \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m               callbacks=[plateau, early_stopping, checkpoint])\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'fold{fold}.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mnn_proba_sub\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m5.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    870\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m               return_dict=True)\n\u001b[0m\u001b[0;32m    873\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1055\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1057\u001b[1;33m           model=self)\n\u001b[0m\u001b[0;32m   1058\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    388\u001b[0m     dataset = dataset_ops.DatasetV2.zip((\n\u001b[0;32m    389\u001b[0m         \u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m         \u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m     ))\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    560\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m     \"\"\"\n\u001b[1;32m--> 562\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, element)\u001b[0m\n\u001b[0;32m   2837\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2838\u001b[0m     \u001b[1;34m\"\"\"See `Dataset.from_tensors()` for details.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2839\u001b[1;33m     \u001b[0melement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2840\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2841\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[1;34m(element)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_spec_from_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_fallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;31m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36mtype_spec_from_value\u001b[1;34m(element, use_fallback)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m   raise TypeError(\"Could not build a TypeSpec for %r with type %s\" %\n\u001b[1;32m--> 466\u001b[1;33m                   (element, type(element).__name__))\n\u001b[0m\u001b[0;32m    467\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mYields\u001b[0m \u001b[0mBytestring\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mPy2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnicode\u001b[0m \u001b[0mString\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpy3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \"\"\"\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m__str__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__unicode__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__bytes__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__unicode__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         self.to_string(buf=buf, max_rows=max_rows, max_cols=max_cols,\n\u001b[1;32m--> 634\u001b[1;33m                        line_width=width, show_dimensions=show_dimensions)\n\u001b[0m\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mto_string\u001b[1;34m(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, max_rows, max_cols, show_dimensions, decimal, line_width)\u001b[0m\n\u001b[0;32m    719\u001b[0m                                            \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m                                            line_width=line_width)\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_string\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m             \u001b[0mstrcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_str_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mline_width\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# no need to wrap around just print\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                 \u001b[1;31m# the whole frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36m_to_str_columns\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    534\u001b[0m                 header_colwidth = max(self.col_space or 0,\n\u001b[0;32m    535\u001b[0m                                       *(self.adj.len(x) for x in cheader))\n\u001b[1;32m--> 536\u001b[1;33m                 \u001b[0mfmt_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_col\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    537\u001b[0m                 fmt_values = _make_fixed_width(fmt_values, self.justify,\n\u001b[0;32m    538\u001b[0m                                                \u001b[0mminimum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheader_colwidth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36m_format_col\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    706\u001b[0m         return format_array(values_to_format, formatter,\n\u001b[0;32m    707\u001b[0m                             \u001b[0mfloat_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_rep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mna_rep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m                             space=self.col_space, decimal=self.decimal)\n\u001b[0m\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnotebook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mborder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mformat_array\u001b[1;34m(values, formatter, float_format, na_rep, digits, space, justify, decimal, leading_space)\u001b[0m\n\u001b[0;32m    905\u001b[0m                         leading_space=leading_space)\n\u001b[0;32m    906\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 907\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfmt_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 928\u001b[1;33m         \u001b[0mfmt_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_strings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    929\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_make_fixed_width\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfmt_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjustify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36m_format_strings\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result_as_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mget_result_as_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1095\u001b[0m             \u001b[0mfloat_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat_format\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1097\u001b[1;33m         \u001b[0mformatted_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformat_values_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_width\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mformat_values_with\u001b[1;34m(float_format)\u001b[0m\n\u001b[0;32m   1077\u001b[0m             \u001b[0mimask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m             values.flat[imask] = np.array([formatter(val)\n\u001b[1;32m-> 1079\u001b[1;33m                                            for val in values.ravel()[imask]])\n\u001b[0m\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_width\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn_proba_sub = np.zeros((7500, 19))\n",
    "nn_proba_val = np.zeros((7292, 19))\n",
    "for fold, (trn_idx, val_idx) in enumerate(kfold.split(x, y)):\n",
    "    y_ = to_categorical(y, num_classes=19)\n",
    "    model = Net()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['acc'])\n",
    "    plateau = ReduceLROnPlateau(monitor=\"val_acc\",\n",
    "                                verbose=0,\n",
    "                                mode='auto',\n",
    "                                factor=1. / np.cbrt(2),\n",
    "                                patience=200)\n",
    "    early_stopping = EarlyStopping(monitor='val_acc',\n",
    "                                   verbose=0,\n",
    "                                   mode='auto',\n",
    "                                   patience=200)\n",
    "    checkpoint = ModelCheckpoint(f'fold{fold}.h5',\n",
    "                                 monitor='val_acc',\n",
    "                                 verbose=0,\n",
    "                                 mode='auto',\n",
    "                                 save_best_only=True)\n",
    "    model.fit(x.iloc[trn_idx], y_[trn_idx],\n",
    "              epochs=1000,\n",
    "              batch_size=512,\n",
    "              verbose=1,\n",
    "              shuffle=True,\n",
    "              validation_data=(x.iloc[val_idx], y_[val_idx]),\n",
    "              callbacks=[plateau, early_stopping, checkpoint])\n",
    "    model.load_weights(f'fold{fold}.h5')\n",
    "    nn_proba_sub += model.predict(t, verbose=0, batch_size=1024) / 5.\n",
    "    nn_proba_val[val_idx] = model.predict(x.iloc[val_idx]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T09:13:13.545043Z",
     "start_time": "2020-07-26T09:13:13.463230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_proba_x</th>\n",
       "      <th>1_proba_x</th>\n",
       "      <th>2_proba_x</th>\n",
       "      <th>3_proba_x</th>\n",
       "      <th>4_proba_x</th>\n",
       "      <th>5_proba_x</th>\n",
       "      <th>6_proba_x</th>\n",
       "      <th>7_proba_x</th>\n",
       "      <th>8_proba_x</th>\n",
       "      <th>9_proba_x</th>\n",
       "      <th>10_proba_x</th>\n",
       "      <th>11_proba_x</th>\n",
       "      <th>12_proba_x</th>\n",
       "      <th>13_proba_x</th>\n",
       "      <th>14_proba_x</th>\n",
       "      <th>15_proba_x</th>\n",
       "      <th>16_proba_x</th>\n",
       "      <th>17_proba_x</th>\n",
       "      <th>18_proba_x</th>\n",
       "      <th>0_proba_y</th>\n",
       "      <th>1_proba_y</th>\n",
       "      <th>2_proba_y</th>\n",
       "      <th>3_proba_y</th>\n",
       "      <th>4_proba_y</th>\n",
       "      <th>5_proba_y</th>\n",
       "      <th>6_proba_y</th>\n",
       "      <th>7_proba_y</th>\n",
       "      <th>8_proba_y</th>\n",
       "      <th>9_proba_y</th>\n",
       "      <th>10_proba_y</th>\n",
       "      <th>11_proba_y</th>\n",
       "      <th>12_proba_y</th>\n",
       "      <th>13_proba_y</th>\n",
       "      <th>14_proba_y</th>\n",
       "      <th>15_proba_y</th>\n",
       "      <th>16_proba_y</th>\n",
       "      <th>17_proba_y</th>\n",
       "      <th>18_proba_y</th>\n",
       "      <th>0_proba_x</th>\n",
       "      <th>1_proba_x</th>\n",
       "      <th>2_proba_x</th>\n",
       "      <th>3_proba_x</th>\n",
       "      <th>4_proba_x</th>\n",
       "      <th>5_proba_x</th>\n",
       "      <th>6_proba_x</th>\n",
       "      <th>7_proba_x</th>\n",
       "      <th>8_proba_x</th>\n",
       "      <th>9_proba_x</th>\n",
       "      <th>10_proba_x</th>\n",
       "      <th>11_proba_x</th>\n",
       "      <th>12_proba_x</th>\n",
       "      <th>13_proba_x</th>\n",
       "      <th>14_proba_x</th>\n",
       "      <th>15_proba_x</th>\n",
       "      <th>16_proba_x</th>\n",
       "      <th>17_proba_x</th>\n",
       "      <th>18_proba_x</th>\n",
       "      <th>0_proba_y</th>\n",
       "      <th>1_proba_y</th>\n",
       "      <th>2_proba_y</th>\n",
       "      <th>3_proba_y</th>\n",
       "      <th>4_proba_y</th>\n",
       "      <th>5_proba_y</th>\n",
       "      <th>6_proba_y</th>\n",
       "      <th>7_proba_y</th>\n",
       "      <th>8_proba_y</th>\n",
       "      <th>9_proba_y</th>\n",
       "      <th>10_proba_y</th>\n",
       "      <th>11_proba_y</th>\n",
       "      <th>12_proba_y</th>\n",
       "      <th>13_proba_y</th>\n",
       "      <th>14_proba_y</th>\n",
       "      <th>15_proba_y</th>\n",
       "      <th>16_proba_y</th>\n",
       "      <th>17_proba_y</th>\n",
       "      <th>18_proba_y</th>\n",
       "      <th>0_proba_x</th>\n",
       "      <th>1_proba_x</th>\n",
       "      <th>2_proba_x</th>\n",
       "      <th>3_proba_x</th>\n",
       "      <th>4_proba_x</th>\n",
       "      <th>5_proba_x</th>\n",
       "      <th>6_proba_x</th>\n",
       "      <th>7_proba_x</th>\n",
       "      <th>8_proba_x</th>\n",
       "      <th>9_proba_x</th>\n",
       "      <th>10_proba_x</th>\n",
       "      <th>11_proba_x</th>\n",
       "      <th>12_proba_x</th>\n",
       "      <th>13_proba_x</th>\n",
       "      <th>14_proba_x</th>\n",
       "      <th>15_proba_x</th>\n",
       "      <th>16_proba_x</th>\n",
       "      <th>17_proba_x</th>\n",
       "      <th>18_proba_x</th>\n",
       "      <th>0_proba_y</th>\n",
       "      <th>1_proba_y</th>\n",
       "      <th>2_proba_y</th>\n",
       "      <th>3_proba_y</th>\n",
       "      <th>4_proba_y</th>\n",
       "      <th>5_proba_y</th>\n",
       "      <th>6_proba_y</th>\n",
       "      <th>7_proba_y</th>\n",
       "      <th>8_proba_y</th>\n",
       "      <th>9_proba_y</th>\n",
       "      <th>10_proba_y</th>\n",
       "      <th>11_proba_y</th>\n",
       "      <th>12_proba_y</th>\n",
       "      <th>13_proba_y</th>\n",
       "      <th>14_proba_y</th>\n",
       "      <th>15_proba_y</th>\n",
       "      <th>16_proba_y</th>\n",
       "      <th>17_proba_y</th>\n",
       "      <th>18_proba_y</th>\n",
       "      <th>0_proba</th>\n",
       "      <th>1_proba</th>\n",
       "      <th>2_proba</th>\n",
       "      <th>3_proba</th>\n",
       "      <th>4_proba</th>\n",
       "      <th>5_proba</th>\n",
       "      <th>6_proba</th>\n",
       "      <th>7_proba</th>\n",
       "      <th>8_proba</th>\n",
       "      <th>9_proba</th>\n",
       "      <th>10_proba</th>\n",
       "      <th>11_proba</th>\n",
       "      <th>12_proba</th>\n",
       "      <th>13_proba</th>\n",
       "      <th>14_proba</th>\n",
       "      <th>15_proba</th>\n",
       "      <th>16_proba</th>\n",
       "      <th>17_proba</th>\n",
       "      <th>18_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031669</td>\n",
       "      <td>0.441127</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.175464</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.037579</td>\n",
       "      <td>0.063476</td>\n",
       "      <td>0.005356</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.023875</td>\n",
       "      <td>0.175142</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.003027</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.015173</td>\n",
       "      <td>0.001460</td>\n",
       "      <td>0.002880</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.3552</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0992</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0512</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020270</td>\n",
       "      <td>0.591118</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.076698</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.013208</td>\n",
       "      <td>0.015505</td>\n",
       "      <td>0.022670</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.015394</td>\n",
       "      <td>0.148776</td>\n",
       "      <td>0.012079</td>\n",
       "      <td>0.002453</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.067546</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.481034</td>\n",
       "      <td>0.374553</td>\n",
       "      <td>6.250838e-06</td>\n",
       "      <td>8.488125e-02</td>\n",
       "      <td>5.771121e-06</td>\n",
       "      <td>3.099303e-04</td>\n",
       "      <td>1.461453e-04</td>\n",
       "      <td>3.566003e-07</td>\n",
       "      <td>1.441350e-08</td>\n",
       "      <td>1.626174e-03</td>\n",
       "      <td>0.049993</td>\n",
       "      <td>6.524021e-03</td>\n",
       "      <td>1.115529e-06</td>\n",
       "      <td>1.644272e-05</td>\n",
       "      <td>9.510262e-08</td>\n",
       "      <td>8.849970e-04</td>\n",
       "      <td>1.559794e-05</td>\n",
       "      <td>1.518542e-06</td>\n",
       "      <td>1.312220e-06</td>\n",
       "      <td>0.276344</td>\n",
       "      <td>4.974163e-03</td>\n",
       "      <td>3.196806e-07</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>3.635841e-06</td>\n",
       "      <td>4.885335e-03</td>\n",
       "      <td>4.949490e-08</td>\n",
       "      <td>1.744127e-07</td>\n",
       "      <td>9.497017e-12</td>\n",
       "      <td>5.787967e-05</td>\n",
       "      <td>0.713638</td>\n",
       "      <td>4.377463e-06</td>\n",
       "      <td>1.541949e-07</td>\n",
       "      <td>3.370034e-05</td>\n",
       "      <td>2.815372e-07</td>\n",
       "      <td>2.940170e-05</td>\n",
       "      <td>4.646852e-07</td>\n",
       "      <td>5.648534e-10</td>\n",
       "      <td>4.114973e-08</td>\n",
       "      <td>0.943114</td>\n",
       "      <td>0.014947</td>\n",
       "      <td>2.832391e-07</td>\n",
       "      <td>3.168322e-02</td>\n",
       "      <td>8.313984e-07</td>\n",
       "      <td>4.897588e-04</td>\n",
       "      <td>2.101600e-08</td>\n",
       "      <td>5.589885e-07</td>\n",
       "      <td>1.111513e-09</td>\n",
       "      <td>5.037563e-03</td>\n",
       "      <td>4.688866e-03</td>\n",
       "      <td>2.013336e-05</td>\n",
       "      <td>2.677102e-10</td>\n",
       "      <td>1.379965e-06</td>\n",
       "      <td>6.209310e-07</td>\n",
       "      <td>1.455863e-05</td>\n",
       "      <td>1.057558e-07</td>\n",
       "      <td>4.065118e-07</td>\n",
       "      <td>5.204594e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008463</td>\n",
       "      <td>0.881487</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.017043</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.021602</td>\n",
       "      <td>0.032368</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.016896</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.004859</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.8154</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0498</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>0.845728</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.038582</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.014556</td>\n",
       "      <td>0.027319</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.032406</td>\n",
       "      <td>0.004537</td>\n",
       "      <td>0.025768</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.573070</td>\n",
       "      <td>0.079127</td>\n",
       "      <td>2.785844e-07</td>\n",
       "      <td>2.294374e-05</td>\n",
       "      <td>1.792608e-04</td>\n",
       "      <td>7.888665e-06</td>\n",
       "      <td>9.447433e-06</td>\n",
       "      <td>4.797030e-06</td>\n",
       "      <td>6.715825e-10</td>\n",
       "      <td>1.557029e-01</td>\n",
       "      <td>0.191702</td>\n",
       "      <td>7.186552e-05</td>\n",
       "      <td>9.514194e-10</td>\n",
       "      <td>2.383866e-07</td>\n",
       "      <td>5.640025e-07</td>\n",
       "      <td>1.009518e-04</td>\n",
       "      <td>6.820856e-08</td>\n",
       "      <td>5.948169e-07</td>\n",
       "      <td>1.545804e-08</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>8.662991e-01</td>\n",
       "      <td>1.819706e-09</td>\n",
       "      <td>0.007660</td>\n",
       "      <td>1.195446e-07</td>\n",
       "      <td>4.581188e-06</td>\n",
       "      <td>4.465006e-05</td>\n",
       "      <td>3.523876e-06</td>\n",
       "      <td>7.000848e-10</td>\n",
       "      <td>1.082985e-01</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>1.673466e-02</td>\n",
       "      <td>1.676316e-08</td>\n",
       "      <td>1.725869e-06</td>\n",
       "      <td>1.140225e-05</td>\n",
       "      <td>7.543473e-05</td>\n",
       "      <td>3.884795e-07</td>\n",
       "      <td>6.601894e-06</td>\n",
       "      <td>2.564135e-07</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>0.968507</td>\n",
       "      <td>5.522135e-08</td>\n",
       "      <td>3.101615e-08</td>\n",
       "      <td>1.772393e-02</td>\n",
       "      <td>3.026936e-08</td>\n",
       "      <td>2.151280e-08</td>\n",
       "      <td>1.092184e-07</td>\n",
       "      <td>9.782048e-09</td>\n",
       "      <td>3.087210e-03</td>\n",
       "      <td>2.351299e-03</td>\n",
       "      <td>4.590864e-06</td>\n",
       "      <td>7.544784e-15</td>\n",
       "      <td>2.370737e-11</td>\n",
       "      <td>6.984537e-11</td>\n",
       "      <td>4.949271e-07</td>\n",
       "      <td>1.316929e-11</td>\n",
       "      <td>1.755843e-09</td>\n",
       "      <td>4.897525e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.093019</td>\n",
       "      <td>0.292943</td>\n",
       "      <td>0.002693</td>\n",
       "      <td>0.094578</td>\n",
       "      <td>0.003387</td>\n",
       "      <td>0.018731</td>\n",
       "      <td>0.081917</td>\n",
       "      <td>0.005751</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>0.055956</td>\n",
       "      <td>0.206233</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.004864</td>\n",
       "      <td>0.003710</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.090085</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.1746</td>\n",
       "      <td>0.1836</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.1022</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0746</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062659</td>\n",
       "      <td>0.253161</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>0.216549</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>0.026043</td>\n",
       "      <td>0.023008</td>\n",
       "      <td>0.014735</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>0.054802</td>\n",
       "      <td>0.149235</td>\n",
       "      <td>0.032038</td>\n",
       "      <td>0.006927</td>\n",
       "      <td>0.004962</td>\n",
       "      <td>0.003788</td>\n",
       "      <td>0.057288</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>0.075951</td>\n",
       "      <td>0.003743</td>\n",
       "      <td>0.806572</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>2.580042e-06</td>\n",
       "      <td>6.057014e-02</td>\n",
       "      <td>1.033879e-04</td>\n",
       "      <td>2.398190e-04</td>\n",
       "      <td>2.423308e-07</td>\n",
       "      <td>3.745946e-06</td>\n",
       "      <td>6.820599e-07</td>\n",
       "      <td>4.296690e-03</td>\n",
       "      <td>0.085365</td>\n",
       "      <td>2.586398e-02</td>\n",
       "      <td>1.232927e-07</td>\n",
       "      <td>4.215697e-04</td>\n",
       "      <td>1.705838e-07</td>\n",
       "      <td>1.158001e-02</td>\n",
       "      <td>3.563197e-03</td>\n",
       "      <td>2.959465e-04</td>\n",
       "      <td>4.850493e-04</td>\n",
       "      <td>0.491533</td>\n",
       "      <td>2.232068e-03</td>\n",
       "      <td>1.448590e-05</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>2.383762e-03</td>\n",
       "      <td>1.103980e-04</td>\n",
       "      <td>1.710348e-06</td>\n",
       "      <td>2.358376e-05</td>\n",
       "      <td>1.211643e-07</td>\n",
       "      <td>1.120005e-03</td>\n",
       "      <td>0.425814</td>\n",
       "      <td>4.230822e-04</td>\n",
       "      <td>3.531143e-05</td>\n",
       "      <td>2.149621e-04</td>\n",
       "      <td>2.756539e-03</td>\n",
       "      <td>7.284040e-02</td>\n",
       "      <td>7.481589e-06</td>\n",
       "      <td>1.729156e-04</td>\n",
       "      <td>1.339113e-06</td>\n",
       "      <td>0.911892</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>6.129648e-06</td>\n",
       "      <td>6.423689e-02</td>\n",
       "      <td>5.044804e-06</td>\n",
       "      <td>4.790473e-05</td>\n",
       "      <td>4.772185e-08</td>\n",
       "      <td>9.070901e-08</td>\n",
       "      <td>8.025390e-08</td>\n",
       "      <td>1.027245e-04</td>\n",
       "      <td>3.157057e-04</td>\n",
       "      <td>3.155915e-03</td>\n",
       "      <td>2.675100e-08</td>\n",
       "      <td>3.449972e-08</td>\n",
       "      <td>3.318110e-08</td>\n",
       "      <td>2.009196e-02</td>\n",
       "      <td>7.357116e-07</td>\n",
       "      <td>1.261588e-04</td>\n",
       "      <td>5.562270e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.533759</td>\n",
       "      <td>0.186844</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>0.045731</td>\n",
       "      <td>0.003993</td>\n",
       "      <td>0.045217</td>\n",
       "      <td>0.053578</td>\n",
       "      <td>0.007821</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.020327</td>\n",
       "      <td>0.027389</td>\n",
       "      <td>0.009348</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.003455</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.037426</td>\n",
       "      <td>0.003093</td>\n",
       "      <td>0.004004</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.4816</td>\n",
       "      <td>0.0860</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0470</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0752</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.0978</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447850</td>\n",
       "      <td>0.161704</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.075070</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.056924</td>\n",
       "      <td>0.026587</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>0.008121</td>\n",
       "      <td>0.109260</td>\n",
       "      <td>0.009079</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.076622</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.985897</td>\n",
       "      <td>0.013341</td>\n",
       "      <td>5.429727e-12</td>\n",
       "      <td>1.496944e-07</td>\n",
       "      <td>4.233210e-07</td>\n",
       "      <td>1.535554e-10</td>\n",
       "      <td>1.174871e-10</td>\n",
       "      <td>1.274067e-12</td>\n",
       "      <td>6.783629e-13</td>\n",
       "      <td>7.623699e-08</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>3.216478e-09</td>\n",
       "      <td>5.155637e-13</td>\n",
       "      <td>7.990734e-10</td>\n",
       "      <td>1.821890e-08</td>\n",
       "      <td>8.297696e-09</td>\n",
       "      <td>9.532391e-09</td>\n",
       "      <td>6.062763e-13</td>\n",
       "      <td>4.525606e-10</td>\n",
       "      <td>0.187337</td>\n",
       "      <td>7.678191e-01</td>\n",
       "      <td>3.207933e-09</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.147073e-05</td>\n",
       "      <td>1.475435e-07</td>\n",
       "      <td>1.232974e-09</td>\n",
       "      <td>2.567746e-06</td>\n",
       "      <td>3.892232e-12</td>\n",
       "      <td>1.021793e-04</td>\n",
       "      <td>0.044670</td>\n",
       "      <td>8.229596e-07</td>\n",
       "      <td>4.422402e-13</td>\n",
       "      <td>1.417321e-09</td>\n",
       "      <td>1.225622e-05</td>\n",
       "      <td>1.733765e-05</td>\n",
       "      <td>7.913169e-11</td>\n",
       "      <td>8.105658e-08</td>\n",
       "      <td>1.764113e-09</td>\n",
       "      <td>0.999362</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>1.819358e-14</td>\n",
       "      <td>5.291691e-09</td>\n",
       "      <td>9.966475e-09</td>\n",
       "      <td>6.542968e-14</td>\n",
       "      <td>3.064274e-16</td>\n",
       "      <td>6.200213e-12</td>\n",
       "      <td>2.166220e-13</td>\n",
       "      <td>1.913936e-10</td>\n",
       "      <td>2.869724e-07</td>\n",
       "      <td>8.110437e-12</td>\n",
       "      <td>6.669734e-19</td>\n",
       "      <td>2.637020e-15</td>\n",
       "      <td>5.307910e-11</td>\n",
       "      <td>7.321822e-11</td>\n",
       "      <td>5.662729e-13</td>\n",
       "      <td>1.543808e-13</td>\n",
       "      <td>5.968376e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.019979</td>\n",
       "      <td>0.010034</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.004741</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>0.016694</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.298494</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.624385</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.1492</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.3028</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.3492</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.077124</td>\n",
       "      <td>0.014179</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.017430</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.051605</td>\n",
       "      <td>0.003278</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.005994</td>\n",
       "      <td>0.462109</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.002051</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.344156</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.119137</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>3.136365e-08</td>\n",
       "      <td>7.199874e-05</td>\n",
       "      <td>6.545521e-05</td>\n",
       "      <td>7.743306e-09</td>\n",
       "      <td>1.623164e-11</td>\n",
       "      <td>5.751525e-10</td>\n",
       "      <td>1.068964e-13</td>\n",
       "      <td>2.886634e-08</td>\n",
       "      <td>0.875192</td>\n",
       "      <td>1.724513e-08</td>\n",
       "      <td>1.845764e-13</td>\n",
       "      <td>5.002106e-09</td>\n",
       "      <td>4.061707e-09</td>\n",
       "      <td>2.432308e-03</td>\n",
       "      <td>7.688006e-08</td>\n",
       "      <td>8.530826e-12</td>\n",
       "      <td>5.536405e-07</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>6.101326e-08</td>\n",
       "      <td>2.910954e-09</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>3.484668e-07</td>\n",
       "      <td>2.579309e-08</td>\n",
       "      <td>1.646639e-11</td>\n",
       "      <td>6.765297e-12</td>\n",
       "      <td>4.121616e-14</td>\n",
       "      <td>2.615919e-10</td>\n",
       "      <td>0.999772</td>\n",
       "      <td>4.373644e-10</td>\n",
       "      <td>4.195139e-14</td>\n",
       "      <td>1.118071e-11</td>\n",
       "      <td>1.429955e-08</td>\n",
       "      <td>3.131265e-07</td>\n",
       "      <td>1.082345e-13</td>\n",
       "      <td>1.314704e-11</td>\n",
       "      <td>8.264869e-10</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>9.241681e-10</td>\n",
       "      <td>2.532094e-07</td>\n",
       "      <td>1.995125e-06</td>\n",
       "      <td>2.685778e-09</td>\n",
       "      <td>4.976275e-06</td>\n",
       "      <td>6.186190e-08</td>\n",
       "      <td>1.725046e-09</td>\n",
       "      <td>2.811346e-08</td>\n",
       "      <td>9.118110e-01</td>\n",
       "      <td>8.716271e-09</td>\n",
       "      <td>2.307553e-12</td>\n",
       "      <td>6.871327e-08</td>\n",
       "      <td>1.848035e-06</td>\n",
       "      <td>7.907541e-02</td>\n",
       "      <td>1.134578e-07</td>\n",
       "      <td>6.290770e-11</td>\n",
       "      <td>2.722544e-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0_proba_x  1_proba_x  2_proba_x  3_proba_x  4_proba_x  5_proba_x  \\\n",
       "0   0.031669   0.441127   0.001782   0.175464   0.002389   0.037579   \n",
       "1   0.008463   0.881487   0.000831   0.017043   0.001019   0.021602   \n",
       "2   0.093019   0.292943   0.002693   0.094578   0.003387   0.018731   \n",
       "3   0.533759   0.186844   0.003628   0.045731   0.003993   0.045217   \n",
       "4   0.019979   0.010034   0.000516   0.004741   0.001214   0.004336   \n",
       "\n",
       "   6_proba_x  7_proba_x  8_proba_x  9_proba_x  10_proba_x  11_proba_x  \\\n",
       "0   0.063476   0.005356   0.002257   0.023875    0.175142    0.012161   \n",
       "1   0.032368   0.001473   0.000847   0.016896    0.002923    0.004792   \n",
       "2   0.081917   0.005751   0.002520   0.055956    0.206233    0.030800   \n",
       "3   0.053578   0.007821   0.002985   0.020327    0.027389    0.009348   \n",
       "4   0.016694   0.004563   0.000455   0.004997    0.298494    0.001269   \n",
       "\n",
       "   12_proba_x  13_proba_x  14_proba_x  15_proba_x  16_proba_x  17_proba_x  \\\n",
       "0    0.003027    0.001950    0.000838    0.015173    0.001460    0.002880   \n",
       "1    0.001404    0.000730    0.000418    0.004859    0.000707    0.001239   \n",
       "2    0.004864    0.003710    0.001461    0.090085    0.002187    0.004168   \n",
       "3    0.006114    0.003455    0.001628    0.037426    0.003093    0.004004   \n",
       "4    0.002567    0.001071    0.000752    0.624385    0.000455    0.001615   \n",
       "\n",
       "   18_proba_x  0_proba_y  1_proba_y  2_proba_y  3_proba_y  4_proba_y  \\\n",
       "0    0.002395     0.1024     0.3552     0.0002     0.0992     0.0002   \n",
       "1    0.000899     0.0168     0.8154     0.0002     0.0350     0.0000   \n",
       "2    0.004997     0.1746     0.1836     0.0006     0.1022     0.0010   \n",
       "3    0.003661     0.4816     0.0860     0.0006     0.0762     0.0018   \n",
       "4    0.001862     0.1492     0.0210     0.0008     0.0178     0.0134   \n",
       "\n",
       "   5_proba_y  6_proba_y  7_proba_y  8_proba_y  9_proba_y  10_proba_y  \\\n",
       "0     0.0316     0.1304     0.0116     0.0034     0.0412      0.1276   \n",
       "1     0.0498     0.0418     0.0016     0.0006     0.0166      0.0054   \n",
       "2     0.0210     0.0746     0.0042     0.0024     0.0394      0.1270   \n",
       "3     0.0200     0.0470     0.0044     0.0004     0.0122      0.0752   \n",
       "4     0.0096     0.0350     0.0062     0.0004     0.0136      0.3028   \n",
       "\n",
       "   11_proba_y  12_proba_y  13_proba_y  14_proba_y  15_proba_y  16_proba_y  \\\n",
       "0      0.0190      0.0092      0.0002      0.0000      0.0512       0.000   \n",
       "1      0.0076      0.0000      0.0006      0.0000      0.0074       0.000   \n",
       "2      0.0432      0.0176      0.0018      0.0042      0.1600       0.000   \n",
       "3      0.0190      0.0214      0.0014      0.0240      0.0978       0.000   \n",
       "4      0.0082      0.0202      0.0052      0.0188      0.3492       0.001   \n",
       "\n",
       "   17_proba_y  18_proba_y  0_proba_x  1_proba_x  2_proba_x  3_proba_x  \\\n",
       "0      0.0162      0.0012        0.0        0.6        0.0        0.0   \n",
       "1      0.0012      0.0000        0.0        0.8        0.0        0.2   \n",
       "2      0.0328      0.0098        0.0        0.4        0.0        0.0   \n",
       "3      0.0294      0.0016        0.2        0.8        0.0        0.0   \n",
       "4      0.0186      0.0090        0.0        0.0        0.0        0.0   \n",
       "\n",
       "   4_proba_x  5_proba_x  6_proba_x  7_proba_x  8_proba_x  9_proba_x  \\\n",
       "0        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "1        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "2        0.0        0.0        0.2        0.0        0.0        0.2   \n",
       "3        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "4        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "   10_proba_x  11_proba_x  12_proba_x  13_proba_x  14_proba_x  15_proba_x  \\\n",
       "0         0.4         0.0         0.0         0.0         0.0         0.0   \n",
       "1         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "3         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "4         0.4         0.0         0.0         0.0         0.0         0.6   \n",
       "\n",
       "   16_proba_x  17_proba_x  18_proba_x  0_proba_y  1_proba_y  2_proba_y  \\\n",
       "0         0.0         0.0         0.0   0.020270   0.591118   0.001390   \n",
       "1         0.0         0.0         0.0   0.005188   0.845728   0.000357   \n",
       "2         0.0         0.2         0.0   0.062659   0.253161   0.003676   \n",
       "3         0.0         0.0         0.0   0.447850   0.161704   0.001688   \n",
       "4         0.0         0.0         0.0   0.077124   0.014179   0.000845   \n",
       "\n",
       "   3_proba_y  4_proba_y  5_proba_y  6_proba_y  7_proba_y  8_proba_y  \\\n",
       "0   0.076698   0.001630   0.013208   0.015505   0.022670   0.001507   \n",
       "1   0.038582   0.000395   0.014556   0.027319   0.000945   0.000409   \n",
       "2   0.216549   0.004394   0.026043   0.023008   0.014735   0.004070   \n",
       "3   0.075070   0.002133   0.056924   0.026587   0.004066   0.001655   \n",
       "4   0.017430   0.003509   0.005217   0.051605   0.003278   0.000820   \n",
       "\n",
       "   9_proba_y  10_proba_y  11_proba_y  12_proba_y  13_proba_y  14_proba_y  \\\n",
       "0   0.015394    0.148776    0.012079    0.002453    0.001991    0.001300   \n",
       "1   0.032406    0.004537    0.025768    0.000484    0.000544    0.000331   \n",
       "2   0.054802    0.149235    0.032038    0.006927    0.004962    0.003788   \n",
       "3   0.008121    0.109260    0.009079    0.006684    0.003642    0.002312   \n",
       "4   0.005994    0.462109    0.001720    0.002051    0.002199    0.003200   \n",
       "\n",
       "   15_proba_y  16_proba_y  17_proba_y  18_proba_y  0_proba_x  1_proba_x  \\\n",
       "0    0.067546    0.001159    0.004001    0.001304   0.481034   0.374553   \n",
       "1    0.000981    0.000295    0.000872    0.000305   0.573070   0.079127   \n",
       "2    0.057288    0.002972    0.075951    0.003743   0.806572   0.000636   \n",
       "3    0.076622    0.001384    0.003340    0.001880   0.985897   0.013341   \n",
       "4    0.344156    0.000824    0.002465    0.001275   0.119137   0.003101   \n",
       "\n",
       "      2_proba_x     3_proba_x     4_proba_x     5_proba_x     6_proba_x  \\\n",
       "0  6.250838e-06  8.488125e-02  5.771121e-06  3.099303e-04  1.461453e-04   \n",
       "1  2.785844e-07  2.294374e-05  1.792608e-04  7.888665e-06  9.447433e-06   \n",
       "2  2.580042e-06  6.057014e-02  1.033879e-04  2.398190e-04  2.423308e-07   \n",
       "3  5.429727e-12  1.496944e-07  4.233210e-07  1.535554e-10  1.174871e-10   \n",
       "4  3.136365e-08  7.199874e-05  6.545521e-05  7.743306e-09  1.623164e-11   \n",
       "\n",
       "      7_proba_x     8_proba_x     9_proba_x  10_proba_x    11_proba_x  \\\n",
       "0  3.566003e-07  1.441350e-08  1.626174e-03    0.049993  6.524021e-03   \n",
       "1  4.797030e-06  6.715825e-10  1.557029e-01    0.191702  7.186552e-05   \n",
       "2  3.745946e-06  6.820599e-07  4.296690e-03    0.085365  2.586398e-02   \n",
       "3  1.274067e-12  6.783629e-13  7.623699e-08    0.000761  3.216478e-09   \n",
       "4  5.751525e-10  1.068964e-13  2.886634e-08    0.875192  1.724513e-08   \n",
       "\n",
       "     12_proba_x    13_proba_x    14_proba_x    15_proba_x    16_proba_x  \\\n",
       "0  1.115529e-06  1.644272e-05  9.510262e-08  8.849970e-04  1.559794e-05   \n",
       "1  9.514194e-10  2.383866e-07  5.640025e-07  1.009518e-04  6.820856e-08   \n",
       "2  1.232927e-07  4.215697e-04  1.705838e-07  1.158001e-02  3.563197e-03   \n",
       "3  5.155637e-13  7.990734e-10  1.821890e-08  8.297696e-09  9.532391e-09   \n",
       "4  1.845764e-13  5.002106e-09  4.061707e-09  2.432308e-03  7.688006e-08   \n",
       "\n",
       "     17_proba_x    18_proba_x  0_proba_y     1_proba_y     2_proba_y  \\\n",
       "0  1.518542e-06  1.312220e-06   0.276344  4.974163e-03  3.196806e-07   \n",
       "1  5.948169e-07  1.545804e-08   0.000562  8.662991e-01  1.819706e-09   \n",
       "2  2.959465e-04  4.850493e-04   0.491533  2.232068e-03  1.448590e-05   \n",
       "3  6.062763e-13  4.525606e-10   0.187337  7.678191e-01  3.207933e-09   \n",
       "4  8.530826e-12  5.536405e-07   0.000226  6.101326e-08  2.910954e-09   \n",
       "\n",
       "   3_proba_y     4_proba_y     5_proba_y     6_proba_y     7_proba_y  \\\n",
       "0   0.000028  3.635841e-06  4.885335e-03  4.949490e-08  1.744127e-07   \n",
       "1   0.007660  1.195446e-07  4.581188e-06  4.465006e-05  3.523876e-06   \n",
       "2   0.000314  2.383762e-03  1.103980e-04  1.710348e-06  2.358376e-05   \n",
       "3   0.000027  1.147073e-05  1.475435e-07  1.232974e-09  2.567746e-06   \n",
       "4   0.000001  3.484668e-07  2.579309e-08  1.646639e-11  6.765297e-12   \n",
       "\n",
       "      8_proba_y     9_proba_y  10_proba_y    11_proba_y    12_proba_y  \\\n",
       "0  9.497017e-12  5.787967e-05    0.713638  4.377463e-06  1.541949e-07   \n",
       "1  7.000848e-10  1.082985e-01    0.000297  1.673466e-02  1.676316e-08   \n",
       "2  1.211643e-07  1.120005e-03    0.425814  4.230822e-04  3.531143e-05   \n",
       "3  3.892232e-12  1.021793e-04    0.044670  8.229596e-07  4.422402e-13   \n",
       "4  4.121616e-14  2.615919e-10    0.999772  4.373644e-10  4.195139e-14   \n",
       "\n",
       "     13_proba_y    14_proba_y    15_proba_y    16_proba_y    17_proba_y  \\\n",
       "0  3.370034e-05  2.815372e-07  2.940170e-05  4.646852e-07  5.648534e-10   \n",
       "1  1.725869e-06  1.140225e-05  7.543473e-05  3.884795e-07  6.601894e-06   \n",
       "2  2.149621e-04  2.756539e-03  7.284040e-02  7.481589e-06  1.729156e-04   \n",
       "3  1.417321e-09  1.225622e-05  1.733765e-05  7.913169e-11  8.105658e-08   \n",
       "4  1.118071e-11  1.429955e-08  3.131265e-07  1.082345e-13  1.314704e-11   \n",
       "\n",
       "     18_proba_y   0_proba   1_proba       2_proba       3_proba       4_proba  \\\n",
       "0  4.114973e-08  0.943114  0.014947  2.832391e-07  3.168322e-02  8.313984e-07   \n",
       "1  2.564135e-07  0.008326  0.968507  5.522135e-08  3.101615e-08  1.772393e-02   \n",
       "2  1.339113e-06  0.911892  0.000019  6.129648e-06  6.423689e-02  5.044804e-06   \n",
       "3  1.764113e-09  0.999362  0.000638  1.819358e-14  5.291691e-09  9.966475e-09   \n",
       "4  8.264869e-10  0.009052  0.000052  9.241681e-10  2.532094e-07  1.995125e-06   \n",
       "\n",
       "        5_proba       6_proba       7_proba       8_proba       9_proba  \\\n",
       "0  4.897588e-04  2.101600e-08  5.589885e-07  1.111513e-09  5.037563e-03   \n",
       "1  3.026936e-08  2.151280e-08  1.092184e-07  9.782048e-09  3.087210e-03   \n",
       "2  4.790473e-05  4.772185e-08  9.070901e-08  8.025390e-08  1.027245e-04   \n",
       "3  6.542968e-14  3.064274e-16  6.200213e-12  2.166220e-13  1.913936e-10   \n",
       "4  2.685778e-09  4.976275e-06  6.186190e-08  1.725046e-09  2.811346e-08   \n",
       "\n",
       "       10_proba      11_proba      12_proba      13_proba      14_proba  \\\n",
       "0  4.688866e-03  2.013336e-05  2.677102e-10  1.379965e-06  6.209310e-07   \n",
       "1  2.351299e-03  4.590864e-06  7.544784e-15  2.370737e-11  6.984537e-11   \n",
       "2  3.157057e-04  3.155915e-03  2.675100e-08  3.449972e-08  3.318110e-08   \n",
       "3  2.869724e-07  8.110437e-12  6.669734e-19  2.637020e-15  5.307910e-11   \n",
       "4  9.118110e-01  8.716271e-09  2.307553e-12  6.871327e-08  1.848035e-06   \n",
       "\n",
       "       15_proba      16_proba      17_proba      18_proba  \n",
       "0  1.455863e-05  1.057558e-07  4.065118e-07  5.204594e-07  \n",
       "1  4.949271e-07  1.316929e-11  1.755843e-09  4.897525e-14  \n",
       "2  2.009196e-02  7.357116e-07  1.261588e-04  5.562270e-08  \n",
       "3  7.321822e-11  5.662729e-13  1.543808e-13  5.968376e-16  \n",
       "4  7.907541e-02  1.134578e-07  6.290770e-11  2.722544e-11  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T09:26:23.638297Z",
     "start_time": "2020-07-26T09:26:23.622353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        0.0\n",
      "3        0.0\n",
      "4        0.0\n",
      "6        0.0\n",
      "7        0.0\n",
      "8        0.0\n",
      "9        0.0\n",
      "12       0.0\n",
      "14       0.0\n",
      "15       0.0\n",
      "16       0.0\n",
      "17       0.0\n",
      "18       0.0\n",
      "19       0.0\n",
      "20       0.0\n",
      "21       0.0\n",
      "23       0.0\n",
      "27       0.0\n",
      "28       0.0\n",
      "29       0.0\n",
      "30       0.0\n",
      "33       0.0\n",
      "35       0.0\n",
      "37       0.0\n",
      "38       0.0\n",
      "40       0.0\n",
      "41       0.0\n",
      "42       0.0\n",
      "43       0.0\n",
      "44       0.0\n",
      "45       0.0\n",
      "46       0.0\n",
      "47       0.0\n",
      "48       0.0\n",
      "49       0.0\n",
      "50       0.0\n",
      "51       0.0\n",
      "52       0.0\n",
      "53       0.0\n",
      "54       0.0\n",
      "55       0.0\n",
      "57       0.0\n",
      "59       0.0\n",
      "60       0.0\n",
      "61       0.0\n",
      "64       0.0\n",
      "66       0.0\n",
      "67       0.0\n",
      "68       0.0\n",
      "69       0.0\n",
      "70       0.0\n",
      "71       0.0\n",
      "72       0.0\n",
      "73       0.0\n",
      "74       0.0\n",
      "75       0.0\n",
      "77       0.0\n",
      "78       0.0\n",
      "79       0.0\n",
      "80       0.0\n",
      "81       0.0\n",
      "82       0.0\n",
      "84       0.0\n",
      "86       0.0\n",
      "87       0.0\n",
      "88       0.0\n",
      "89       0.0\n",
      "90       0.0\n",
      "91       0.0\n",
      "93       0.0\n",
      "95       0.0\n",
      "97       0.0\n",
      "98       0.0\n",
      "99       0.0\n",
      "101      0.0\n",
      "102      0.0\n",
      "103      0.0\n",
      "104      0.0\n",
      "107      0.0\n",
      "108      0.0\n",
      "109      0.0\n",
      "110      0.0\n",
      "111      0.0\n",
      "112      0.0\n",
      "113      0.0\n",
      "114      0.0\n",
      "115      0.0\n",
      "116      0.0\n",
      "117      0.0\n",
      "118      0.0\n",
      "119      0.0\n",
      "120      0.0\n",
      "121      0.0\n",
      "122      0.0\n",
      "123      0.0\n",
      "124      0.0\n",
      "125      0.0\n",
      "126      0.0\n",
      "127      0.0\n",
      "128      0.0\n",
      "129      0.0\n",
      "130      0.0\n",
      "132      0.0\n",
      "134      0.0\n",
      "136      0.0\n",
      "137      0.0\n",
      "138      0.0\n",
      "139      0.0\n",
      "140      0.0\n",
      "141      0.0\n",
      "142      0.0\n",
      "143      0.0\n",
      "144      0.0\n",
      "145      0.0\n",
      "146      0.0\n",
      "147      0.0\n",
      "148      0.0\n",
      "149      0.0\n",
      "150      0.0\n",
      "154      0.0\n",
      "156      0.0\n",
      "157      0.0\n",
      "158      0.0\n",
      "159      0.0\n",
      "160      0.0\n",
      "162      0.0\n",
      "163      0.0\n",
      "164      0.0\n",
      "166      0.0\n",
      "167      0.0\n",
      "168      0.0\n",
      "169      0.0\n",
      "171      0.0\n",
      "172      0.0\n",
      "173      0.0\n",
      "174      0.0\n",
      "175      0.0\n",
      "176      0.0\n",
      "178      0.0\n",
      "179      0.0\n",
      "180      0.0\n",
      "181      0.0\n",
      "182      0.0\n",
      "185      0.0\n",
      "187      0.0\n",
      "188      0.0\n",
      "189      0.0\n",
      "190      0.0\n",
      "191      0.0\n",
      "192      0.0\n",
      "193      0.0\n",
      "194      0.0\n",
      "196      0.0\n",
      "198      0.0\n",
      "199      0.0\n",
      "200      0.0\n",
      "201      0.0\n",
      "202      0.0\n",
      "203      0.0\n",
      "204      0.0\n",
      "205      0.0\n",
      "206      0.0\n",
      "207      0.0\n",
      "208      0.0\n",
      "209      0.0\n",
      "211      0.0\n",
      "212      0.0\n",
      "213      0.0\n",
      "214      0.0\n",
      "215      0.0\n",
      "216      0.0\n",
      "217      0.0\n",
      "218      0.0\n",
      "219      0.0\n",
      "222      0.0\n",
      "223      0.0\n",
      "224      0.0\n",
      "225      0.0\n",
      "226      0.0\n",
      "227      0.0\n",
      "228      0.0\n",
      "229      0.0\n",
      "230      0.0\n",
      "231      0.0\n",
      "232      0.0\n",
      "233      0.0\n",
      "234      0.0\n",
      "235      0.0\n",
      "236      0.0\n",
      "237      0.0\n",
      "238      0.0\n",
      "239      0.0\n",
      "241      1.0\n",
      "242      1.0\n",
      "243      1.0\n",
      "244      1.0\n",
      "246      1.0\n",
      "249      1.0\n",
      "250      1.0\n",
      "251      1.0\n",
      "253      1.0\n",
      "255      1.0\n",
      "256      1.0\n",
      "257      1.0\n",
      "258      1.0\n",
      "259      1.0\n",
      "260      1.0\n",
      "261      1.0\n",
      "264      1.0\n",
      "265      1.0\n",
      "266      1.0\n",
      "267      1.0\n",
      "268      1.0\n",
      "269      1.0\n",
      "271      1.0\n",
      "272      1.0\n",
      "273      1.0\n",
      "274      1.0\n",
      "276      1.0\n",
      "280      1.0\n",
      "281      1.0\n",
      "282      1.0\n",
      "283      1.0\n",
      "285      1.0\n",
      "286      1.0\n",
      "288      1.0\n",
      "289      1.0\n",
      "290      1.0\n",
      "291      1.0\n",
      "292      1.0\n",
      "293      1.0\n",
      "294      1.0\n",
      "295      1.0\n",
      "296      1.0\n",
      "299      1.0\n",
      "300      1.0\n",
      "301      1.0\n",
      "302      1.0\n",
      "303      1.0\n",
      "304      1.0\n",
      "305      1.0\n",
      "306      1.0\n",
      "307      1.0\n",
      "308      1.0\n",
      "309      1.0\n",
      "310      1.0\n",
      "311      1.0\n",
      "312      1.0\n",
      "313      1.0\n",
      "315      1.0\n",
      "317      1.0\n",
      "318      1.0\n",
      "320      1.0\n",
      "322      1.0\n",
      "325      1.0\n",
      "327      1.0\n",
      "328      1.0\n",
      "329      1.0\n",
      "330      1.0\n",
      "331      1.0\n",
      "332      1.0\n",
      "333      1.0\n",
      "334      1.0\n",
      "335      1.0\n",
      "336      1.0\n",
      "338      1.0\n",
      "339      1.0\n",
      "341      1.0\n",
      "342      1.0\n",
      "344      1.0\n",
      "345      1.0\n",
      "346      1.0\n",
      "347      1.0\n",
      "349      1.0\n",
      "350      1.0\n",
      "351      1.0\n",
      "352      1.0\n",
      "353      1.0\n",
      "354      1.0\n",
      "355      1.0\n",
      "356      1.0\n",
      "359      1.0\n",
      "360      1.0\n",
      "361      1.0\n",
      "362      1.0\n",
      "363      1.0\n",
      "364      1.0\n",
      "365      1.0\n",
      "366      1.0\n",
      "367      1.0\n",
      "369      1.0\n",
      "370      1.0\n",
      "371      1.0\n",
      "372      1.0\n",
      "374      1.0\n",
      "376      1.0\n",
      "377      1.0\n",
      "378      1.0\n",
      "379      1.0\n",
      "381      1.0\n",
      "        ... \n",
      "6918    17.0\n",
      "6920    17.0\n",
      "6921    17.0\n",
      "6922    17.0\n",
      "6923    17.0\n",
      "6924    17.0\n",
      "6925    17.0\n",
      "6926    17.0\n",
      "6927    17.0\n",
      "6928    17.0\n",
      "6929    17.0\n",
      "6930    17.0\n",
      "6933    17.0\n",
      "6934    17.0\n",
      "6935    17.0\n",
      "6937    17.0\n",
      "6938    17.0\n",
      "6939    17.0\n",
      "6940    17.0\n",
      "6941    17.0\n",
      "6942    17.0\n",
      "6943    17.0\n",
      "6944    17.0\n",
      "6946    17.0\n",
      "6947    18.0\n",
      "6948    18.0\n",
      "6949    18.0\n",
      "6950    18.0\n",
      "6951    18.0\n",
      "6953    18.0\n",
      "6954    18.0\n",
      "6955    18.0\n",
      "6956    18.0\n",
      "6957    18.0\n",
      "6958    18.0\n",
      "6959    18.0\n",
      "6960    18.0\n",
      "6961    18.0\n",
      "6962    18.0\n",
      "6964    18.0\n",
      "6965    18.0\n",
      "6966    18.0\n",
      "6967    18.0\n",
      "6969    18.0\n",
      "6970    18.0\n",
      "6971    18.0\n",
      "6974    18.0\n",
      "6975    18.0\n",
      "6976    18.0\n",
      "6977    18.0\n",
      "6978    18.0\n",
      "6979    18.0\n",
      "6980    18.0\n",
      "6981    18.0\n",
      "6982    18.0\n",
      "6983    18.0\n",
      "6984    18.0\n",
      "6986    18.0\n",
      "6987    18.0\n",
      "6988    18.0\n",
      "6989    18.0\n",
      "6990    18.0\n",
      "6991    18.0\n",
      "6992    18.0\n",
      "6993    18.0\n",
      "6994    18.0\n",
      "6995    18.0\n",
      "6997    18.0\n",
      "6998    18.0\n",
      "6999    18.0\n",
      "7000    18.0\n",
      "7001    18.0\n",
      "7002    18.0\n",
      "7003    18.0\n",
      "7004    18.0\n",
      "7005    18.0\n",
      "7006    18.0\n",
      "7007    18.0\n",
      "7008    18.0\n",
      "7009    18.0\n",
      "7010    18.0\n",
      "7011    18.0\n",
      "7012    18.0\n",
      "7013    18.0\n",
      "7014    18.0\n",
      "7015    18.0\n",
      "7016    18.0\n",
      "7017    18.0\n",
      "7019    18.0\n",
      "7020    18.0\n",
      "7021    18.0\n",
      "7022    18.0\n",
      "7023    18.0\n",
      "7025    18.0\n",
      "7027    18.0\n",
      "7028    18.0\n",
      "7030    18.0\n",
      "7031    18.0\n",
      "7033    18.0\n",
      "7034    18.0\n",
      "7036    18.0\n",
      "7037    18.0\n",
      "7038    18.0\n",
      "7039    18.0\n",
      "7040    18.0\n",
      "7041    18.0\n",
      "7042    18.0\n",
      "7046    18.0\n",
      "7048    18.0\n",
      "7049    18.0\n",
      "7050    18.0\n",
      "7052    18.0\n",
      "7053    18.0\n",
      "7054    18.0\n",
      "7055    18.0\n",
      "7056    18.0\n",
      "7058    18.0\n",
      "7059    18.0\n",
      "7060    18.0\n",
      "7061    18.0\n",
      "7062    18.0\n",
      "7063    18.0\n",
      "7064    18.0\n",
      "7065    18.0\n",
      "7066    18.0\n",
      "7067    18.0\n",
      "7068    18.0\n",
      "7070    18.0\n",
      "7071    18.0\n",
      "7073    18.0\n",
      "7074    18.0\n",
      "7075    18.0\n",
      "7076    18.0\n",
      "7077    18.0\n",
      "7078    18.0\n",
      "7080    18.0\n",
      "7081    18.0\n",
      "7082    18.0\n",
      "7083    18.0\n",
      "7086    18.0\n",
      "7088    18.0\n",
      "7089    18.0\n",
      "7091    18.0\n",
      "7092    18.0\n",
      "7093    18.0\n",
      "7094    18.0\n",
      "7095    18.0\n",
      "7096    18.0\n",
      "7097    18.0\n",
      "7098    18.0\n",
      "7099    18.0\n",
      "7100    18.0\n",
      "7101    18.0\n",
      "7103    18.0\n",
      "7104    18.0\n",
      "7105    18.0\n",
      "7106    18.0\n",
      "7107    18.0\n",
      "7108    18.0\n",
      "7109    18.0\n",
      "7113    18.0\n",
      "7114    18.0\n",
      "7115    18.0\n",
      "7116    18.0\n",
      "7117    18.0\n",
      "7118    18.0\n",
      "7119    18.0\n",
      "7121    18.0\n",
      "7122    18.0\n",
      "7123    18.0\n",
      "7124    18.0\n",
      "7125    18.0\n",
      "7126    18.0\n",
      "7127    18.0\n",
      "7128    18.0\n",
      "7130    18.0\n",
      "7131    18.0\n",
      "7135    18.0\n",
      "7136    18.0\n",
      "7137    18.0\n",
      "7138    18.0\n",
      "7139    18.0\n",
      "7140    18.0\n",
      "7141    18.0\n",
      "7143    18.0\n",
      "7144    18.0\n",
      "7145    18.0\n",
      "7146    18.0\n",
      "7147    18.0\n",
      "7148    18.0\n",
      "7149    18.0\n",
      "7150    18.0\n",
      "7151    18.0\n",
      "7152    18.0\n",
      "7154    18.0\n",
      "7155    18.0\n",
      "7156    18.0\n",
      "7157    18.0\n",
      "7158    18.0\n",
      "7159    18.0\n",
      "7160    18.0\n",
      "7161    18.0\n",
      "7162    18.0\n",
      "7163    18.0\n",
      "7164    18.0\n",
      "7166    18.0\n",
      "7167    18.0\n",
      "7168    18.0\n",
      "7170    18.0\n",
      "7171    18.0\n",
      "7173    18.0\n",
      "7174    18.0\n",
      "7175    18.0\n",
      "7176    18.0\n",
      "7177    18.0\n",
      "7178    18.0\n",
      "7179    18.0\n",
      "7180    18.0\n",
      "7181    18.0\n",
      "7182    18.0\n",
      "7183    18.0\n",
      "7184    18.0\n",
      "7185    18.0\n",
      "7186    18.0\n",
      "7187    18.0\n",
      "7188    18.0\n",
      "7189    18.0\n",
      "7190    18.0\n",
      "7192    18.0\n",
      "7193    18.0\n",
      "7194    18.0\n",
      "7196    18.0\n",
      "7198    18.0\n",
      "7199    18.0\n",
      "7202    18.0\n",
      "7203    18.0\n",
      "7204    18.0\n",
      "7205    18.0\n",
      "7206    18.0\n",
      "7207    18.0\n",
      "7209    18.0\n",
      "7210    18.0\n",
      "7211    18.0\n",
      "7212    18.0\n",
      "7213    18.0\n",
      "7214    18.0\n",
      "7215    18.0\n",
      "7216    18.0\n",
      "7218    18.0\n",
      "7219    18.0\n",
      "7220    18.0\n",
      "7221    18.0\n",
      "7223    18.0\n",
      "7225    18.0\n",
      "7226    18.0\n",
      "7227    18.0\n",
      "7228    18.0\n",
      "7229    18.0\n",
      "7230    18.0\n",
      "7231    18.0\n",
      "7233    18.0\n",
      "7235    18.0\n",
      "7236    18.0\n",
      "7237    18.0\n",
      "7239    18.0\n",
      "7240    18.0\n",
      "7242    18.0\n",
      "7243    18.0\n",
      "7244    18.0\n",
      "7248    18.0\n",
      "7249    18.0\n",
      "7250    18.0\n",
      "7252    18.0\n",
      "7253    18.0\n",
      "7255    18.0\n",
      "7256    18.0\n",
      "7257    18.0\n",
      "7258    18.0\n",
      "7259    18.0\n",
      "7260    18.0\n",
      "7261    18.0\n",
      "7262    18.0\n",
      "7264    18.0\n",
      "7265    18.0\n",
      "7266    18.0\n",
      "7267    18.0\n",
      "7268    18.0\n",
      "7271    18.0\n",
      "7272    18.0\n",
      "7273    18.0\n",
      "7274    18.0\n",
      "7275    18.0\n",
      "7279    18.0\n",
      "7280    18.0\n",
      "7281    18.0\n",
      "7282    18.0\n",
      "7283    18.0\n",
      "7286    18.0\n",
      "7287    18.0\n",
      "7291    18.0\n",
      "Name: behavior_id, Length: 5833, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for fold, (trn_idx, val_idx) in enumerate(kfold.split(x, y_)):\n",
    "    # = to_categorical(y, num_classes=19)\n",
    "    print(y.iloc[trn_idx])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T09:26:11.539836Z",
     "start_time": "2020-07-26T09:26:11.534847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T09:38:19.450572Z",
     "start_time": "2020-07-26T09:38:16.413676Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "        4.9800e+00],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "        9.1400e+00],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "        4.0300e+00],\n",
       "       ...,\n",
       "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        5.6400e+00],\n",
       "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "        6.4800e+00],\n",
       "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        7.8800e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  target  \n",
       "0     15.3  396.90   4.98    24.0  \n",
       "1     17.8  396.90   9.14    21.6  \n",
       "2     17.8  392.83   4.03    34.7  \n",
       "3     18.7  394.63   2.94    33.4  \n",
       "4     18.7  396.90   5.33    36.2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "boston_data = datasets.load_boston()\n",
    "df_boston = pd.DataFrame(boston_data.data,columns=boston_data.feature_names)\n",
    "df_boston['target'] = pd.Series(boston_data.target)\n",
    "df_boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x253eb195b80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x253eb195b80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df_boston.loc[df_boston['target']>25,'INDUS'])\n",
    "sns.distplot(df_boston.loc[df_boston['target']<=25,'INDUS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x253eb335d90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x253eb335d90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df_boston.loc[df_boston['target']>30,'CRIM'])\n",
    "sns.distplot(df_boston.loc[df_boston['target']<=30,'CRIM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "def JS_divergence(p,q):\n",
    "    M=(p+q)/2\n",
    "    return 0.5*scipy.stats.entropy(p, M)+0.5*scipy.stats.entropy(q, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07926668754001932\n"
     ]
    }
   ],
   "source": [
    "print(JS_divergence(df_boston['target'],df_boston['target'].sample(frac=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_divergence(p,q):\n",
    "    return scipy.stats.entropy(p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1512329814093707\n"
     ]
    }
   ],
   "source": [
    "print(KL_divergence(df_boston['target'],df_boston['target'].sample(frac=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor(df_boston['target'])\n",
    "target = torch.tensor(df_boston['target'].sample(frac=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-520.1946, dtype=torch.float64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.kl_div(input, input, size_average=None, reduce=None, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-826a23617415>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkl_divergence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2.\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2.\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\distributions\\kl.py\u001b[0m in \u001b[0;36mkl_divergence\u001b[1;34m(p, q)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0m_KL_MEMOIZE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfun\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.distributions.kl.kl_divergence(torch.tensor([1.,2.,3.]),torch.tensor([1.,2.,3.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
