{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T02:27:32.700147Z",
     "start_time": "2020-07-24T02:27:31.459679Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "h6WZJIXQGf8H",
    "outputId": "3c741f05-224a-45f3-b2a9-269a6f6a316a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.backend import expand_dims\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers import Input\n",
    "\n",
    "#from keras.models import Model\n",
    "#from keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\n",
    "#from keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout\n",
    "\n",
    "from utils.constants import NB_CLASSES_LIST, MAX_TIMESTEPS_LIST\n",
    "from utils.keras_utils import train_model, evaluate_model, set_trainable\n",
    "#from utils.layer_utils import AttentionLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import multi_gpu_model\n",
    "import os\n",
    "# 使用第一张与第三张GPU卡\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KqMW3r5V1L58"
   },
   "outputs": [],
   "source": [
    "def acc_combo(y, y_pred):\n",
    "    # 数值ID与行为编码的对应关系\n",
    "    mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
    "        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
    "        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
    "        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
    "        16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
    "    # 将行为ID转为编码\n",
    "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
    "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
    "        return 1.0\n",
    "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
    "        return 1.0/7\n",
    "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
    "        return 1.0/3\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def squeeze_excite_block(input):\n",
    "    ''' Create a squeeze-excite block\n",
    "    Args:\n",
    "        input: input tensor\n",
    "        filters: number of output filters\n",
    "        k: width factor\n",
    "    Returns: a keras tensor\n",
    "    '''\n",
    "    filters = input.shape[-1] # channel_axis = -1 for TF\n",
    "\n",
    "    se = GlobalAveragePooling1D()(input)\n",
    "    se = Reshape((1, filters))(se)\n",
    "    se = Dense(filters // 16,  activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = multiply([input, se])\n",
    "    return se\n",
    "\n",
    "def get_dic(df,  main_col, fea_col, agg):\n",
    "    dic = df.groupby(main_col)[fea_col].agg(agg).to_dict()\n",
    "    fea_name = '_'.join([main_col, fea_col, agg])\n",
    "    return fea_name, dic\n",
    "    \n",
    "def get_1st_order_xyz_features(df, fea_cols, main_col = 'fragment_id'): \n",
    "    df_fea           = pd.DataFrame()\n",
    "    df_fea[main_col] = df[main_col].unique()\n",
    "    ## count 特征 ##\n",
    "    _, dic = get_dic(df, main_col, fea_cols[0], 'count') \n",
    "    df_fea['cnt']    = df_fea[main_col].map(dic).values\n",
    "    \n",
    "    ## 数值统计特征 ##\n",
    "    for f in tqdm(fea_cols):\n",
    "        for agg in ['min','max','mean','std','skew','median']:\n",
    "\n",
    "            fea_name, dic       = get_dic(df, main_col, f, agg) \n",
    "            df_fea[fea_name]    = df_fea[main_col].map(dic).values\n",
    "            \n",
    "        df_fea['_'.join([main_col, f, 'gap'])]   = df_fea['_'.join([main_col, f, 'max'])] - df_fea['_'.join([main_col, f, 'min'])]\n",
    "        df_fea['_'.join([main_col, f, 'skew2'])] = (df_fea['_'.join([main_col, f, 'mean'])] - df_fea['_'.join([main_col, f, 'median'])]) / df_fea['_'.join([main_col, f, 'std'])]\n",
    "        \n",
    "    return df_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['behavior_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "colab_type": "code",
    "id": "em-v2blx7vY2",
    "outputId": "dec0769e-74f5-4dbd-f055-8b599c5f6786"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7292/7292 [00:09<00:00, 740.16it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 7500/7500 [00:09<00:00, 754.80it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = pd.read_csv('sensor_train.csv')\n",
    "test = pd.read_csv('sensor_test.csv')\n",
    "#train_fly = pd.read_csv('train_fly.csv')\n",
    "#test_fly = pd.read_csv('test_fly.csv')\n",
    "sub = pd.read_csv('sub.csv')\n",
    "y = train.groupby('fragment_id')['behavior_id'].min()\n",
    "\n",
    "train['mod'] = (train.acc_x ** 2 + train.acc_y ** 2 + train.acc_z ** 2) ** .5\n",
    "train['modg'] = (train.acc_xg ** 2 + train.acc_yg ** 2 + train.acc_zg ** 2) ** .5\n",
    "train['xy'] = (train.acc_x ** 2 + train.acc_y ** 2) ** .5\n",
    "train['xy_g'] = (train.acc_xg ** 2 + train.acc_yg ** 2) ** .5\n",
    "test['mod'] = (test.acc_x ** 2 + test.acc_y ** 2 + test.acc_z ** 2) ** .5\n",
    "test['modg'] = (test.acc_xg ** 2 + test.acc_yg ** 2 + test.acc_zg ** 2) ** .5\n",
    "test['xy'] = (test.acc_x ** 2 + test.acc_y ** 2) ** .5\n",
    "test['xy_g'] = (test.acc_xg ** 2 + test.acc_yg ** 2) ** .5\n",
    "\n",
    "#origin_fea_cols = ['acc_x','acc_y','acc_z','acc_xg','acc_yg','acc_zg','mod','modg','xy','xy_g']\n",
    "#train_xyz_fea1 = get_1st_order_xyz_features(train,origin_fea_cols,main_col='fragment_id')\n",
    "#test_xyz_fea1 = get_1st_order_xyz_features(test,origin_fea_cols,main_col='fragment_id')\n",
    "\n",
    "#train = train.merge(train_xyz_fea1,how='left',on='fragment_id')\n",
    "#test = test.merge(test_xyz_fea1,how='left',on='fragment_id')\n",
    "feature_cols = [col for col in train.columns if col not in ['fragment_id','time_point','behavior_id']]\n",
    "std_scaler = StandardScaler()\n",
    "train[feature_cols] = std_scaler.fit_transform(train[feature_cols])\n",
    "test[feature_cols] = std_scaler.transform(test[feature_cols])\n",
    "\n",
    "feauture_len = len(feature_cols)\n",
    "\n",
    "x = np.zeros((7292, 60, feauture_len, 1))\n",
    "t = np.zeros((7500, 60, feauture_len, 1))\n",
    "for i in tqdm(range(7292)):\n",
    "    tmp = train[train.fragment_id == i][:60]\n",
    "    x[i,:,:, 0] = resample(tmp.drop(['fragment_id', 'time_point', 'behavior_id'],\n",
    "                                    axis=1), 60, np.array(tmp.time_point))[0]\n",
    "for i in tqdm(range(7500)):\n",
    "    tmp = test[test.fragment_id == i][:60]\n",
    "    t[i,:,:, 0] = resample(tmp.drop(['fragment_id', 'time_point'],\n",
    "                                    axis=1), 60, np.array(tmp.time_point))[0]\n",
    "                  \n",
    "\n",
    "kfold = StratifiedKFold(5,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1cd3jgdFAHs"
   },
   "outputs": [],
   "source": [
    "def Net():\n",
    "    input = Input(shape=(60, 8, 1))\n",
    "    X = Conv2D(filters=64,\n",
    "               kernel_size=(3, 3),\n",
    "               activation='relu',\n",
    "               kernel_initializer='he_normal',\n",
    "               padding='same')(input)\n",
    "    X = Conv2D(filters=128,\n",
    "               kernel_size=(3, 3),\n",
    "               activation='relu',\n",
    "               kernel_initializer='he_normal',\n",
    "               padding='same')(X)\n",
    "    X = MaxPooling2D()(X)\n",
    "    X = Conv2D(filters=256,\n",
    "               kernel_size=(3, 3),\n",
    "               activation='relu',\n",
    "               kernel_initializer='he_normal',\n",
    "               padding='same')(X)\n",
    "    X = Conv2D(filters=512,\n",
    "               kernel_size=(3, 3),\n",
    "               activation='relu',\n",
    "               kernel_initializer='he_normal',\n",
    "               padding='same')(X)\n",
    "\n",
    "    X = GlobalMaxPooling2D()(X)\n",
    "    X = Dropout(0.3)(X)\n",
    "    X = expand_dims(X,axis=-1)\n",
    "\n",
    "    \n",
    "    \n",
    "    gru_1 = GRU(128,return_sequences=True,kernel_initializer='he_normal', name='gru1')(X)\n",
    "    gru_1b = GRU(128,return_sequences=True,go_backwards=True,kernel_initializer='he_normal',name='gru1_b')(X)\n",
    "    gru1_merged = add([gru_1, gru_1b])\n",
    "    X = Dense(19, kernel_initializer='he_normal',\n",
    "                  name='dense')(gru1_merged)\n",
    "\n",
    "    X = Activation('softmax', name='softmax')(X)\n",
    "\n",
    "\n",
    "    return Model([input], X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y5kh8RFvYpxQ"
   },
   "outputs": [],
   "source": [
    "t = t.reshape(7500, 60, 10)\n",
    "x = x.reshape(7292, 60, 10)\n",
    "MAX_TIMESTEPS = 60\n",
    "MAX_NB_VARIABLES = 10\n",
    "NB_CLASS = 19\n",
    "x = x.swapaxes(1,2)\n",
    "t = t.swapaxes(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9TZkoqWd82EL",
    "outputId": "527032b8-5341-42cd-9600-099632a4d200"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 10, 60)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute_4 (Permute)             (None, 60, 10)       0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 60, 128)      10368       permute_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 60, 128)      512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 60, 128)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1, 128)       0           global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1, 8)         1024        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 128)       1024        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 60, 128)      0           activation_2[0][0]               \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 60, 256)      164096      multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 60, 256)      1024        conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 60, 256)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 256)       0           global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 16)        4096        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 256)       4096        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 60, 256)      0           activation_3[0][0]               \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 60, 128)      98432       multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "masking_3 (Masking)             (None, 10, 60)       0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 60, 128)      512         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 8)            2208        masking_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 60, 128)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 8)            0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 136)          0           dropout_2[0][0]                  \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 19)           2603        concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 289,995\n",
      "Trainable params: 288,971\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "12/12 [==============================] - 6s 464ms/step - loss: 2.5061 - acc: 0.2133 - val_loss: 2.8776 - val_acc: 0.0836\n",
      "Epoch 2/500\n",
      "12/12 [==============================] - 4s 368ms/step - loss: 2.0712 - acc: 0.3465 - val_loss: 2.7963 - val_acc: 0.1062\n",
      "Epoch 3/500\n",
      "12/12 [==============================] - 4s 368ms/step - loss: 1.8770 - acc: 0.3856 - val_loss: 2.6717 - val_acc: 0.1549\n",
      "Epoch 4/500\n",
      "12/12 [==============================] - 4s 367ms/step - loss: 1.7444 - acc: 0.4216 - val_loss: 2.5579 - val_acc: 0.1988\n",
      "Epoch 5/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 1.6471 - acc: 0.4504 - val_loss: 2.4886 - val_acc: 0.1837\n",
      "Epoch 6/500\n",
      "12/12 [==============================] - 5s 427ms/step - loss: 1.5636 - acc: 0.4733 - val_loss: 2.3565 - val_acc: 0.2324\n",
      "Epoch 7/500\n",
      "12/12 [==============================] - 4s 364ms/step - loss: 1.4766 - acc: 0.5021 - val_loss: 2.2045 - val_acc: 0.3550\n",
      "Epoch 8/500\n",
      "12/12 [==============================] - 4s 366ms/step - loss: 1.4114 - acc: 0.5210 - val_loss: 2.0962 - val_acc: 0.3790\n",
      "Epoch 9/500\n",
      "12/12 [==============================] - 4s 368ms/step - loss: 1.3564 - acc: 0.5292 - val_loss: 2.0311 - val_acc: 0.3893\n",
      "Epoch 10/500\n",
      "12/12 [==============================] - 4s 363ms/step - loss: 1.2956 - acc: 0.5548 - val_loss: 1.9918 - val_acc: 0.3900\n",
      "Epoch 11/500\n",
      "12/12 [==============================] - 5s 376ms/step - loss: 1.2516 - acc: 0.5613 - val_loss: 1.8672 - val_acc: 0.4270\n",
      "Epoch 12/500\n",
      "12/12 [==============================] - 4s 368ms/step - loss: 1.2087 - acc: 0.5767 - val_loss: 1.7995 - val_acc: 0.4462\n",
      "Epoch 13/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 1.1702 - acc: 0.5891 - val_loss: 1.8114 - val_acc: 0.4421\n",
      "Epoch 14/500\n",
      "12/12 [==============================] - 4s 369ms/step - loss: 1.1345 - acc: 0.6055 - val_loss: 1.7373 - val_acc: 0.4483\n",
      "Epoch 15/500\n",
      "12/12 [==============================] - 4s 349ms/step - loss: 1.0928 - acc: 0.6275 - val_loss: 1.6981 - val_acc: 0.4380\n",
      "Epoch 16/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 1.0657 - acc: 0.6307 - val_loss: 1.7020 - val_acc: 0.4339\n",
      "Epoch 17/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 1.0387 - acc: 0.6384 - val_loss: 1.6257 - val_acc: 0.4455\n",
      "Epoch 18/500\n",
      "12/12 [==============================] - 4s 349ms/step - loss: 1.0213 - acc: 0.6477 - val_loss: 1.6741 - val_acc: 0.4154\n",
      "Epoch 19/500\n",
      "12/12 [==============================] - 4s 364ms/step - loss: 0.9793 - acc: 0.6664 - val_loss: 1.5795 - val_acc: 0.4517\n",
      "Epoch 20/500\n",
      "12/12 [==============================] - 4s 365ms/step - loss: 0.9482 - acc: 0.6732 - val_loss: 1.5495 - val_acc: 0.4654\n",
      "Epoch 21/500\n",
      "12/12 [==============================] - 4s 371ms/step - loss: 0.9228 - acc: 0.6854 - val_loss: 1.4978 - val_acc: 0.5010\n",
      "Epoch 22/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.9072 - acc: 0.6825 - val_loss: 1.5300 - val_acc: 0.4839\n",
      "Epoch 23/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.8902 - acc: 0.7003 - val_loss: 1.4904 - val_acc: 0.4846\n",
      "Epoch 24/500\n",
      "12/12 [==============================] - 5s 410ms/step - loss: 0.8609 - acc: 0.7091 - val_loss: 1.3873 - val_acc: 0.5250\n",
      "Epoch 25/500\n",
      "12/12 [==============================] - 5s 411ms/step - loss: 0.8408 - acc: 0.7163 - val_loss: 1.3785 - val_acc: 0.5278\n",
      "Epoch 26/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.8072 - acc: 0.7367 - val_loss: 1.4719 - val_acc: 0.4894\n",
      "Epoch 27/500\n",
      "12/12 [==============================] - 5s 394ms/step - loss: 0.7842 - acc: 0.7398 - val_loss: 1.3783 - val_acc: 0.5332\n",
      "Epoch 28/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.7674 - acc: 0.7399 - val_loss: 1.5448 - val_acc: 0.4736\n",
      "Epoch 29/500\n",
      "12/12 [==============================] - 4s 371ms/step - loss: 0.7539 - acc: 0.7550 - val_loss: 1.3736 - val_acc: 0.5415\n",
      "Epoch 30/500\n",
      "12/12 [==============================] - 4s 349ms/step - loss: 0.7288 - acc: 0.7554 - val_loss: 1.4318 - val_acc: 0.5010\n",
      "Epoch 31/500\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7213 - acc: 0.760 - 4s 351ms/step - loss: 0.7213 - acc: 0.7600 - val_loss: 1.4081 - val_acc: 0.5086\n",
      "Epoch 32/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.7027 - acc: 0.7668 - val_loss: 1.3690 - val_acc: 0.5380\n",
      "Epoch 33/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.6782 - acc: 0.7819 - val_loss: 1.3966 - val_acc: 0.5161\n",
      "Epoch 34/500\n",
      "12/12 [==============================] - 4s 372ms/step - loss: 0.6696 - acc: 0.7802 - val_loss: 1.3625 - val_acc: 0.5456\n",
      "Epoch 35/500\n",
      "12/12 [==============================] - 4s 355ms/step - loss: 0.6578 - acc: 0.7850 - val_loss: 1.3557 - val_acc: 0.5415\n",
      "Epoch 36/500\n",
      "12/12 [==============================] - 5s 385ms/step - loss: 0.6408 - acc: 0.7864 - val_loss: 1.3224 - val_acc: 0.5469\n",
      "Epoch 37/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.6550 - acc: 0.7835 - val_loss: 1.7307 - val_acc: 0.4668\n",
      "Epoch 38/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.6105 - acc: 0.8037 - val_loss: 1.4331 - val_acc: 0.5106\n",
      "Epoch 39/500\n",
      "12/12 [==============================] - 5s 386ms/step - loss: 0.5872 - acc: 0.8128 - val_loss: 1.3237 - val_acc: 0.5497\n",
      "Epoch 40/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.5607 - acc: 0.8222 - val_loss: 1.3924 - val_acc: 0.5476\n",
      "Epoch 41/500\n",
      "12/12 [==============================] - 4s 373ms/step - loss: 0.5575 - acc: 0.8219 - val_loss: 1.3306 - val_acc: 0.5579\n",
      "Epoch 42/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.5367 - acc: 0.8325 - val_loss: 1.5044 - val_acc: 0.5264\n",
      "Epoch 43/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.5468 - acc: 0.8181 - val_loss: 1.4848 - val_acc: 0.5271\n",
      "Epoch 44/500\n",
      "12/12 [==============================] - 5s 410ms/step - loss: 0.5317 - acc: 0.8272 - val_loss: 1.3044 - val_acc: 0.5593\n",
      "Epoch 45/500\n",
      "12/12 [==============================] - 4s 365ms/step - loss: 0.5167 - acc: 0.8296 - val_loss: 1.3111 - val_acc: 0.5970\n",
      "Epoch 46/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.5245 - acc: 0.8272 - val_loss: 1.2620 - val_acc: 0.5778\n",
      "Epoch 47/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.5008 - acc: 0.8407 - val_loss: 1.4206 - val_acc: 0.5264\n",
      "Epoch 48/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.4759 - acc: 0.8495 - val_loss: 1.2595 - val_acc: 0.5812\n",
      "Epoch 49/500\n",
      "12/12 [==============================] - 4s 352ms/step - loss: 0.4727 - acc: 0.8455 - val_loss: 1.2769 - val_acc: 0.5764\n",
      "Epoch 50/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.4601 - acc: 0.8507 - val_loss: 1.3959 - val_acc: 0.5751\n",
      "Epoch 51/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.4507 - acc: 0.8532 - val_loss: 1.3507 - val_acc: 0.5497\n",
      "Epoch 52/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.4614 - acc: 0.8500 - val_loss: 1.2779 - val_acc: 0.5922\n",
      "Epoch 53/500\n",
      "12/12 [==============================] - 4s 352ms/step - loss: 0.4494 - acc: 0.8551 - val_loss: 1.4025 - val_acc: 0.5689\n",
      "Epoch 54/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.4373 - acc: 0.8611 - val_loss: 1.3970 - val_acc: 0.5565\n",
      "Epoch 55/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.4242 - acc: 0.8642 - val_loss: 1.2846 - val_acc: 0.5908\n",
      "Epoch 56/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.4021 - acc: 0.8743 - val_loss: 1.3413 - val_acc: 0.5942\n",
      "Epoch 57/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.3986 - acc: 0.8728 - val_loss: 1.3280 - val_acc: 0.5846\n",
      "Epoch 58/500\n",
      "12/12 [==============================] - 4s 349ms/step - loss: 0.4060 - acc: 0.8709 - val_loss: 1.3742 - val_acc: 0.5798\n",
      "Epoch 59/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.3934 - acc: 0.8781 - val_loss: 1.3322 - val_acc: 0.5949\n",
      "Epoch 60/500\n",
      "12/12 [==============================] - 4s 352ms/step - loss: 0.3781 - acc: 0.8819 - val_loss: 1.3996 - val_acc: 0.5792\n",
      "Epoch 61/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.3759 - acc: 0.8779 - val_loss: 1.3790 - val_acc: 0.5730\n",
      "Epoch 62/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.3773 - acc: 0.8793 - val_loss: 1.4294 - val_acc: 0.5600\n",
      "Epoch 63/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.3771 - acc: 0.8731 - val_loss: 1.3968 - val_acc: 0.5785\n",
      "Epoch 64/500\n",
      "12/12 [==============================] - 4s 368ms/step - loss: 0.3587 - acc: 0.8874 - val_loss: 1.3116 - val_acc: 0.6052\n",
      "Epoch 65/500\n",
      "12/12 [==============================] - 4s 363ms/step - loss: 0.3450 - acc: 0.8858 - val_loss: 1.2823 - val_acc: 0.6059\n",
      "Epoch 66/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.3394 - acc: 0.8935 - val_loss: 1.3255 - val_acc: 0.5894\n",
      "Epoch 67/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.3568 - acc: 0.8815 - val_loss: 1.4114 - val_acc: 0.5744\n",
      "Epoch 68/500\n",
      "12/12 [==============================] - 4s 348ms/step - loss: 0.3364 - acc: 0.8958 - val_loss: 1.3485 - val_acc: 0.5901\n",
      "Epoch 69/500\n",
      "12/12 [==============================] - 5s 377ms/step - loss: 0.3426 - acc: 0.8879 - val_loss: 1.3163 - val_acc: 0.6134\n",
      "Epoch 70/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.3244 - acc: 0.8961 - val_loss: 1.3987 - val_acc: 0.5922\n",
      "Epoch 71/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.3357 - acc: 0.8925 - val_loss: 1.4750 - val_acc: 0.5661\n",
      "Epoch 72/500\n",
      "12/12 [==============================] - 4s 349ms/step - loss: 0.3195 - acc: 0.9001 - val_loss: 1.3493 - val_acc: 0.6066\n",
      "Epoch 73/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.2888 - acc: 0.9121 - val_loss: 1.4262 - val_acc: 0.5846\n",
      "Epoch 74/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.2950 - acc: 0.9088 - val_loss: 1.5007 - val_acc: 0.5977\n",
      "Epoch 75/500\n",
      "12/12 [==============================] - 4s 362ms/step - loss: 0.2842 - acc: 0.9136 - val_loss: 1.3306 - val_acc: 0.6162\n",
      "Epoch 76/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.3063 - acc: 0.9028 - val_loss: 1.4286 - val_acc: 0.5984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.3048 - acc: 0.9016 - val_loss: 1.8339 - val_acc: 0.5613\n",
      "Epoch 78/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.2962 - acc: 0.9073 - val_loss: 1.6905 - val_acc: 0.5305\n",
      "Epoch 79/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.3016 - acc: 0.9023 - val_loss: 1.3743 - val_acc: 0.5942\n",
      "Epoch 80/500\n",
      "12/12 [==============================] - 4s 352ms/step - loss: 0.2722 - acc: 0.9103 - val_loss: 1.5200 - val_acc: 0.5565\n",
      "Epoch 81/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.2648 - acc: 0.9131 - val_loss: 1.5540 - val_acc: 0.6093\n",
      "Epoch 82/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.2566 - acc: 0.9158 - val_loss: 1.3904 - val_acc: 0.5942\n",
      "Epoch 83/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.2533 - acc: 0.9179 - val_loss: 1.3574 - val_acc: 0.5977\n",
      "Epoch 84/500\n",
      "12/12 [==============================] - 4s 349ms/step - loss: 0.2529 - acc: 0.9169 - val_loss: 1.4416 - val_acc: 0.5908\n",
      "Epoch 85/500\n",
      "12/12 [==============================] - 4s 349ms/step - loss: 0.2489 - acc: 0.9234 - val_loss: 1.6114 - val_acc: 0.5888\n",
      "Epoch 86/500\n",
      "12/12 [==============================] - 4s 363ms/step - loss: 0.2696 - acc: 0.9139 - val_loss: 1.3622 - val_acc: 0.6210\n",
      "Epoch 87/500\n",
      "12/12 [==============================] - 4s 348ms/step - loss: 0.2555 - acc: 0.9196 - val_loss: 1.4573 - val_acc: 0.5942\n",
      "Epoch 88/500\n",
      "12/12 [==============================] - 4s 369ms/step - loss: 0.2464 - acc: 0.9220 - val_loss: 1.5068 - val_acc: 0.5874\n",
      "Epoch 89/500\n",
      "12/12 [==============================] - 4s 353ms/step - loss: 0.2431 - acc: 0.9220 - val_loss: 1.4222 - val_acc: 0.6032\n",
      "Epoch 90/500\n",
      "12/12 [==============================] - 4s 362ms/step - loss: 0.2558 - acc: 0.9162 - val_loss: 1.8736 - val_acc: 0.5120\n",
      "Epoch 91/500\n",
      "12/12 [==============================] - 4s 361ms/step - loss: 0.2838 - acc: 0.9033 - val_loss: 1.6642 - val_acc: 0.5922\n",
      "Epoch 92/500\n",
      "12/12 [==============================] - 4s 359ms/step - loss: 0.2516 - acc: 0.9181 - val_loss: 1.4319 - val_acc: 0.6134\n",
      "Epoch 93/500\n",
      "12/12 [==============================] - 4s 355ms/step - loss: 0.2192 - acc: 0.9330 - val_loss: 1.4499 - val_acc: 0.5826\n",
      "Epoch 94/500\n",
      "12/12 [==============================] - 4s 360ms/step - loss: 0.2125 - acc: 0.9383 - val_loss: 1.5279 - val_acc: 0.5826\n",
      "Epoch 95/500\n",
      "12/12 [==============================] - 4s 354ms/step - loss: 0.2022 - acc: 0.9402 - val_loss: 1.4179 - val_acc: 0.6086\n",
      "Epoch 96/500\n",
      "12/12 [==============================] - 4s 359ms/step - loss: 0.1918 - acc: 0.9431 - val_loss: 1.4444 - val_acc: 0.6141\n",
      "Epoch 97/500\n",
      "12/12 [==============================] - 4s 349ms/step - loss: 0.2095 - acc: 0.9371 - val_loss: 1.5649 - val_acc: 0.6025\n",
      "Epoch 98/500\n",
      "12/12 [==============================] - 4s 354ms/step - loss: 0.2291 - acc: 0.9299 - val_loss: 1.5759 - val_acc: 0.5696\n",
      "Epoch 99/500\n",
      "12/12 [==============================] - 4s 354ms/step - loss: 0.2195 - acc: 0.9299 - val_loss: 1.5589 - val_acc: 0.5819\n",
      "Epoch 100/500\n",
      "12/12 [==============================] - 4s 356ms/step - loss: 0.1976 - acc: 0.9405 - val_loss: 1.4345 - val_acc: 0.5956\n",
      "Epoch 101/500\n",
      "12/12 [==============================] - 4s 356ms/step - loss: 0.1935 - acc: 0.9427 - val_loss: 1.6916 - val_acc: 0.5607\n",
      "Epoch 102/500\n",
      "12/12 [==============================] - 4s 369ms/step - loss: 0.2045 - acc: 0.9331 - val_loss: 1.3487 - val_acc: 0.6244\n",
      "Epoch 103/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.1874 - acc: 0.9433 - val_loss: 1.6093 - val_acc: 0.5977\n",
      "Epoch 104/500\n",
      "12/12 [==============================] - 4s 354ms/step - loss: 0.2007 - acc: 0.9355 - val_loss: 1.5366 - val_acc: 0.5922\n",
      "Epoch 105/500\n",
      "12/12 [==============================] - 4s 353ms/step - loss: 0.2014 - acc: 0.9349 - val_loss: 1.4598 - val_acc: 0.6175\n",
      "Epoch 106/500\n",
      "12/12 [==============================] - 4s 358ms/step - loss: 0.1753 - acc: 0.9475 - val_loss: 1.4811 - val_acc: 0.5984\n",
      "Epoch 107/500\n",
      "12/12 [==============================] - 4s 363ms/step - loss: 0.1833 - acc: 0.9424 - val_loss: 1.4759 - val_acc: 0.6189\n",
      "Epoch 108/500\n",
      "12/12 [==============================] - 4s 361ms/step - loss: 0.1932 - acc: 0.9395 - val_loss: 1.5622 - val_acc: 0.6004\n",
      "Epoch 109/500\n",
      "12/12 [==============================] - 4s 353ms/step - loss: 0.1809 - acc: 0.9421 - val_loss: 1.8148 - val_acc: 0.5346\n",
      "Epoch 110/500\n",
      "12/12 [==============================] - 4s 353ms/step - loss: 0.1689 - acc: 0.9501 - val_loss: 1.5460 - val_acc: 0.5970\n",
      "Epoch 111/500\n",
      "12/12 [==============================] - 4s 359ms/step - loss: 0.1569 - acc: 0.9554 - val_loss: 1.5181 - val_acc: 0.5963\n",
      "Epoch 112/500\n",
      "12/12 [==============================] - 4s 353ms/step - loss: 0.1534 - acc: 0.9544 - val_loss: 1.5568 - val_acc: 0.5819\n",
      "Epoch 113/500\n",
      "12/12 [==============================] - 4s 356ms/step - loss: 0.1662 - acc: 0.9462 - val_loss: 1.8482 - val_acc: 0.5627\n",
      "Epoch 114/500\n",
      "12/12 [==============================] - 4s 356ms/step - loss: 0.1876 - acc: 0.9402 - val_loss: 1.5127 - val_acc: 0.6018\n",
      "Epoch 115/500\n",
      "12/12 [==============================] - 4s 356ms/step - loss: 0.1657 - acc: 0.9487 - val_loss: 1.6710 - val_acc: 0.5997\n",
      "Epoch 116/500\n",
      "12/12 [==============================] - 4s 356ms/step - loss: 0.1710 - acc: 0.9475 - val_loss: 1.5724 - val_acc: 0.6025\n",
      "Epoch 117/500\n",
      "12/12 [==============================] - 4s 360ms/step - loss: 0.1613 - acc: 0.9518 - val_loss: 1.4784 - val_acc: 0.6189\n",
      "Epoch 118/500\n",
      "12/12 [==============================] - 4s 351ms/step - loss: 0.1486 - acc: 0.9575 - val_loss: 1.5016 - val_acc: 0.6066\n",
      "Epoch 119/500\n",
      "12/12 [==============================] - 4s 349ms/step - loss: 0.1598 - acc: 0.9484 - val_loss: 1.5448 - val_acc: 0.6114\n",
      "Epoch 120/500\n",
      "12/12 [==============================] - 4s 350ms/step - loss: 0.1361 - acc: 0.9594 - val_loss: 1.6422 - val_acc: 0.6052\n",
      "Epoch 121/500\n",
      "12/12 [==============================] - 4s 362ms/step - loss: 0.1347 - acc: 0.9606 - val_loss: 1.6437 - val_acc: 0.6011\n",
      "Epoch 122/500\n",
      "12/12 [==============================] - 4s 361ms/step - loss: 0.1462 - acc: 0.9535 - val_loss: 1.6081 - val_acc: 0.5860\n",
      "Epoch 123/500\n",
      "12/12 [==============================] - 4s 355ms/step - loss: 0.1465 - acc: 0.9556 - val_loss: 1.5756 - val_acc: 0.6018\n",
      "Epoch 124/500\n",
      "12/12 [==============================] - 4s 356ms/step - loss: 0.1562 - acc: 0.9518 - val_loss: 1.7192 - val_acc: 0.5798\n",
      "Epoch 125/500\n",
      "12/12 [==============================] - 4s 360ms/step - loss: 0.1570 - acc: 0.9513 - val_loss: 1.6827 - val_acc: 0.5942\n",
      "Epoch 126/500\n",
      "12/12 [==============================] - 5s 395ms/step - loss: 0.1432 - acc: 0.9544 - val_loss: 1.6170 - val_acc: 0.6066\n",
      "Epoch 127/500\n",
      "12/12 [==============================] - 4s 359ms/step - loss: 0.1285 - acc: 0.9599 - val_loss: 1.5359 - val_acc: 0.5970\n",
      "Epoch 128/500\n",
      "12/12 [==============================] - 4s 354ms/step - loss: 0.1157 - acc: 0.9702 - val_loss: 1.5045 - val_acc: 0.6223\n",
      "Epoch 129/500\n",
      "12/12 [==============================] - 4s 373ms/step - loss: 0.1632 - acc: 0.9542 - val_loss: 1.7211 - val_acc: 0.5744\n",
      "Epoch 130/500\n",
      "12/12 [==============================] - 5s 381ms/step - loss: 0.1493 - acc: 0.9532 - val_loss: 1.4609 - val_acc: 0.6203\n",
      "Epoch 131/500\n",
      "12/12 [==============================] - 4s 359ms/step - loss: 0.1165 - acc: 0.9676 - val_loss: 1.7390 - val_acc: 0.5949\n",
      "Epoch 132/500\n",
      "12/12 [==============================] - 4s 361ms/step - loss: 0.1105 - acc: 0.9712 - val_loss: 1.7226 - val_acc: 0.6052\n",
      "Epoch 133/500\n",
      "12/12 [==============================] - 4s 359ms/step - loss: 0.1214 - acc: 0.9626 - val_loss: 1.6558 - val_acc: 0.6141\n",
      "Epoch 134/500\n",
      "12/12 [==============================] - 4s 366ms/step - loss: 0.1176 - acc: 0.9666 - val_loss: 1.5748 - val_acc: 0.6059\n",
      "Epoch 135/500\n",
      "12/12 [==============================] - 4s 366ms/step - loss: 0.1075 - acc: 0.9697 - val_loss: 1.5492 - val_acc: 0.6148\n",
      "Epoch 136/500\n",
      "12/12 [==============================] - 4s 358ms/step - loss: 0.0933 - acc: 0.9777 - val_loss: 1.5690 - val_acc: 0.6114\n",
      "Epoch 137/500\n",
      "12/12 [==============================] - 4s 369ms/step - loss: 0.0982 - acc: 0.9731 - val_loss: 1.5977 - val_acc: 0.6052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/500\n",
      "12/12 [==============================] - 5s 377ms/step - loss: 0.0942 - acc: 0.9765 - val_loss: 1.6336 - val_acc: 0.5990\n",
      "Epoch 139/500\n",
      "12/12 [==============================] - 4s 358ms/step - loss: 0.0966 - acc: 0.9755 - val_loss: 1.6854 - val_acc: 0.6189\n",
      "Epoch 140/500\n",
      "12/12 [==============================] - 4s 354ms/step - loss: 0.1027 - acc: 0.9707 - val_loss: 1.7624 - val_acc: 0.5744\n",
      "Epoch 141/500\n",
      "12/12 [==============================] - 4s 373ms/step - loss: 0.0953 - acc: 0.9757 - val_loss: 1.6209 - val_acc: 0.6004\n",
      "Epoch 142/500\n",
      "12/12 [==============================] - 4s 352ms/step - loss: 0.0941 - acc: 0.9753 - val_loss: 1.6766 - val_acc: 0.6107\n",
      "Epoch 143/500\n",
      "12/12 [==============================] - 4s 352ms/step - loss: 0.1012 - acc: 0.9721 - val_loss: 1.7519 - val_acc: 0.6134\n",
      "Epoch 144/500\n",
      " 8/12 [===================>..........] - ETA: 1s - loss: 0.0955 - acc: 0.9756"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-973a37c1251b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m                                  \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                                  save_best_only=True)\n\u001b[1;32m---> 23\u001b[1;33m     model.fit(x[trn_idx], y_[trn_idx],\n\u001b[0m\u001b[0;32m     24\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "proba_t = np.zeros((7500, 19))\n",
    "proba_val = np.zeros((7292, 19))\n",
    "for fold, (trn_idx, val_idx) in enumerate(kfold.split(x, y)):\n",
    "    y_ = to_categorical(y, num_classes=19)\n",
    "    model = generate_model()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['acc'])\n",
    "    plateau = ReduceLROnPlateau(monitor=\"val_acc\",\n",
    "                                verbose=0,\n",
    "                                mode='auto',\n",
    "                                factor=1. / np.cbrt(2),\n",
    "                                patience=200)\n",
    "    early_stopping = EarlyStopping(monitor='val_acc',\n",
    "                                   verbose=0,\n",
    "                                   mode='auto',\n",
    "                                   patience=100)\n",
    "    checkpoint = ModelCheckpoint(f'fold{fold}.h5',\n",
    "                                 monitor='val_acc',\n",
    "                                 verbose=0,\n",
    "                                 mode='auto',\n",
    "                                 save_best_only=True)\n",
    "    model.fit(x[trn_idx], y_[trn_idx],\n",
    "              epochs=500,\n",
    "              batch_size=512,\n",
    "              verbose=1,\n",
    "              shuffle=True,\n",
    "              validation_data=(x[val_idx], y_[val_idx]),\n",
    "              callbacks=[plateau, early_stopping, checkpoint])\n",
    "    model.load_weights(f'fold{fold}.h5')\n",
    "    proba_t += model.predict(t, verbose=0, batch_size=1024) / 5.\n",
    "    proba_val[val_idx] = model.predict(x[val_idx]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tGNR5_DI1YQP"
   },
   "outputs": [],
   "source": [
    "oof_y = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "CvRAbwRJ1nOW",
    "outputId": "18860b3f-caf2-4fdd-d5a7-aabb563ededb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07913\n",
      "0.15023\n"
     ]
    }
   ],
   "source": [
    "print(round(accuracy_score(y, oof_y), 5))\n",
    "score = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(y, oof_y)) / oof_y.shape[0]\n",
    "print(round(score, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7292, 1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.argmax(1)[:,np.newaxis].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 41.20013046,  23.46347046,  57.5158844 ,   3.        ],\n",
       "       [ 42.66721344,  29.75338936,  71.60204315,   6.        ],\n",
       "       [ 36.19981003,  43.88243103,  72.85785675,   6.        ],\n",
       "       ...,\n",
       "       [264.80477905, 196.98394775, 317.60546875,   6.        ],\n",
       "       [194.07733154, 213.60491943, 252.17758179,  10.        ],\n",
       "       [127.26914978, 111.34897614, 160.05895996,   6.        ]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((mid_layer_output,y_pred.argmax(1)[:,np.newaxis]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('fold4.h5')\n",
    "\n",
    "from keras import backend as K\n",
    "mid_layer = Model([model.input],\n",
    "                       [model.layers[-2].output])\n",
    "mid_layer_output = mid_layer.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_plot = pd.DataFrame(np.concatenate([mid_layer_output,y_pred.argmax(1)[:,np.newaxis]],axis=1),\n",
    "            columns=['x','y','z','behavior'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 41.20013 ,  23.46347 ,  57.515884],\n",
       "       [ 42.667213,  29.75339 ,  71.60204 ],\n",
       "       [ 36.19981 ,  43.88243 ,  72.85786 ],\n",
       "       ...,\n",
       "       [264.80478 , 196.98395 , 317.60547 ],\n",
       "       [194.07733 , 213.60492 , 252.17758 ],\n",
       "       [127.26915 , 111.348976, 160.05896 ]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "%matplotlib qt \n",
    "#colors = cm.rainbow(np.linspace(0, 1, 19))\n",
    "colors = [plt.cm.tab10(i/float(19-1)) for i in range(19)]\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = Axes3D(fig)\n",
    "for label,color in zip(range(19),colors):\n",
    "    cnt = hidden_plot.loc[hidden_plot['behavior']==label].shape[0]\n",
    "    ax.scatter(hidden_plot.loc[hidden_plot['behavior']==label,'x'],\n",
    "               hidden_plot.loc[hidden_plot['behavior']==label,'y'],\n",
    "               hidden_plot.loc[hidden_plot['behavior']==label,'z'],\n",
    "               color=color,s=cnt/10,alpha=0.5,label=label)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JkLjhfcU9Bga"
   },
   "outputs": [],
   "source": [
    "sub.behavior_id = np.argmax(proba_t,axis=1)\n",
    "sub.to_csv('sub.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YcOi3_Pl-yV8"
   },
   "outputs": [],
   "source": [
    "def Net():\n",
    "    input = Input(shape=(60, 8))\n",
    " \n",
    "    X = Conv1D(filters=64,kernel_size=2,padding='same',activation='relu')(input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = MaxPool1D(pool_size=2)(X)\n",
    "    X = GRU(128,kernel_initializer='he_normal')(input)\n",
    "    X = Dropout(0.8)(X)\n",
    "    X = Dense(128,activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dense(256,activation='relu')(X)\n",
    "    X = Dropout(0.8)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dense(19,activation='softmax')(X)\n",
    "\n",
    "\n",
    "    return Model([input], X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZNKnRkM_hmal"
   },
   "outputs": [],
   "source": [
    "x = x.reshape(7292,60,8)\n",
    "t = t.reshape(7500, 60, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46ctBe5hkeaU"
   },
   "outputs": [],
   "source": [
    "def Net1():\n",
    "\n",
    "    ip = Input(shape=(60,10,1))\n",
    "    \n",
    "    x = Reshape(target_shape=(60,10))(ip)\n",
    "    x = LSTM(16,return_sequences=False)(x)\n",
    "    #x = AttentionLSTM(16)(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "    \n",
    "\n",
    "    #y = Permute((2, 1))(ip)\n",
    "    y = Conv2D(filters=128,\n",
    "               kernel_size=(3, 3),\n",
    "               activation='relu',\n",
    "               kernel_initializer='he_uniform',\n",
    "               padding='same')(ip)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv2D(filters=256,\n",
    "               kernel_size=(3, 3),\n",
    "               activation='relu',\n",
    "               kernel_initializer='he_uniform',\n",
    "               padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv2D(filters=128,\n",
    "               kernel_size=(3, 3),\n",
    "               activation='relu',\n",
    "               kernel_initializer='he_uniform',\n",
    "               padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling2D()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    out = Dense(19, activation='softmax')(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # add load model code here to fine-tune\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "X2IAl-8A6Z1R",
    "outputId": "6a77461b-b3f5-4e38-fbf8-d7ba6c9b32fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 60, 8, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 60, 8, 128)   1280        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 60, 8, 128)   512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 60, 8, 128)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 60, 8, 256)   295168      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 60, 8, 256)   1024        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 60, 8, 256)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 60, 8, 128)   295040      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 60, 8)        0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 60, 8, 128)   512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 32)           5248        reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 60, 8, 128)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32)           0           lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 128)          0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 160)          0           dropout_5[0][0]                  \n",
      "                                                                 global_average_pooling2d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 19)           3059        concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 601,843\n",
      "Trainable params: 600,819\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mode = Net1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tjMonCCXZzDY"
   },
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n",
    "\n",
    "    x = Masking()(ip)\n",
    "    x = LSTM(8)(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    out = Dense(NB_CLASS, activation='softmax')(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "    model.summary()\n",
    "\n",
    "    # add load model code here to fine-tune\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('XLA_GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8DGZlSthYPLN"
   },
   "outputs": [],
   "source": [
    "def generate_model_2():\n",
    "    ip = Input(shape=(MAX_NB_VARIABLES, MAX_TIMESTEPS))\n",
    "\n",
    "    ''' sabsample timesteps to prevent OOM due to Attention LSTM '''\n",
    "    stride = 2\n",
    "\n",
    "    x = Permute((2, 1))(ip)\n",
    "    x = Conv1D(MAX_NB_VARIABLES // stride, 8, strides=stride, padding='same', activation='relu', use_bias=False,\n",
    "               kernel_initializer='he_uniform')(x) # (None, variables / stride, timesteps)\n",
    "    x = Permute((2, 1))(x)\n",
    "\n",
    "    x = Masking()(x)\n",
    "    x = AttentionLSTM(128)(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "\n",
    "    y = Permute((2, 1))(ip)\n",
    "    y = Conv1D(128, 8, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(256, 5, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    y = squeeze_excite_block(y)\n",
    "\n",
    "    y = Conv1D(128, 3, padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = GlobalAveragePooling1D()(y)\n",
    "\n",
    "    x = concatenate([x, y])\n",
    "\n",
    "    out = Dense(NB_CLASS, activation='softmax')(x)\n",
    "\n",
    "    model = Model(ip, out)\n",
    "    model.summary()\n",
    "\n",
    "    # add load model code here to fine-tune\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fi5LOBzJdx28"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "XW_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
